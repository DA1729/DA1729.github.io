<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Non-Algorithmic Foundations of Randomness</title>
      <link href="/2026/03/01/Non-Algorithmic-Theory-of-Randomness/"/>
      <url>/2026/03/01/Non-Algorithmic-Theory-of-Randomness/</url>
      
        <content type="html"><![CDATA[<p>The algorithmic theory of randomness has long served as a powerful source of intuition in mathematics and computer science. However, its practical application is often hindered by the presence of unspecified additive or multiplicative constants. This post examines Vladimir Vovk’s framework, which provides a precise language for randomness by replacing these constants with the rigorous definitions of p-variables and e-variables.</p><h2><span id="the-limitation-of-algorithmic-heuristics">The Limitation of Algorithmic Heuristics</span></h2><p>Traditional algorithmic randomness results are frequently stated “up to an additive constant,” a convention that renders them difficult to apply in finite, real-world contexts. Vovk proposes a shift toward a language that does not rely on these hidden factors, making mathematical results in this domain essentially computable and applicable in practice. The primary testing ground for this precision is the definition of Bernoulli sequences<br>a problem central to the work of Andrei Kolmogorov.</p><h2><span id="formalizing-evidence-p-variables-and-e-variables">Formalizing Evidence: p-variables and e-variables</span></h2><p>The framework centers on two measurable functions, $f:\Omega\rightarrow[0,\infty]$, defined with respect to a probability measure $P$:</p><ul><li><strong>p-variable</strong>: A function such that for any $\epsilon&gt;0$, $P{f\le\epsilon}\le\epsilon$.</li><li><strong>e-variable</strong>: A function such that $\int fdP\le1$.</li></ul><p>In this context, e-values (often termed “betting scores”) represent the amount of evidence against a hypothesis $P$. While p-values and e-values operate on different scales, they are fundamentally related. For any $\kappa\in(0,1)$, the following inclusion holds:<br>$$\kappa\mathcal{P}_P^{\kappa-1}\subseteq\mathcal{E}_P\subseteq\mathcal{P}_P^{-1}$$<br>This relationship demonstrates that while p-values and e-values are equivalent on a crude scale (where a p-value of $p$ roughly corresponds to an e-value of $1&#x2F;p$), the e-variable approach often yields cleaner, more precise mathematical identities.</p><h2><span id="composite-hypotheses-and-the-upper-envelope">Composite Hypotheses and the Upper Envelope</span></h2><p>When dealing with a statistical model $P&#x3D;(P_\theta|\theta\in\Theta)$ rather than a simple hypothesis, the definition of evidence must account for the parameter space. An e-variable with respect to a composite model is defined by its upper envelope:<br>$$\forall\theta\in\Theta:\int_{\Omega}f(\omega)P_\theta(d\omega)\le1$$<br>This leads to a significant simplification of Bayesian statistics. Given a prior probability measure $Q$ on $\Theta$ and a joint probability measure $T$, Vovk establishes the <strong>Product of Evidence</strong> theorem:<br>$$\mathcal{E}_T &#x3D; \mathcal{E}_P\mathcal{E}_Q$$<br>Unlike the algorithmic version of this result, this formula is exact. it indicates that evidence against a Bayesian model is simply the product of the evidence against the prior and the evidence against the likelihood.</p><h2><span id="the-bernoulli-problem-iid-vs-exchangeability">The Bernoulli Problem: IID vs. Exchangeability</span></h2><p>Applying this framework to Bernoulli sequences clarifies a long-standing approximation in Kolmogorov’s work. Kolmogorov initially defined Bernoulli sequences as exchangeable sequences<br>those whose probability remains invariant under permutation. Vovk’s framework provides a narrower, more accurate decomposition:<br>$$\mathcal{E}_{Bern} &#x3D; \mathcal{E}_{exch}\mathcal{E}_{bin}$$<br>This identity reveals that a sequence is Bernoulli if and only if it is both exchangeable <strong>and</strong> its distribution of successes is binomial. </p><p>Crucially, the framework allows us to quantify the “importance” of these components. By comparing the supremum ranges of these classes<br>where the supremum of $\mathcal{E}_{exch}$ grows much faster than that of $\mathcal{E}_{bin}$<br>we can mathematically confirm that exchangeability is indeed the dominant characteristic of Bernoulliness, justifying Kolmogorov’s original intuition while providing the precision he lacked.</p><h2><span id="conclusion">Conclusion</span></h2><p>The transition from the “approximate” results of algorithmic information theory to the “exact” identities of p- and e-variables represents a significant step toward a practical theory of randomness. By eliminating unspecified constants, we gain a framework that is not only theoretically elegant but also ready for rigorous statistical application.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Randomness </tag>
            
            <tag> Algorithmic Information Theory </tag>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algebraic Cryptanalysis of Lattice Cryptography</title>
      <link href="/2025/12/27/Block-Korkine-Zolotarev-BKZ-Algorithm/"/>
      <url>/2025/12/27/Block-Korkine-Zolotarev-BKZ-Algorithm/</url>
      
        <content type="html"><![CDATA[<p>I have been wanting to dive into algebraic cryptanalysis of the modern cryptographic primitives. Almost all of PQC and FHE is based on mathematical lattices, and hard problems based on it. For those who are not aware, cryptanalysis is a branch of cryptology, where we try to find weaknesses and vulnerabilities in a given cryptosystem. For this, we must have a deep and conceptual understanding of the system itself in order to try and find such compromises.</p><h2><span id="definition-of-a-lattice">Definition of a Lattice</span></h2><p>In simple terms, a <strong>lattice</strong> $\mathcal{L}$ is a collection of points in $n$-dimensional space with a regular, repeating structure. Mathematically, we define it using a <strong>basis</strong>.</p><p>If we have $n$ linearly independent vectors, $\{b_1, \cdots, b_n\}$ in $\mathbb{R}^n$, the lattice generated by this basis is the set of all <strong>integer</strong> linear combinations of these vectors: $$ \mathcal{L} &#x3D; \{ \sum_{i&#x3D;1}^{n} x_ib_i : x_i \in \mathbb{Z}\}$$ </p><p>Now, one might say that it’s a vector space over the field of real numbers. But note that we have constrained our $x_i$s to integers only, in case of a vector space, these could have been any real number. This constraint creates a <strong>discrete</strong> grid of points. </p><h3><span id="basis-matrix-and-determinant">Basis Matrix and Determinant</span></h3><p>We usually represent the basis as a matrix $\mathcal{B}$, where each column is a basis vector $b_i$. </p><h4><span id="the-fundamental-parallelpiped">The Fundamental Parallelpiped</span></h4><p>If you take the basis vectors, and “fill in” the space between them (using coefficients between 0 and 1), you get a shape called the <strong>fundamental parallelpiped</strong>, denoted as $\mathcal{P}(\mathcal{B})$: $${P}(\mathcal{B}) &#x3D; \{ \sum_{i&#x3D;1}^{n} a_ib_i : a_i \in [0, 1)\}$$</p><p>Below are two figures visualizing the same fundamental parallelpiped in 2D and 3D, and for very obvious reasons, I cannot present a visualization of any higher dimension.</p><p align="center">  <img src="/images/2d_parallel.png" width="45%">  <img src="/images/3d_parallel.png" width="45%"></p><h4><span id="the-determinant-volume">The Determinant (Volume)</span></h4><p>The volume of this shape is an invariant of the lattice called <strong>determinant</strong>, denoted $\text{det}(\mathcal{L})$. It represents the “inverse density” of the points. For a full-rank lattice: $$\text{det}(\mathcal{L}) &#x3D; |\text{det}(B)|$$</p><h2><span id="good-basis-vs-bad-basis-problem">“Good Basis” vs “Bad Basis” Problem</span></h2><p>This is the core of lattice-based cryptography. A single lattice can be represented by infinitely many different bases. </p><ul><li><strong>Good Basis:</strong> Vectors are relatively short and nearly <strong>orthogonal</strong> (perpendicular). This makes it easy to find the point in the lattice closest to some target. I’ll elaborate more on this later. </li><li><strong>Bad Basis:</strong> Vectors are very long and meet at extremely sharp angles. This makes the lattice look like a chaotic “haystack,” hiding its structure.</li></ul><p>In a cryptosystem, the <strong>Secret Key</strong> is usually a Good Basis, and the <strong>Public Key</strong> is a “randomized” Bad Basis. Cryptanalysis is essentially the attempt to turn that Bad Basis back into a Good one.</p><h2><span id="minkowski-s-theorem">Minkowski’s Theorem</span></h2><p>This is a core theorem of lattices. It is a theorem which if one gets the proper intuition of, gets great insights into analysis of lattice structures. It essentially guarantees that if a lattice has a certain volume, there <strong>must</strong> exist a non-zero short vector within a specific bound. To get a good intuition of this <strong>short vector</strong>, imagine a lattice on the 2D plane with the fundamental parallelopipeds sketched throughout, considering all the normal vectors from the origin to these edges, the one having the least “length” is labelled as the shortest vector.</p><p>In cryptanalysis, one should not be afraid of formalism, sooo, take a look below: </p><blockquote><p><strong>Theorem (Minkowski’s Theorem)</strong><br>Let $\mathcal{L}$ be a full-rank lattice in $\mathbb{R}^n$ with determinant $\text{det}(\mathcal{L})$. If $S \subset \mathbb{R}^n$ is a <strong>convex</strong>, <strong>centrally symmetric</strong> set with $\text{vol}(S) &gt; 2^n$, then $S$ contains at least one non-zero lattice point $v \in \mathcal{L} \setminus \{0\}$.</p></blockquote><p>How does this statement map to the explanation I gave above? Well, imagine placing the lattice points in space and drawing the fundamental parallelpiped around each point, so that space is perfectly tiled. Now center a symmetric “ball” (or any convex shape) at the origin and slowly expand it. If this shape becomes large enough compared to the volume of a single parallelpiped, it is impossible for it to cover only the origin, there simply isn’t enough room to avoid capturing any other lattice point. When this happens, the vector from the origin to that point is a <em>short lattice vector</em>. Minkowski’s theorem makes this intuition precise: the volume of the lattice directly limits how far the nearest non-zero lattice point can be. For a convex set, it is the set of points, where each line connecting any two points lies completely inside the curve traced by the set.</p><p>Now, let’s look into the proof. First, let’s have a look at the following lemma, or the <strong>Blichfeldt’s Theorem</strong>, which if you really see, is nothing but the <strong>pigeon-hole principle</strong>.</p><blockquote><p><strong>Theorem (Blichfeldt’s Theorem)</strong><br>If a set $S’$ has a volume strictly greater than the determinant of the lattice ($\text{vol}(S’) &gt; \text{det}(\mathcal{L})$), then there must be at least two distinct points $z_1, z_2 \in S’$ such that their difference $z_1 - z_2$ is a lattice point.</p></blockquote><p>To get some formal intuition, imagine a random lattice, the best visualization is the 2D plane tiled with the same parallelpiped throughout. Now imagine a cloth with area greater than the area of a single parallelpiped, note to avoid confusion, that I am referring to the volume of the lattice as area in this case. Instead of working with normal cartesian coordinates, any point is identified by which parallelpiped the point lies in and its position relative to that particular parallelpiped only. If we just place this cloth (set) over this lattice, we shall have at least two points with same internal relative coordinates with different parallelpiped coordinates, now the global vector joining these two points, will just be a lattice vector with some offset with respect to the grid lines.</p><h5><span id="proof-blichfeldt-s-theorem">Proof (Blichfeldt’s Theorem)</span></h5><p>Let $\mathcal{L} \subset \mathbb{R}^n$ be a full-rank lattice, and let $F$ be a fundamental domain of $\mathcal{L}$ with volume<br>$$<br>\operatorname{vol}(F) &#x3D; \det(\mathcal{L}) &#x3D; V.<br>$$</p><p>Let $S’ \subset \mathbb{R}^n$ be a measurable set with volume<br>$$<br>\operatorname{vol}(S’) &#x3D; V’ &gt; V.<br>$$</p><p>Every point $x \in \mathbb{R}^n$ can be written uniquely as<br>$$<br>x &#x3D; \alpha + \beta,<br>$$<br>where $\alpha \in \mathcal{L}$ is a lattice vector identifying the lattice cell containing $x$, and $\beta \in F$ is the local position of $x$ within that cell.</p><p>Define the mapping<br>$$<br>\pi : \mathbb{R}^n \to F<br>$$<br>by reducing points modulo the lattice:<br>$$<br>\pi(x) &#x3D; \beta.<br>$$<br>This map translates each point of $S’$ into the reference cell $F$ without changing volume.</p><p>Since translations preserve volume,<br>$$<br>\operatorname{vol}(\pi(S’)) &#x3D; \operatorname{vol}(S’) &#x3D; V’ &gt; V &#x3D; \operatorname{vol}(F).<br>$$<br>However, $\pi(S’) \subseteq F$, so this is only possible if the map $\pi$ is <strong>not injective</strong>.</p><p>Hence, there exist two distinct points $P_1, P_2 \in S’$ such that<br>$$<br>\pi(P_1) &#x3D; \pi(P_2) &#x3D; \beta.<br>$$<br>Writing<br>$$<br>P_1 &#x3D; \alpha_1 + \beta \quad \text{and} \quad P_2 &#x3D; \alpha_2 + \beta,<br>$$<br>with $\alpha_1, \alpha_2 \in \mathcal{L}$, we obtain<br>$$<br>P_2 - P_1 &#x3D; \alpha_2 - \alpha_1 \in \mathcal{L}.<br>$$</p><p>Thus, the difference of two distinct points in $S’$ is a nonzero lattice vector, completing the proof.</p><h5><span id="proof-minkowski-s-theorem">Proof (Minkowski’s Theorem)</span></h5><p>With the above theorem in our hands, let’s see the proof of the core theorem. </p><p>Let $\mathcal{L} \subset \mathbb{R}^n$ be a full-rank lattice and let $S \subset \mathbb{R}^n$ be convex and centrally symmetric with<br>$$<br>\operatorname{vol}(S) &gt; 2^n \det(\mathcal{L}).<br>$$</p><p>Define the scaled set<br>$$<br>S’ &#x3D; \frac{1}{2} S.<br>$$<br>Then<br>$$<br>\operatorname{vol}(S’) &#x3D; 2^{-n} \operatorname{vol}(S) &gt; \det(\mathcal{L}).<br>$$</p><p>By <strong>Blichfeldt’s Theorem</strong>, there exist two distinct points<br>$$<br>x_1, x_2 \in S’<br>$$<br>such that<br>$$<br>x_1 - x_2 \in \mathcal{L}.<br>$$</p><p>Since $S’ &#x3D; \frac{1}{2} S$, we have $2x_1, 2x_2 \in S$. Therefore,<br>$$<br>2(x_1 - x_2) &#x3D; (2x_1) - (2x_2).<br>$$</p><p>Because $S$ is centrally symmetric, $-2x_2 \in S$, and because $S$ is convex, the sum of two points in $S$ lies in $S$. Hence,<br>$$<br>v :&#x3D; 2(x_1 - x_2) \in S \cap \mathcal{L}.<br>$$</p><p>Since $x_1 \neq x_2$, we have $v \neq 0$, proving that $S$ contains a non-zero lattice point.<br><strong>QED</strong></p><hr><p>Now, one might wonder, why did we only assume $\operatorname{vol}(S) &gt; 2^n \det(\mathcal{L})$ and not any arbitrary factor? Well, the constant $2^n$ is not arbitrary, it is <strong>optimal</strong>. To see why, observe that shrinking a set by a factor of two in each dimension scales its volume by $2^{-n}$. The proof of Minkowski’s theorem relies on shrinking $S$ to<br>$$<br>S’ &#x3D; \frac{1}{2} S,<br>$$<br>so that<br>$$<br>\operatorname{vol}(S’) &#x3D; 2^{-n} \operatorname{vol}(S).<br>$$</p><p>The hypothesis $\operatorname{vol}(S) &gt; 2^n \det(\mathcal{L})$ ensures that<br>$$<br>\operatorname{vol}(S’) &gt; \det(\mathcal{L}),<br>$$</p><p>which is precisely the condition needed to apply <strong>Blichfeldt’s Theorem</strong>.</p><p>Crucially, this bound cannot be improved in general. For example, in the lattice $\mathbb{Z}^n$, the centrally symmetric cube<br>$$<br>(-1,1)^n<br>$$<br>has volume $2^n$ and contains no non-zero lattice points. Any smaller constant would allow counterexamples.</p><p>Thus, $2^n$ is the smallest universal threshold that guarantees the existence of a non-zero lattice vector.</p><hr><p>Now, why is this theorem sooo important in post-quantum cryptology? For the <strong>designer</strong> of the cryptosystem, the theorem defines the “safety zone.” It tells us exactly how small we can make our errors (noise) while still ensuring that the secret key remains unique and theoretically recoverable. </p><p>For a <strong>cryptanalyst</strong>, the theorem is a guarantee of success, given enough time. It proves that the “shortest vector” is mathematically bound to exist within a specific radius. However, there is a catch, which ensures the lattice-based security: <strong>Minkowski’s Theorem proves the vector exists, but it doesn’t give us a map to find it.</strong> In high dimensions ($n &gt; 500$), even though we know a short vector is “hiding” in that convex set, finding it is like searching for a needle in a hyper-dimensional haystack.</p><hr><p>Let’s end this part here. This should be enough for us to formally understand the hard problems of lattice and reverse engineering, or solving those problems.</p><p>PS: I am an undergrad student only trying to make sense out of all the math behind the modern crypto, if you find any error, logical, formatting, language, etc… feel free to inform me via <a href="daksh_p@ph.iitr.ac.in">email</a> or <a href="https://x.com/sp0oky_daksh">twitter</a>.</p><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Cryptanalysis </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
            <tag> Post-Quantum Cryptography </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MPC in the Head (MPCitH)</title>
      <link href="/2025/12/06/MPC-in-the-Head-MPCitH/"/>
      <url>/2025/12/06/MPC-in-the-Head-MPCitH/</url>
      
        <content type="html"><![CDATA[<p>In continuation to my MPC blogs and my study of <a href="https://faest.info/">FAEST</a>, here is a blog explaining a method of constructing advanced <strong>Zero Knowledge (ZK)</strong> proofs by using simpler tools from <strong>Multi-Party Computation (MPC)</strong>.</p><h2><span id="zero-knowledge-proof">Zero-Knowledge Proof</span></h2><p>Let’s say that you are competing with a friend over a “Where’s Waldo” puzzle and the one who finds Waldo in lesser time wins. Now, if you went first, after finding Waldo, you obviously cannot just prove to your friend that you have solved it by pointing it out in plain sight… it would ruin their turn. And if you don’t prove it, your friend could easily argue that you’re bluffing.<br>You must find a way to convince your friend that you know where Waldo is without showing them <em>where</em> Waldo is. Again, sounds like a Martin Gardner puzzle… and again I’m pretty sure there must be some similar problem in his collection.</p><p>Let’s break it down somewhat formally:</p><ul><li><strong>Goal</strong>: Prove you know a secret (location of Waldo) without revealing the location.</li><li><strong>Roles</strong>: You are the <strong>Prover</strong> and your friend is the <strong>Verifier</strong>.</li></ul><p>Now how to prove it? Imagine you take a giant sheet of cardboard, much larger than the puzzle book, and cut a tiny hole in the center, just big enough to see Waldo.</p><ul><li><strong>Setup</strong>: You tell your friend to turn around, then you place the cardboard over the puzzle book so that <strong>only</strong> Waldo is visible through the hole.</li><li><strong>Proof</strong>: You invite your friend to look. They look through the hole and see Waldo.</li><li><strong>Trick</strong>: Because the cardboard is huge and blocks out all the landmarks, your friend has no context. They just know that you know where Waldo is, but have no idea where on the page he is.</li></ul><p>Now, why do cryptographers obsess so much over such proofs? First, let’s see what core points are satisfied with a ZKP:</p><ul><li><strong>Completeness (works for truth-tellers)</strong>: If you actually know where Waldo is, you can always perform this trick successfully.</li><li><strong>Soundness (fails for liars)</strong>: If you were bluffing, you cannot position the cardboard to show the real Waldo. You can’t cheat the verifier.</li><li><strong>Zero-Knowledge (it leaks nothing)</strong>: After the game, your friend learns nothing new. If they tried to find Waldo themselves afterwards, they would have no advantage compared to before you showed them the proof. The “view” they saw (Waldo through the hole) is something they could have easily imagined themselves without your help.</li></ul><h2><span id="formalization">Formalization</span></h2><p>Ok, now let’s formalize things a little. In cryptography, we don’t just “prove” things, we prove membership in a language defined by a relation $\mathcal{R}$.</p><p>Let $x$ be the public statement (the “instance”) and $w$ be the secret witness. We define the relation as a set of pairs:<br>$$\mathcal{R} &#x3D; \{(x, w): \text{statement } x \text{ is true with witness } w \}$$</p><p>In our Waldo example:</p><ul><li>$x$: The specific page of the puzzle book</li><li>$w$: The $(x, y)$ coordinates of Waldo</li><li>$(x, w) \in \mathcal{R}$ if and only if Waldo is actually at those coordinates on that page.</li></ul><p>A ZKP is an interactive protocol between two probabilistic polynomial-time (PPT) algorithms: the <strong>Prover</strong> ($\mathcal{P}$) and the <strong>Verifier</strong> ($\mathcal{V}$).</p><p>Many of you might already know what probabilistic polynomial-time (PPT) algorithms are. At the time of writing the blog, I did not, so here is a quick explanation for readers like me:</p><ul><li>Its running time is bounded by a polynomial in the size of the input $n$, for all possible random choices it makes. So, if $T(n)$ is the worst-case number of steps, then $T(n) &#x3D; \text{poly}(n)$.</li><li><strong>Has access to randomness</strong>, or has an extra stream of input: a stream of random bits. So the algorithm is a function: $A(x, r)$ where $x$ is the actual input and $r$ is the random string the algorithm uses.</li><li><strong>It is allowed to have a probability of error</strong>. Depending on the problem, the algorithm may give a correct answer with high probability, which can be amplified by repetition.</li></ul><h3><span id="three-properties">Three Properties</span></h3><h4><span id="completeness">Completeness</span></h4><p>If the statement is true and the prover is honest, the verifier accepts.<br>$$\forall (x, w) \in \mathcal{R} : \text{Pr}[\langle \mathcal{P}(w), \mathcal{V} \rangle (x) &#x3D; 1] &#x3D; 1$$</p><h4><span id="soundness-knowledge-error">Soundness (Knowledge Error)</span></h4><p>If the statement is false (or the Prover doesn’t know $w$), a malicious prover $\mathcal{P}^*$ cannot convince the verifier, except with some negligible probability $\epsilon$ (soundness error).<br>$$\forall x \notin L, \forall \mathcal{P}^* : \text{Pr}[\langle \mathcal{P}^*, \mathcal{V} \rangle (x) &#x3D; 1 ] \leq \epsilon$$</p><h4><span id="zero-knowledge-simulation-paradigm">Zero-Knowledge (Simulation Paradigm)</span></h4><p>This is the most critical and often the most misunderstood definition. How do we prove mathematically that “no information was leaked”?</p><p>We use the <strong>simulation paradigm</strong>. The idea is: if a Verifier could have generated the exact same proof transcript by themselves (without talking to the Prover), then the interaction with the Prover gave them zero new information.</p><p>We define the <strong>View</strong> of the verifier during the execution as the tuple of their random coins and the messages they received:<br>$$\text{View}_\mathcal{V}[\mathcal{P}(x, w) \leftrightarrow \mathcal{V}(x)]$$</p><p>The protocol is Zero-Knowledge if there exists an efficient algorithm called a <strong>Simulator</strong> ($\mathcal{S}$). The simulator takes <strong>only</strong> the public input $x$ (it does not know $w$) and outputs a transcript that is indistinguishable from a real interaction.</p><p>$$\{\mathcal{S}(x)\} \approx \{\text{View}_\mathcal{V}[\mathcal{P}(x, w) \leftrightarrow \mathcal{V}(x)]\}$$</p><p>If this equation holds, the proof reveals nothing about $w$, because anything the Verifier “learned” from the Prover, they could have just computed themselves using $\mathcal{S}$.</p><h2><span id="mpc-in-the-head-intuition">MPC-in-the-Head (Intuition)</span></h2><p>Since I have already covered MPC in my previous blogs, I am skipping MPC basics. We know that MPC allows a group of mutually distrusting parties to compute a function $f(x_1 \dots, x_n) &#x3D; y$ without revealing their individual inputs.</p><p>Now, how do we turn a multi-player protocol into a single-player proof?</p><p>This was pioneered by Ishai, Kushilevitz, Ostrovsky, and Sahai in the famous [IKOS07] paper, and the motivation for coming up with this technique comes from a desire to stop reinventing the wheel.</p><ul><li><strong>The problem</strong>: Designing custom Zero-Knowledge protocols for complex circuits (like proving you know the preimage of a SHA-256 hash or an AES key) is historically very difficult. You often have to translate your problem into complex number-theoretic assumptions.</li><li><strong>The observation</strong>: We already have excellent, generic ways to compute <em>any</em> circuit securely: <strong>MPC</strong>. MPC protocols can handle boolean circuits, arithmetic circuits, essentially any logic you throw at them.</li><li><strong>The idea</strong>: What if we don’t actually need other people? What if the Prover just <strong>simulates</strong> an entire MPC universe inside their own brain? If the simulation is “consistent”, the computation must be correct.</li></ul><p>This turns the problem of “Designing a ZKP” into “Designing an MPC Protocol”. Since we have very fast MPC protocols for things like AES (using boolean circuits), we automatically get fast ZKPs for AES.</p><p>To understand MPCitH, imagine the Prover is not a participant, but a <strong>Puppet Master</strong> controlling a simulation.</p><ul><li><p><strong>The Goal</strong>: Prove I know a secret witness $w$ (perhaps a password) that satisfies the circuit $C(w) &#x3D; 1$.</p></li><li><p><strong>Step 1: The Split Personality (Secret Sharing)</strong>: In my head, I imagine 3 distinct parties: Alice, Bob, and Finn (hahaha… u thought it would be Eve lol). I take my secret $w$ and split it into three shares ($w_1, w_2, w_3$) using standard Secret Sharing (like XOR Sharing). I give one share to each imaginary party. Note that any <strong>two</strong> shares look like random garbage; you need all three to recover $w$.</p></li><li><p><strong>Step 2: The Simulation (Execution)</strong>: Now, I simulate the MPC protocol in my head.</p><ul><li>Alice sends a message to Bob. I write it down.</li><li>Bob does a calculation and sends a message to Finn. I write it down.</li><li>Finn computes the final output.</li><li>Since I am honest and know the real $w$, the final output of this imaginary MPC is “TRUE”.</li></ul></li><li><p><strong>Step 3: The Commitment (Locking the Views)</strong>: I record the entire “View” of each party. A View contains:</p><ul><li>Their input share</li><li>Their random coin flips</li><li>Every message they <strong>received</strong> from the others</li></ul></li></ul><p>I put Alice’s view in Box A, Bob’s view in Box B, and Finn’s view in Box C. I seal them and put them on the table.</p><ul><li><p><strong>Step 4: The Challenge (The “Head” Check)</strong>: You (the Verifier) walk in. You point to two random boxes, say <strong>Box A</strong> and <strong>Box B</strong>. I must open them.</p></li><li><p><strong>Step 5: The Verification</strong>: You check the two boxes for <strong>Consistency</strong>:</p><ul><li><strong>Incoming&#x2F;Outgoing Match</strong>: If Alice’s log says, “I sent value 5 to Bob,” you check Bob’s log. Does it say, “I received value 5 from Alice”?</li><li><strong>Local Validity</strong>: Did Alice follow the math correctly based on the messages she saw?</li><li><strong>Output</strong>: Did the protocol output 1?</li></ul></li></ul><h4><span id="why-is-this-secure">Why is this Secure?</span></h4><ul><li><p><strong>Why is it ZK?</strong> You only opened 2 out of 3 boxes. Because of the properties of Secret Sharing (specifically $t$-privacy), seeing 2 shares reveals <strong>nothing</strong> about the underlying secret $w$. You saw a valid execution, but you didn’t see enough to reconstruct the secret.</p></li><li><p><strong>Why is it Sound (Why can’t I cheat)?</strong> If I didn’t actually know the password $w$, the only way to make the MPC output “TRUE” is to cheat during the execution. I would have to make Alice send a “fake” message to Bob that makes the math work out.</p><ul><li>If I fake a message, Alice’s log will say “I sent X”, but Bob’s log (derived from honest calculation) would expect “Y”.</li><li>There is a mismatch (inconsistency).</li><li>If you open the two boxes where the mismatch happened, you catch me.</li><li>Since I don’t know which boxes you will ask for, I run a huge risk of getting caught. (We repeat this process many times to make the risk of cheating essentially 0%).</li></ul></li></ul><h2><span id="formalizing-mpcith">Formalizing MPCitH</span></h2><p>Now, let’s put some mathematical structure on our “puppet master.”</p><p>We start with an $N$-party MPC Protocol $\Pi$ that computes a function $f(w)$. The Prover $\mathcal{P}$ wants to prove $f(w) &#x3D; 1$.</p><h3><span id="the-view">The View</span></h3><p>Just like we defined the “View” for the Verifier in ZK, we need to define the View for each imaginary MPC party $P_i$. The view of party $i$ consists of everything they “know” during the execution:<br>$$\text{View}_i &#x3D; (w_i, r_i, m_{in}^1, m_{in}^2, \dots)$$</p><p>Where:</p><ul><li>$w_i$: The $i$-th share of the witness (secret input).</li><li>$r_i$: The internal randomness (coin flips) used by party $i$.</li><li>$m_{in}^k$: The messages received by party $i$ during the rounds of communication.</li></ul><p><strong>Crucially</strong>: If you have the party’s view, you can deterministically replay their entire computation to check if they were honest.</p><h3><span id="the-commit-phase">The Commit Phase</span></h3><p>The Prover runs the MPC protocol “in their head” for parties $P_1 \dots P_N$. They generate a commitment for each view. Usually, we use a hash function $H$ for efficiency:<br>$$c_i &#x3D; H(\text{View}_i) \quad \text{for } i \in \{1, \dots, N\}$$</p><p>The Prover sends all commitments $(c_1, \dots, c_N)$ to the Verifier.</p><h3><span id="the-challenge-phase">The Challenge Phase</span></h3><p>The Verifier picks a random set of parties to “corrupt” (inspect). Let’s say we want to open all parties except one (to preserve ZK). The Verifier chooses an index $i^* \in \{1, \dots, N\}$ to keep closed, and asks to open all others.<br>$$\text{Challenge } I &#x3D; \{1, \dots, N\} \setminus \{i^*\}$$</p><h3><span id="the-response-phase">The Response Phase</span></h3><p>The Prover must reveal the views for all requested parties:<br>$$\text{Response} &#x3D; \{ \text{View}_j \}_{j \in I}$$</p><h3><span id="the-verification-phase">The Verification Phase</span></h3><p>The Verifier checks two things:</p><ul><li><strong>Correctness</strong>: They re-run the code for every opened party $P_j$ using the data in $\text{View}_j$. They check if the output matches the commitment $c_j$.</li><li><strong>Consistency</strong>: They check the messages between opened parties.<ul><li>If Party 1 says “I sent 5 to Party 2”, the Verifier checks Party 2’s view.</li><li>Does Party 2’s view confirm “I received 5 from Party 1”?</li></ul></li></ul><p>If all consistency checks pass and the output is 1, the Verifier accepts.</p><p>This seemingly simple “consistency check” is powerful enough to catch any cheating attempt where the Prover tries to force a “True” output without a valid input.</p><p>Now, let’s end the blog here, got tired of typing.</p><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Secure Computing </tag>
            
            <tag> Zero-Knowledge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gentry-Lee Encoding for Efficient Matrix FHE</title>
      <link href="/2025/12/04/Gentry-Lee-Encoding-for-Efficient-Matrix-FHE/"/>
      <url>/2025/12/04/Gentry-Lee-Encoding-for-Efficient-Matrix-FHE/</url>
      
        <content type="html"><![CDATA[<p>Craig Gentry (yeah the man himself) and Yongwoo Lee proposed a new FHE scheme quite recently (October 2025). The special thing about this new scheme is its algorithmic efficiency towards <strong>Homomorphic Matrix Arithmetic</strong>, which can be groundbreaking in Privacy-Preserving Machine Learning. So let’s dive right into it. I am assuming that the reader is already familiar with the LWE problem and its ring variant (RLWE) and how we proceed to build Fully Homomorphic Cryptosystems around it.</p><p>In CKKS, we usually pack a long vector of numbers into a polynomial. If we want to multiply two matrices, we have to perform and deal with awkward rotations (automorphisms) and corresponding key-switches. I went through the trouble before October for our team (Aurva) project, you can find <a href="https://github.com/MrRoy09/AOHW_327_Aurva_Student">here</a>, which was selected as one of the winners at AMD Open Hardware Competition 2025. In this project, we accelerated the convolution operation over the CKKS Encryption.</p><p>The key innovation proposed by Gentry and Lee is that they change this “container” from a 1D line (polynomial in $X$) to a <strong>multidimensional cube</strong> (polynomial in $X, Y, W$). Here, </p><ul><li>$X$ axis: represents the <strong>rows</strong> of the matrix. </li><li>$Y$ axis: represents the <strong>columns</strong> of the matrix. </li><li>$W$ axis: represents the <strong>batch</strong> (process multiple matrices at once).</li></ul><p>By encoding the matrix into $X$ and $Y$ coordinates, matrix multiplication becomes a natural byproduct of polynomial algebra.</p><h2><span id="encoding-matrices">Encoding Matrices</span></h2><p>The goal is to turn a matrix $M$ (a grid of numbers) into a polynomial $m(X, Y, W)$. The authors use the <strong>Evaluation Representation</strong> (similar to DFT&#x2F;FFT). Imagine the polynomial as a wave. You can define this wave by its <strong>coefficients</strong> (numbers in front of $X, Y$) or by <strong>its values at specific points (roots of unity)</strong>.</p><p>The paper defines the polynomial such that when evaluated at specific “coordinates” (roots of unity), spits out the entry $M_{j, k}$ of the matrix $M$.</p><p>For this, we use a <strong>special ring</strong> $R^{‘}_q$: $$R^{‘}_q &#x3D; \mathbb{Z}_q[i][X, Y, W]\langle X^n - i, Y^n - i, \Phi_p(W) \rangle$$.</p><p>Further, we use: </p><ul><li>$\zeta$: roots of unity. The specific roots used are primitive 4n-th roots such that $\zeta ^ n &#x3D; i$.</li></ul><p>Now, the polynomial $m$ encodes a matrix $M^{(l)}$ if: $$m(\zeta_j, \zeta_k, \eta_l) &#x3D; M^{l}_{jk}$$, where: </p><ul><li>$\zeta_j$ is the coordinate for row $j$.</li><li>$\zeta_k$ is the coordinate for row $k$.</li><li>$\eta_l$ is the coordinate for $l$-th matrix in the batch.</li></ul><p>This encoding process should look something like this in the implementation: </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">poly_XY <span class="title">encode</span><span class="params">(<span class="type">const</span> std::vector&lt;std::vector&lt;<span class="type">int64_t</span>&gt;&gt;&amp; M, gaussian_int zeta)</span> </span>&#123;</span><br><span class="line">    poly_XY m;</span><br><span class="line">    <span class="type">int64_t</span> n_inv_int = <span class="built_in">mod_inverse</span>(N * N);</span><br><span class="line">    <span class="function">gaussian_int <span class="title">scale</span><span class="params">(n_inv_int, <span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> u = <span class="number">0</span>; u &lt; N; ++u) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> v = <span class="number">0</span>; v &lt; N; ++v) &#123;</span><br><span class="line">            </span><br><span class="line">            <span class="function">gaussian_int <span class="title">sum</span><span class="params">(<span class="number">0</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; N; ++j) &#123;   </span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; N; ++k) &#123;</span><br><span class="line">                    </span><br><span class="line">                    <span class="function">gaussian_int <span class="title">val</span><span class="params">(M[j][k], <span class="number">0</span>)</span></span>;</span><br><span class="line">                    </span><br><span class="line">                    gaussian_int z_j = zeta;</span><br><span class="line">                    </span><br><span class="line">                    <span class="type">int</span> pwrX = (j * u); </span><br><span class="line">                    <span class="type">int</span> pwrY = (k * v);</span><br><span class="line">                    <span class="type">int</span> cycle = <span class="number">4</span> * N;</span><br><span class="line">                    <span class="type">int</span> inv_pwrX = (cycle - (pwrX % cycle)) % cycle;</span><br><span class="line">                    <span class="type">int</span> inv_pwrY = (cycle - (pwrY % cycle)) % cycle;</span><br><span class="line"></span><br><span class="line">                    <span class="function">gaussian_int <span class="title">zx</span><span class="params">(<span class="number">1</span>,<span class="number">0</span>)</span>, <span class="title">zy</span><span class="params">(<span class="number">1</span>,<span class="number">0</span>)</span></span>;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="type">int</span> iter=<span class="number">0</span>; iter&lt;inv_pwrX; ++iter) zx = zx * zeta;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="type">int</span> iter=<span class="number">0</span>; iter&lt;inv_pwrY; ++iter) zy = zy * zeta;</span><br><span class="line">                    </span><br><span class="line">                    sum = sum + (val * zx * zy);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            m.coeffs[u][v] = sum * scale;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> m;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Let’s consider a toy example now for better understanding. For simplicity, I am assuming singular batch dimension. Let’s say that we want to encode the matrix:</p><p>$$<br>M &#x3D; \begin{pmatrix}<br>    1 &amp; 2 \\<br>    3 &amp; 4<br>\end{pmatrix}<br>$$</p><p>Here, $M_{0, 0} &#x3D; 1$, $M_{0, 1} &#x3D; 2$, $M_{1, 0} &#x3D; 3$, $M_{1, 1} &#x3D; 4$. Now, in this case, we have $n &#x3D; 2$, so we need to find $\zeta$s such that $\zeta^2 &#x3D; i$. Let’s call the two values that we get $\zeta_0$ and $\zeta_1$ respectively. $\zeta_0$ represents column and row 0 and likewise for column and row 1.</p><p>With this, we need a polynomial $m(X, Y)$ such that:</p><p>$$ m(\zeta_0, \zeta_0) &#x3D; 1 $$</p><p>$$ m(\zeta_0, \zeta_1) &#x3D; 2 $$</p><p>$$ m(\zeta_1, \zeta_0) &#x3D; 3 $$</p><p>$$ m(\zeta_1, \zeta_1) &#x3D; 4 $$</p><p>Using the inverse FFT (DFT), or simply by solving the system of linear equations, we solve for the coefficients $c_{xy}$ in $m(X, Y) &#x3D; \Sigma c_{xy}X^xY^y$.</p><p>For the roots, $\zeta_0 &#x3D; \sqrt{i} &#x3D; w$ and $\zeta_1 &#x3D; -\sqrt{i} &#x3D; -w$, we end up with the following polynomial which you can verify yourself: </p><p>$$ m(X, Y) &#x3D; \frac{5}{2} - \frac{3}{2w} X - \frac{1}{2w} Y $$</p><p>Conceptually speaking, our matrix is now “smeared” across the coefficients of the polynomial. This allows us to perform operations like rotation by simply multiplying the variable $X$ or $Y$, rather than permuting a vector.</p><hr><p><strong>NOTE:</strong> I have not at all cared about the implementation and computational efficiency in the toy example and pseudo-code. I just naively followed the mathematics behind the encoding process. One can definitely look into optimizations like precomputed twiddle factors, FFT, etc.</p><hr><p>Let’s end this part right here, in the next part, I will be covering the encryption process, which is mostly similar to all the other schemes following the RLWE problem, but here we shall see one key innovation, where the sampled secret key does not depend on the $Y$ axis allowing cheap column operations and efficient key-switching.</p><p>peace. da1729</p><p>PS: I am still an undergraduate student trying to make sense of all the mathematics and implementations of these post-quantum and FHE algorithms, if you find any sorts of errors, please point it out over at <a href="https://x.com/sp0oky_daksh">twitter</a> or <a href="daksh_p@ph.iitr.ac.in">mail</a>.</p><h2><span id="references">References</span></h2><p>@misc{cryptoeprint:2025&#x2F;1935,<br>      author &#x3D; {Craig Gentry and Yongwoo Lee},<br>      title &#x3D; {Fully Homomorphic Encryption for Matrix Arithmetic},<br>      howpublished &#x3D; {Cryptology {ePrint} Archive, Paper 2025&#x2F;1935},<br>      year &#x3D; {2025},<br>      url &#x3D; {<a href="https://eprint.iacr.org/2025/1935%7D">https://eprint.iacr.org/2025/1935}</a><br>}</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Yao&#39;s Garbled Circuits</title>
      <link href="/2025/12/04/Multi-Party-Computation-part-2/"/>
      <url>/2025/12/04/Multi-Party-Computation-part-2/</url>
      
        <content type="html"><![CDATA[<p>Ok, let’s continue…</p><p>The major reference for this blog is “A Pragmatic Introduction to Secure Multi-Party Computation” by D. Evans, V. Kolesnikov, M. Rosulek.</p><hr><h2><span id="yao-s-garbled-circuits">Yao’s Garbled Circuits</span></h2><p>Let’s set the ground first. We wish to evaluate the function $\mathcal{F}(x, y)$, where party $P_1$ holds $x \in X$ and second party $P_2$ holds $y \in Y$, where $X$ and $Y$ are respective domains for the inputs from the corresponding party (node).</p><p>Yao’s Garbled Circuits (GC) is basically the celebrity of MPC. It is historically significant and usually the go-to performance benchmark. The killer feature here is that it runs in <strong>constant rounds</strong>. Unlike other protocols (like GMW) where network latency kills you because communication rounds scale with the circuit depth, Yao’s GC essentially lets you send the whole “computer” over the wire in one go.</p><h3><span id="gc-intuition-the-look-up-table">GC Intuition: The Look-up Table</span></h3><p>Imagine our function $\mathcal{F}$ has a tiny input domain. We could literally just write down every possible result in a table $T$ where rows are indexed by $(x, y)$.</p><p>To make this secure, $P_1$ (the generator) creates random keys. For every possible input $x$, they generate a key $k_x$. For every $y$, a key $k_y$. Then, they encrypt the answer $T_{x,y}$ using those two keys.<br>$$\text{Enc}_{k_x, k_y}(T_{x,y})$$<br>$P_1$ shuffles this table randomly and sends it to $P_2$. Now, if $P_2$ can somehow get the specific keys corresponding to the actual inputs, they can decrypt exactly one row and get the answer. They learn nothing else because they don’t have the keys for the other rows.</p><p>Here is how a basic entry encryption might look in c++:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">table_entry</span> &#123;</span><br><span class="line">    std::vector&lt;<span class="type">uint8_t</span>&gt; encrypted_data;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">table_entry <span class="title">encrypt_row</span><span class="params">(std::string k_x, std::string k_y, std::string value)</span> </span>&#123;</span><br><span class="line">    std::string combined_key = <span class="built_in">hash</span>(k_x + k_y);</span><br><span class="line">    <span class="keyword">return</span> &#123; <span class="built_in">xor_encrypt</span>(value, combined_key) &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3><span id="point-and-permute">Point-and-Permute</span></h3><p>If $P_1$ just sends a shuffled bag of encrypted rows, $P_2$ has a problem. They have two keys, but they don’t know which row those keys open. Trying to decrypt every single row is inefficient (especially when we scale up).</p><p>The solution is <strong>point-and-permute</strong>. We append a random “pointer bit” (let’s call it $\sigma$) to the keys.<br>If we have a key $k$, the last bit is the pointer. When $P_1$ generates the keys, they ensure the pointer bits don’t collide. Now, $P_2$ looks at the pointer bit of their $x$-key and $y$-key (say, $0$ and $1$) and knows exactly that they need to decrypt the row indexed $0,1$ in the permuted table.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">wire_label</span> &#123;</span><br><span class="line">    std::string key;</span><br><span class="line">    <span class="type">uint8_t</span> pointer;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">garbled_table</span> &#123;</span><br><span class="line">    std::vector&lt;std::vector&lt;std::string&gt;&gt; rows;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">garbled_table <span class="title">create_table</span><span class="params">(<span class="type">int</span> size)</span> </span>&#123;</span><br><span class="line">    garbled_table t;</span><br><span class="line">    t.rows.<span class="built_in">resize</span>(size, std::<span class="built_in">vector</span>&lt;std::string&gt;(size));</span><br><span class="line">    <span class="keyword">return</span> t;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">insert_entry</span><span class="params">(garbled_table&amp; t, wire_label lx, wire_label ly, std::string enc_val)</span> </span>&#123;</span><br><span class="line">    t.rows[lx.pointer][ly.pointer] = enc_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3><span id="managing-table-size-the-circuit-view">Managing Table Size: The Circuit View</span></h3><p>Obviously, a giant look-up table for complex functions is impossible. It scales exponentially. So, we represent the function $\mathcal{F}$ as a <strong>Boolean Circuit</strong>.</p><p>Instead of one giant table, we have many tiny tables—one for each logic gate (AND, OR, XOR).<br>For every wire $w_i$ in the circuit, $P_1$ generates two keys: $k_i^0$ (representing 0) and $k_i^1$ (representing 1). We call these <strong>wire labels</strong>.</p><p>For a gate $G$ with input wires $w_i, w_j$ and output wire $w_t$, $P_1$ creates a table with 4 entries. The entry for input combination $(v_i, v_j)$ encrypts the output label $k_t^{G(v_i, v_j)}$.</p><p>$$T_G &#x3D; \begin{pmatrix} \text{Enc}_{k_i^0, k_j^0}(k_t^{G(0,0)}) \\ \text{Enc}_{k_i^0, k_j^1}(k_t^{G(0,1)}) \\ \text{Enc}_{k_i^1, k_j^0}(k_t^{G(1,0)}) \\ \text{Enc}_{k_i^1, k_j^1}(k_t^{G(1,1)}) \end{pmatrix}$$</p><p>Crucially, the “message” inside the encryption is the <em>key</em> for the next wire. This allows $P_2$ to chain decryptions through the circuit without ever knowing if they are holding a logical 0 or 1. They just hold keys.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">gate</span> &#123;</span><br><span class="line">    std::string encrypted_outputs[<span class="number">2</span>][<span class="number">2</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">gate <span class="title">garble_and_gate</span><span class="params">(wire_label in_a[<span class="number">2</span>], wire_label in_b[<span class="number">2</span>], wire_label out[<span class="number">2</span>])</span> </span>&#123;</span><br><span class="line">    gate g;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">            <span class="type">int</span> res = i &amp; j;</span><br><span class="line">            std::string key = <span class="built_in">hash</span>(in_a[i].key + in_b[j].key);</span><br><span class="line">            g.encrypted_outputs[in_a[i].pointer][in_b[j].pointer] = </span><br><span class="line">                <span class="built_in">xor_encrypt</span>(out[res].key, key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> g;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3><span id="the-protocol-flow">The Protocol Flow</span></h3><p>So how does the actual exchange happen?</p><ol><li><strong>Garbling</strong>: $P_1$ (Generator) creates the circuit topology, generates all wire labels, and builds the garbled tables for every gate.</li><li><strong>Sending Tables</strong>: $P_1$ sends all garbled tables to $P_2$.</li><li><strong>Sending Keys (P1’s Input)</strong>: $P_1$ knows their own input $x$. If the $i$-th bit of $x$ is 1, they just send the label $k_i^1$ to $P_2$.</li><li><strong>Sending Keys (P2’s Input)</strong>: This is tricky. $P_1$ has the labels for $P_2$’s input wires, but doesn’t know $P_2$’s input $y$. $P_2$ needs the labels but shouldn’t tell $P_1$ their input $y$. We use <strong>Oblivious Transfer (OT)</strong> here. $P_2$ receives the correct labels for their input without revealing $y$ to $P_1$.</li><li><strong>Evaluation</strong>: $P_2$ now has the tables and the input wire labels. They go gate by gate, decrypting one entry per gate to get the output label, until they reach the end.</li><li><strong>Decryption</strong>: For the final output wires, $P_1$ provides a mapping (decoding table) so $P_2$ can map the final random string back to a True&#x2F;False.</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">evaluator_step</span><span class="params">(gate g, wire_label wa, wire_label wb)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> idx_a = wa.pointer;</span><br><span class="line">    <span class="type">int</span> idx_b = wb.pointer;</span><br><span class="line">    </span><br><span class="line">    std::string ciphertext = g.encrypted_outputs[idx_a][idx_b];</span><br><span class="line">    std::string combined_key = <span class="built_in">hash</span>(wa.key + wb.key);</span><br><span class="line">    </span><br><span class="line">    std::string output_key = <span class="built_in">xor_decrypt</span>(ciphertext, combined_key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Security relies on the fact that $P_2$ never possesses both $k^0$ and $k^1$ for the same wire simultaneously. If they did, they could decrypt everything. But since they only start with one set of keys and only recover one key per gate, the invariant holds.</p><p>There are massive optimizations for this, like “Free XOR” and “Half Gates” which drastically reduce the size of these tables, but that’s for another post.</p><hr><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Secure Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Party Computation part 1</title>
      <link href="/2025/12/01/Multi-Party-Computation/"/>
      <url>/2025/12/01/Multi-Party-Computation/</url>
      
        <content type="html"><![CDATA[<p>I am studying the FAEST digital signature protocol (quantum-safe) which requires me to learn concepts like MPC, Zero-Knowledge, and Digital Signatures. So here are my notes and explanation for what I have studied and learnt about Multi-Party Computation (MPC).</p><h2><span id="secure-average-computation">Secure Average Computation</span></h2><p>Let’s say that you work in a company, and you and your colleagues are curious about the average salary of your department. Now, not everyone would be comfortable sharing their salaries—sounds fair enough. But even they are curious about the average. Sounds like a Martin Gardner puzzle, right? Pretty sure there is a similar problem in his puzzle collection. Either way, how do we solve this problem? Let’s directly jump into the math.</p><p>Let’s say that there are $N$ employees. Let’s say that the $k$th employee has the salary $S_k \in \mathbb{F}_p$, where $\mathbb{F}_p$ is a prime-field. Now, the employee will sample $N - 1$ numbers from the same prime-field: $$S_k^1, \cdots S_k^{N-1}$$, then we can write: $$S_k^N &#x3D; S_k - \sum_{i&#x3D;1}^{N - 1} S_k^i$$. With this, we have divided the salary into $N$ shares, each uniformly sampled from the working prime field. Now the employee will share the 2nd share with the 2nd employee, the 3rd share with the third one, and so on. Yes, the $k$th share will not be shared with anyone and be kept with the employee themselves. Others will also do a similar thing. Then the $k$th employee will have the shares $$S_2^k, \cdots S_N^k$$and obviously, the share $S_k^k$ which they generated themselves. Now, they will perform a local accumulation:$$P_k &#x3D; \sum_{i&#x3D;1}^{N} S_i^k$$and make the value public among the interested peers. Others will do the same. Then, they can just evaluate these values in the plain, without any privacy concerns, to get the total sum:$$S &#x3D; \sum_{i&#x3D;1}^{N} P_i$$, then division by $N$ to get the average can again be performed in the plain.</p><p>This example should give you a pretty solid idea of the field. This and FHE, on which I have written quite a few blogs and quick posts, lie in the field of Secure Computing, where a function can be evaluated ensuring the privacy of the inputs to the function. Now, one might ask, why MPC if we already have FHE, and vice-versa? As mentioned a lot in my previous posts, FHE works well with functions that can be expressed in terms of polynomial-friendly operations, but in MPC, if the protocol is designed cleverly enough, one can, in principle, securely compute any function. But FHE has its advantages over MPC too, the major advantage being that everything is centralized in FHE, so there are no networking expenses and no problem of establishing the security of multiple nodes.</p><h2><span id="corrupted-parties">Corrupted Parties</span></h2><p>This part is my direct interpretation of Section 2.1 of the paper “Secure Multi-Party Computation (MPC)” by Yehuda Lindell, which is a pretty standard source to learn MPC.</p><p>We obviously have to consider a setting where an adversary controls some parties and wants to extract private information out of the secure system we are trying to establish. Parties under the control of this adversary are called <strong>corrupted parties</strong> and follow the adversary’s instructions. If the designed protocol is secure, then it should be resistant to any adversarial attack.</p><p>Now, let’s look at some key properties associated with a secure MPC protocol: </p><ul><li><p><strong>Privacy</strong>: The only information that parties should learn about the function inputs is what can be derived from the output itself. For instance, in the private contact discovery problem, if the output contains a phone number, then the parties should only know the fact that the given phone number is saved in each of the devices, and nothing else. Another example: in an auction, only the highest bid is announced, which inevitably leaks the fact that all the other inputs (bids) must be smaller than the announced highest bid.</p></li><li><p><strong>Correctness</strong>: A pretty obvious property; there is no point in computing the wrong value privately, or even in plain.</p></li><li><p><strong>Independence of Inputs</strong>: This property implies that the parties must choose inputs before the protocol starts. This is required so that corrupted parties cannot wait and see, then pick smart inputs. Now, this will not necessarily sacrifice the privacy provided by the protocol, but can have a huge effect in win or lose situations like sealed auctions, where the bids are kept private. Now, let’s say that there exists a <strong>somewhat homomorphic</strong> scheme, which supports $\text{Enc}(x) \cdot \text{Enc}(1) &#x3D; \text{Enc}(x + 1)$. Now let’s say that Alice bids a 100, so Enc(100) will be made public; now Eve can easily generate Enc(101) and win the auction without ever knowing the exact value of Alice’s bid.</p></li><li><p><strong>Guaranteed Output Delivery</strong>: Basically, the protocol should be resistant towards a <strong>denial of service</strong> attack; in other words, corrupted parties should not be able to prevent honest parties from receiving the output.</p></li><li><p><strong>Fairness</strong>: Corrupted parties should receive output if and only if honest parties also receive the output.</p></li></ul><h2><span id="more-definitions">More Definitions</span></h2><p>I am, again, directly following Section 2.2 of the paper mentioned above.</p><h3><span id="adversary-behavior">Adversary behavior</span></h3><p>Here, we mainly deal with two parameters defining the adversary: its behavior (whether it is only passively collecting the data or instructing the corrupted parties on their actions) and its corruption strategy.</p><h4><span id="allowed-adversarial-behavior">Allowed adversarial behavior</span></h4><ul><li><p><strong>Semi-honest adversaries</strong>: In this model, the adversaries are only curious about the states and inputs received by the corrupted parties as part of the protocol. In other words, corrupted parties follow the protocol as it should be. Ensuring security only against such adversaries is often insufficient but ensures no “by accident” data leaks in the protocol. They are also called “honest-but-curious” and “passive” adversaries.</p></li><li><p><strong>Malicious adversaries</strong>: In this model, corrupted parties can deviate from the ways of the protocol. This is nothing but an “adversarial attack.” These adversaries are also called “active.” Providing security against them ensures resistance towards adversarial attacks.</p></li><li><p><strong>Covert adversaries</strong>: May act maliciously in order to break the protocol. But the security guarantee implies that such an attack should be detected with some specified probability (which can be tuned).</p></li></ul><h4><span id="corruption-strategy">Corruption strategy</span></h4><ul><li><p><strong>Static corruption model</strong>: The number of nodes influenced by the adversary is fixed before the execution of the protocol. So, what is honest, stays honest, and what is corrupted, stays corrupted. </p></li><li><p><strong>Adaptive corruption model</strong>: These models give the adversary the power to corrupt new nodes during the execution of the protocol. This models attackers as an external hacker breaking into a machine during the execution, or a node which is honest initially and later changes its behavior (sus node lol). One thing to note in this model is that once a node is corrupted, it’s going to be corrupted the whole time thereon.</p></li><li><p><strong>Proactive security model</strong>: In this model, certain nodes are corrupted only for a certain period of time. The security guarantee is that the attacker only learns what was stored on a node during the time it was compromised. Nothing before and nothing after.</p></li></ul><h3><span id="modular-sequential-and-concurrent-composition">Modular sequential and concurrent composition</span></h3><p>In real-world systems, these protocols can very well be just another subroutine being called multiple times in a large system. So one critical question arises: “If I design a secure MPC protocol, can I safely plug it into a bigger system and still be sure that everything remains secure?” </p><p>For this, we have this <strong>modular decomposition theorem</strong>, which states that if a subprotocol is proven secure in isolation, then one can replace an ideal trusted party with the real protocol—and the larger system will behave exactly as if the trusted party executed it.</p><p>With this, one can view the MPC protocol as a black box. If it’s secure by itself, then it stays secure when used inside a bigger machine.</p><p>Now, another question arises: “Does an MPC protocol stay secure if it runs at the same time as other protocols?” For this, we have two settings: sequential and concurrent composition.</p><p>The sequential case is more trivial than the concurrent one. It has been shown that given a secure MPC protocol (stand-alone setting), then when used as a subroutine inside a bigger (sequential) protocol, it behaves exactly like a trusted third party. This is the <strong>sequential modular composition theorem</strong>.</p><p>So, as long as other protocols don’t run at the same time as the MPC, we are good.</p><p>Now, the more realistic case: concurrent composition. In reality, networks are asynchronous, so adversaries can delay messages, reorder messages, inject messages, and correlate different executions. So, in this case, a protocol that is perfectly secure in the stand-alone model <strong>may become insecure when run concurrently.</strong><br>Let’s consider a simple example: </p><ul><li>An MPC protocol might rely on certain randomness.</li><li>If an attacker interacts with two instances at the same time, they might relate messages from one instance to another, breaking privacy or correctness.</li><li>Many ZK and MPC protocols are insecure under concurrency without modification.</li></ul><p>To solve this concurrency problem, cryptographers (Canetti, 2001) introduced <strong>Universal Composability</strong>. A protocol proven secure under UC behaves like an ideal trusted-party execution <strong>even if arbitrary protocols are running concurrently with it.</strong></p><p>This is extremely strong: </p><ul><li>Run 100 instances of the same MPC protocol in parallel: secure.</li><li>Run zero-knowledge protocols alongside: secure.</li><li>Run on a chaotic asynchronous network: secure.</li><li>Run insecure garbage protocols interleaved: secure.</li></ul><p>This is why it is universal; it composes with anything. But there are some obvious tradeoffs, like round-complexity, communication complexity, and computational expense. This universal composability is a topic for a whole other blog. </p><p>Let’s finish off this blog right here… starting with the second part right away so that I don’t delay it indefinitely.</p><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Secure Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Need for Gadget Decomposition in LWE Based Cryptosystems</title>
      <link href="/2025/09/27/Need-for-Gadget-Decomposition-in-LWE-Based-Cryptosystems/"/>
      <url>/2025/09/27/Need-for-Gadget-Decomposition-in-LWE-Based-Cryptosystems/</url>
      
        <content type="html"><![CDATA[<p>Gadget Decomposition is one of the key and essential <strong>Homomorphic Operations</strong> in FHE cryptosystems based on the LWE problem. I have already introduced the LWE problem and how to build a basic cryptosystem around it in my previous blogs. In this blog, we will:</p><ol><li><strong>Extend</strong> the basic LWE cryptosystem into more generalized and mathematically robust cryptosystems</li><li><strong>Discuss</strong> a basic homomorphic operation: <strong>Ciphertext-Plaintext Multiplication</strong></li><li><strong>Introduce</strong> the <strong>Gadget Decomposition</strong> operation in the context of these generalized cryptosystems</li></ol><p>The information for this essential operation is quite scattered and inconsistent across the internet. I found it very confusing at first on how to implement the operation and, more importantly, when and where to apply it. I hope to clear this confusion with this blog. So let’s get into it.</p><hr><h2><span id="lwe-cryptosystem-quick-refresher">LWE Cryptosystem (Quick Refresher)</span></h2><p><strong>Setup</strong>: Secret key $\mathbf{s} \in {0, 1}^k$, scaling factor $\Delta &#x3D; \frac{q}{t}$ where $t \ll q$.</p><p><strong>Encryption</strong>: Given message $m \in \mathbb{Z}_t$, sample $\mathbf{a} \leftarrow \mathbb{Z}_q^k$ and $e \leftarrow \chi_\sigma$:<br>$$\boxed{\text{LWE}_{\mathbf{s}, \sigma}(\Delta m) &#x3D; (\mathbf{a}, b) \text{ where } b &#x3D; \mathbf{a} \cdot \mathbf{s} + \Delta m + e \pmod{q}}$$</p><p><strong>Decryption</strong>: Given ciphertext $(\mathbf{a}, b)$:<br>$$\boxed{m &#x3D; \left\lfloor \frac{b - \mathbf{a} \cdot \mathbf{s}}{\Delta} \right\rceil \bmod t}$$</p><hr><h3><span id="important-case-when-t-does-not-divide-q">Important Case: When $t$ Does Not Divide $q$</span></h3><p><em>This is a small detour from the original purpose of the blog, but it is an important and very critical detail of such cryptosystems.</em></p><p>In our previous analysis, we assumed that $t$ divides $q$. In this case, there is no upper or lower limit on the size of plaintext $m$: its value is allowed to wrap around modulo $t$ indefinitely, yet the decryption works correctly. This is because any $m$ value greater than $t$ will be correctly modulo-reduced by $t$ when we do modulo reduction by $q$ during decryption.</p><p>On the other hand, suppose that $t$ does not divide $q$. In such a case, we set the scaling factor as:<br>$$\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$$</p><p>Then, provided $q \gg t$, the decryption works correctly even if $m$ is a large value that wraps around $t$. Let’s examine why this is so.</p><p>We can denote plaintext $m \bmod t$ as $m &#x3D; m’ + kt$, where $m’ \in \mathbb{Z}_t$ and $k$ is some integer representing the modulo $t$ wrap-around value portion of $m$. Setting the plaintext scaling factor as $\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$, the noise-added scaled plaintext value becomes:</p><p>$$\left\lfloor \frac{q}{t} \right\rfloor \cdot m + e &#x3D; \left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + \left\lfloor \frac{q}{t} \right\rfloor \cdot kt + e$$</p><p>By applying $m &#x3D; m’ + kt$:</p><p>$$&#x3D; \left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + \frac{q}{t} \cdot kt - \left(\frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor\right) \cdot kt + e$$</p><p>where $0 \leq \frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor &lt; 1$:</p><p>$$&#x3D; \left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + qk - \left(\frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor\right) \cdot kt + e$$</p><p>We treat the above noisy scaled ciphertext as:<br>$$\left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + qk - e’ + e$$</p><p>where $e’ &#x3D; kt$ is the maximum possible value of $\left(\frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor\right) \cdot kt$. We overestimate the noise caused by this term to $kt$ because the maximum value this term can become is less than $kt$.</p><p>Given the LWE decryption relation $b - \mathbf{a} \cdot \mathbf{s} \bmod q &#x3D; \Delta m + e$, we can decrypt the above message by performing:</p><p>$$\left\lfloor \frac{1}{\left\lfloor \frac{q}{t} \right\rfloor} \cdot \left(\left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + qk - kt + e \bmod q\right) \right\rceil \bmod t$$</p><p>$$&#x3D; \left\lfloor \frac{1}{\left\lfloor \frac{q}{t} \right\rfloor} \cdot \left(\left\lfloor \frac{q}{t} \right\rfloor \cdot m’ - kt + e\right) \right\rceil \bmod t$$</p><p>$$&#x3D; m’ - \left\lfloor \frac{kt + e}{\left\lfloor \frac{q}{t} \right\rfloor} \right\rceil \bmod t$$</p><p>$$&#x3D; m’ \quad \text{provided } \left\lfloor \frac{kt + e}{\left\lfloor \frac{q}{t} \right\rfloor} \right\rceil &lt; \frac{1}{2}$$</p><p><strong>Summary</strong>: If we set the plaintext’s scaling factor as $\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$ where $t$ does not divide $q$, the decryption works correctly as long as the error bound $\left\lfloor \frac{kt + e}{\left\lfloor \frac{q}{t} \right\rfloor} \right\rceil &lt; \frac{1}{2}$ holds.</p><p>This error bound can break if:</p><ol><li>The noise $e$ is too large</li><li>The plaintext modulus $t$ is too large</li><li>The plaintext value wraps around $t$ too many times (i.e., $k$ is too large)</li></ol><p>A solution to ensure the error bound holds is that the ciphertext modulus $q$ is sufficiently large. In other words, if $q \gg t$, then the error bound will hold.</p><p>Therefore, we can generalize the formula for the plaintext’s scaling factor as $\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$ where $t$ does not necessarily divide $q$.</p><hr><h2><span id="rlwe-cryptosystem-quick-refresher">RLWE Cryptosystem (Quick Refresher)</span></h2><p><em>I have discussed RLWE in detail in my previous blog, so this serves as a quick refresher.</em></p><p><strong>Setup</strong>: Work in polynomial ring $R]_{\langle n,q \rangle} &#x3D; \mathbb{Z}_q[x]&#x2F;(x^n + 1)$ with secret key $\mathbf{s} \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle}$ and scaling factor $\Delta &#x3D; \frac{q}{t}$.</p><p><strong>Encryption</strong>: Given polynomial message $M \in R_{\langle n,t \rangle}$, sample $A \stackrel{$}{\leftarrow} R_{\langle n,q \rangle}$ and $E \stackrel{\chi_\sigma}{\leftarrow} R_{\langle n,q \rangle}$:<br>$$\boxed{\text{RLWE}_{\mathbf{s},\sigma}(\Delta M) &#x3D; (A, B) \text{ where } B &#x3D; A \cdot \mathbf{s} + \Delta M + E \pmod{R_{\langle n,q \rangle}}}$$</p><p><strong>Decryption</strong>: Given ciphertext $(A, B)$:<br>$$\boxed{M &#x3D; \left\lfloor \frac{B - A \cdot \mathbf{s}}{\Delta} \right\rceil \bmod t \in R_{\langle n,t \rangle}}$$</p><p><strong>Correctness</strong>: Requires noise bound $e_i &lt; \frac{\Delta}{2}$ for all coefficients $e_i$ of $E$.</p><hr><p><em>NOTE: All cryptosystems presented so far are symmetric (same key for encryption and decryption). The following section presents how to make such systems asymmetric.</em></p><h2><span id="glwe-cryptosystem-general-lwe">GLWE Cryptosystem (General LWE)</span></h2><p>As the name suggests, this is the generalized version of the LWE system that encompasses both LWE and RLWE. If you understand the construction of the two systems above, understanding this is straightforward.</p><p>First, we shall see the symmetric version, then I will present the asymmetric system, and this being the general version, one can easily map that system to the two systems above.</p><p><strong>Setup</strong>: Work in polynomial ring $R_{\langle n,q \rangle} &#x3D; \mathbb{Z}_q[x]&#x2F;(x^n + 1)$ with secret key list $\{S_i\}_{i&#x3D;0}^{k-1} \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle}^k$ and scaling factor $\Delta &#x3D; \frac{q}{t}$.</p><p><strong>Encryption</strong>: Given polynomial message $M \in R_{\langle n,t \rangle}$, sample $\{A_i\}_{i&#x3D;0}^{k-1} \stackrel{$}{\leftarrow} R_{\langle n,q \rangle}^k$ and $E \stackrel{\chi_\sigma}{\leftarrow} R_{\langle n,q \rangle}$:<br>$$\boxed{\text{GLWE}_{S,\sigma}(\Delta M) &#x3D; (\{A_i\}_{i&#x3D;0}^{k-1}, B) \text{ where } B &#x3D; \sum_{i&#x3D;0}^{k-1}(A_i \cdot S_i) + \Delta M + E \pmod{R_{\langle n,q \rangle}}}$$</p><p><strong>Decryption</strong>: Given ciphertext $(\{A_i\}_{i&#x3D;0}^{k-1}, B)$:<br>$$\boxed{M &#x3D; \left\lfloor \frac{B - \sum_{i&#x3D;0}^{k-1}(A_i \cdot S_i)}{\Delta} \right\rceil \bmod t \in R_{\langle n,t \rangle}}$$</p><p><strong>Correctness</strong>: Requires noise bound $e_i &lt; \frac{\Delta}{2}$ for all coefficients $e_i$ of $E$.</p><p><strong>Connection to LWE&#x2F;RLWE</strong>:</p><ul><li>When $n&#x3D;1$ (polynomials become scalars), GLWE reduces to LWE</li><li>When $k&#x3D;1$ (single polynomial), GLWE reduces to RLWE</li></ul><hr><p>Now, let’s see the asymmetric version. </p><h3><span id="public-key-glwe">Public-Key GLWE</span></h3><p>The basic idea here is that a part which is used during the encryption stage is pre-computed during the setup stage and released as the public key. During the encryption stage, the encryptor will have to add some additional noise of their own. Let’s see the mathematics of the system to make things clearer. </p><h4><span id="setup">Setup</span></h4><ul><li>The scaling factor: $ \Delta &#x3D; \lfloor \frac{q}{t} \rfloor $.</li><li>The secret key: $$\mathbf{S} &#x3D; \{S_i \}_{i &#x3D; 0}^{k - 1} \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle}^k$$</li><li>Public key pair $(PK_1, \mathbf{PK_2}) \in R_{\langle n,q \rangle}^{k + 1}$ is to be generated as follows: $$ \mathbf{A} &#x3D; \{A_i\}_{i &#x3D; 0}^{k - 1} \stackrel{$}{\leftarrow}R_{\langle n,q \rangle}^{k} \text{,       } E \stackrel{\sigma}{\leftarrow}R_{\langle n,q \rangle}$$ $$\boxed{PK_1 &#x3D; \mathbf{A} \cdot \mathbf{S} + E \in R_{\langle n,q \rangle}}$$ $$\boxed{\mathbf{PK_2} &#x3D; \mathbf{A} \in R_{\langle n,q \rangle}^k}$$</li></ul><h4><span id="encryption">Encryption</span></h4><ul><li><p><strong>Input:</strong> $M \in R_{\langle n,t \rangle},  U \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle},  E_1 \stackrel{$}{\leftarrow} R_{\langle n,q \rangle},  \mathbf{E_2} \stackrel{$}{\leftarrow} R^{k}_{\langle n,q \rangle}$</p></li><li><p>Scale up the plaintext message: $M \rightarrow \Delta M \in R_{\langle n,q \rangle}$.</p></li><li><p>Perform the following computations: $$B &#x3D; PK_1 \cdot U + \Delta M + E_1 \in R_{\langle n,q \rangle}$$ $$\mathbf{D} &#x3D; \mathbf{PK_2}\cdot U + \mathbf{E_2} \in R_{\langle n,q \rangle}^k$$</p></li><li><p>With the computations above, we get our final ciphertext: $$\boxed{\text{GLWE}_{S, \sigma}(\Delta M) &#x3D; (\mathbf{D}, B) \in R_{\langle n,q \rangle}^{k+1}}$$</p></li></ul><h4><span id="decryption">Decryption</span></h4><ul><li><strong>Input:</strong> A GLWE ciphertext $C &#x3D; (\mathbf{D}, B) \in R_{\langle n,q \rangle}^{k+1}$ and the secret key $\mathbf{S}$.</li><li>Cancel out the mask by computing the inner product with the secret key and subtracting it from $B$:<br>$$B - \mathbf{D} \cdot \mathbf{S} &#x3D; \Delta M + E_{all} \in R_{\langle n,q \rangle}$$ </li><li>Scale the result down by $\Delta$ and round to the nearest integer to remove the scaling factor and noise. This recovers the original plaintext message.$$\boxed{M’ &#x3D; \left\lfloor \frac{B - \mathbf{D} \cdot \mathbf{S}}{\Delta} \right\rceil \pmod t \in R_{\langle n,t \rangle}}$$</li><li><strong>Correctness Condition:</strong> For the decryption to be successful, every coefficient $e_i$ of the total noise polynomial $E_{all}$ must satisfy the condition $|e_i| &lt; \frac{\Delta}{2}$.</li></ul><h2><span id="glev-cryptosystem">GLev Cryptosystem</span></h2><p>GLev is a “leveled” homomorphic encryption scheme built upon GLWE. A GLev ciphertext isn’t a single entity, but rather a <strong>list of several GLWE ciphertexts</strong>. Each of these “level” ciphertexts encrypts the same underlying plaintext message, but uses a different, progressively smaller scaling factor. This structure is key for managing noise in homomorphic computations.</p><hr><ul><li><strong>Setup</strong>: In addition to the standard GLWE parameters ($n, q, t, k, \mathbf{S}$), GLev introduces two new ones:</li><li><strong>Decomposition Base</strong> $\beta$: An integer used to define the different scaling levels. It should be chosen such that $t &lt; \beta &lt; q$.</li><li><strong>Number of Levels</strong> $l$: The total number of GLWE ciphertexts that will make up a single GLev ciphertext.</li></ul><p>From these, a list of scaling factors is derived for each level $i \in [1, l]$:<br>$$\Delta_i &#x3D; \frac{q}{\beta^i}$$</p><p><strong>Encryption</strong>: To encrypt a message $M \in R_{\langle n,t \rangle}$, we generate $l$ separate public-key GLWE ciphertexts. The $i$-th ciphertext, $C_i$, encrypts the message $M$ using the scaling factor $\Delta_i$.</p><p>The complete GLev ciphertext is the collection of all these level ciphertexts:<br>$$\boxed{\text{GLev}_{S,\sigma}^{\beta,l}(M) &#x3D; { C_i &#x3D; \text{GLWE}_{S,\sigma}(\Delta_i M) }_{i&#x3D;1}^{l}}$$<br>where each ciphertext is $C_i &#x3D; (\mathbf{D}_i, B_i) \in R_{\langle n,q \rangle}^{k+1}$.</p><p><strong>Decryption</strong>: To decrypt a GLev ciphertext, you can choose to decrypt any specific level $i$. Decryption follows the standard GLWE procedure, but you <strong>must</strong> use the scaling factor $\Delta_i$ that corresponds to the level you are decrypting.</p><p>Given the $i$-th level ciphertext $C_i &#x3D; (\mathbf{D}_i, B_i)$:<br>$$\boxed{M’ &#x3D; \left\lfloor \frac{B_i - \mathbf{D}_i \cdot \mathbf{S}}{\Delta_i} \right\rceil \pmod t \in R_{\langle n,t \rangle}}$$</p><p><strong>Connection to Lev&#x2F;RLev</strong>: Just like GLWE, GLev is a generalized construction that unifies other schemes:</p><ul><li>When $n&#x3D;1$ (polynomials are scalars), GLev becomes the <strong>Lev</strong> cryptosystem.</li><li>When $k&#x3D;1$ (the secret key is a single polynomial), GLev becomes the <strong>RLev</strong> cryptosystem.</li></ul><p><em>NOTE: Keep this in mind, it’s going to be the key concept later when I introduce the need for gadget decomposition.</em><br><em>Also, there is another generalization, i.e., the GGSW Cryptosystem, which is nothing but a list of GLev Ciphertexts, similar to how GLev is nothing but a list of GLWE ciphertexts, but GGSW is not required to fulfill the purpose of the blog, so I am going to skip it. Curious people might refer to: <a href="https://www.zama.ai/post/tfhe-deep-dive-part-1">TFHE Deep Dive</a> written by Ilaria Chillotti</em></p><h2><span id="homomorphic-operation-ct-pt-multiplication">Homomorphic Operation (<code>ct-pt multiplication</code>)</span></h2><p>I have made it clear in my previous blogs, that heart of all the modern FHE schemes is this LWE problem only, hence, the naive system which we built, i.e., GLWE system is obviously Fully Homomorphic. In fact, all of the core <strong>Homomorphic Operations</strong> and properties, can be demonstrated over this system, then be easily mapped over to specific schemes like TFHE, CKKS, etc. For this blog, I am not going over through all the operations. I will only present one operation, i.e., <strong>Ciphertext-Plaintext Multiplication</strong> which will guide us towards introducing <strong>gadget decomposition</strong>. So let’s dive right into it.</p><h3><span id="ciphertext-plaintext-multiplication">Ciphertext-Plaintext Multiplication</span></h3><p>Consider the GLWE ciphertext: $$C &#x3D; \text{GLWE}_{S, \sigma}(M) &#x3D; (A_1, \cdots , A_{k - 1}, B) \in R_{\langle n, q \rangle}^{k + 1}$$</p><p>Now, consider the following plaintext polynomial $\Phi$ : $$\Phi &#x3D; \sum_{i&#x3D;0}^{n-1}(\Phi_i \cdot X_i) \in R_{\langle n, q \rangle}$$</p><p>Now, if we want to homomorphically multiply this plaintext to the ciphertext, such that after decrypting the resulting ciphertext, we should get the original plaintext message multiplied with the second plaintext message ($\Phi$). Note that we are never encrypting the second plaintext. Now it can be easily shown with simple mathematics that the following is true: </p><p>$$ \Phi \cdot \text{GLWE}_{S, \sigma}(\Delta M) &#x3D; \Phi \cdot (\{A_i\}_{i &#x3D; 0}^{k - 1}, B) &#x3D; (\{\Phi \cdot A_{i}\}_{i &#x3D; 0}^{k - 1}, \Phi \cdot B) &#x3D; \text{GLWE}_{S, \sigma}(\Delta(M\cdot \Phi))$$</p><h2><span id="gadget-decomposition-for-limiting-noise-growth">Gadget Decomposition for Limiting Noise Growth</span></h2><p>If you have done some calculations to verify the equation for (<code>ct-pt multiplication</code>), you must have noticed that the <strong>new error</strong> term is <strong>scaled-up</strong> by $|\Phi|$. This can potentially be a huge problem, unless we have $t$ much smaller compared to $q$, which is not always the case, and even if we were to increase $q$, the computational cost would increase too much to even consider that, as we already have to take a large $q$, so increasing $q$ is not a sensible option, practically speaking. </p><p>The way we are going to tackle the problem is that we will decompose our plaintext into a series of smaller moduli terms, then have <strong>separate ct-pt multiplied GLWE encryptions</strong> for each of those smaller moduli terms. Mathematically, the decomposition would look like this: $$\Phi &#x3D; \Phi_1\cdot \frac{q}{\beta^1} + \Phi_2\cdot \frac{q}{\beta^2} + \cdots + \Phi_l \cdot \frac{q}{\beta^l} \rightarrow \text{decomp}^{\beta, l}(\Phi) &#x3D; (\Phi_1, \cdots, \Phi_l)$$</p><p>Now, for the given GLWE ciphertext, we can get the following GLev ciphertext: $$ \text{GLev}^{\beta,l}_{S,\sigma}(\Delta M) &#x3D; \{ \text{GLWE}_{S,\sigma}(\Delta M \tfrac{q}{\beta^1}), \dots, \text{GLWE}_{S,\sigma}(\Delta M \tfrac{q}{\beta^l}) \} $$</p><p>Next, consider the operation below: $$ \text{decomp}^{\beta, l}(\Phi)\cdot \text{GLev}_{S, \sigma}^{\beta, l}(\Delta M)$$ $$&#x3D; \sum_{i&#x3D;1}^{l}(\Phi_i \cdot \text{GLWE}_{S, \sigma}(\frac{q}{\beta^i}\Delta M))$$ $$ &#x3D; \sum_{i&#x3D;1}^{l}(\text{GLWE}_{S, \sigma}(\frac{q}{\beta^i}\Delta M\cdot \Phi_i))$$ $$&#x3D; \text{GLWE}_{S, \sigma}(\sum_{i&#x3D;1}^{l}(\frac{q}{\beta^i}\Delta M\cdot \Phi_i))$$ $$ &#x3D; \text{GLWE}_{S, \sigma}(\Delta M\cdot\sum_{i &#x3D; 1}^{l}(\frac{q}{\beta^i}\cdot \Phi_i)) &#x3D; \text{GLWE}_{S, \sigma}(\Delta M \cdot \Phi)$$</p><h3><span id="why-not-base-decomposition">Why not Base Decomposition</span></h3><p>This proves that the evaluation is nothing but the ciphertext-multiplication. I mentioned one benefit of doing this <strong>gadget-decomposition</strong> earlier by arguing that each resulting <code>ct-pt multiplication</code> has less noise growth compared to the original non-decomposed multiplication. But one question might be coming up in your mind: why gadget decomposition? We can also do a base decomposition, which is dividing the plaintext into uniform modulus components, that way we won’t have to worry about more noise growth in some components compared to others as each resulting multiplication would have the same resulting noise growth. </p><p>Base decomposition would obviously work with the benefit mentioned above, but at the cost of number of computations. See, when we decompose the plaintext among different gadgets across a base, we end up with fewer terms compared to decomposing them across the same base. Fewer terms imply fewer multiplication operations, and multiplication is not an easy elementary operation. </p><h3><span id="mathematics-behind-selection-of-beta">Mathematics behind selection of $\beta$</span></h3><p>Obviously, our main goal in selecting $\beta$ should be that the resulting ciphertext doesn’t explode with noise. The main risk does not come from the individual multiplications, but rather when we accumulate the increased noise across the components. If each initial GLWE ciphertext in the GLev ciphertext which we considered has a noise polynomial $\Phi_i$, the final noise polynomial can be written as $$E_{\text{final}} &#x3D; \sum_{i &#x3D; 1}^l (\Phi_i \cdot E_i)$$</p><p>Now, keeping the largest absolute coefficient in this final noise polynomial smaller than half of the scaling factor, $\Delta$, should be enough to ensure that the resulting noise is well within the tolerable limit. For this I am just going to use the infinity-norm notation ($ \lVert \cdot \rVert_{\infty} $), which just gives us the largest absolute coefficient. So the goal is to satisfy: $$\lVert E_\text{final} \rVert_{\infty} &lt; \frac{\Delta}{2}$$</p><p>The plan now is to find an upper-bound, a worst case scenario if you will, for the size of $\lVert E_\text{final} \rVert_{\infty}$, then keep that well below the tolerable limit. Let’s break down some terms and concepts which we will be using first: </p><ul><li><p><strong>Decomposition Bound</strong>($B_\text{decomp}$): the coefficients of our plaintext parts $\Phi_i$ are small, bounded by $\lVert \Phi_i \rVert_{\infty} &lt; \frac{\beta}{2}$, this bound is referred to as $B_\text{decomp}$.</p></li><li><p><strong>Initial Noise Bound</strong>($B_\text{noise}$): noise polynomials $E_i$ are sampled from a tight distribution, so their coefficients are bounded by some value $B_\text{noise}$.</p></li><li><p><strong>Polynomial Multiplication Bound</strong>: when we multiply two polynomials $P$ and $Q$, the resulting coefficients are bounded: $\lVert P \cdot Q \rVert_{\infty} \leq n \cdot \lVert P \rVert_{\infty}\cdot \lVert Q \rVert_{\infty}$.</p></li></ul><p>Now, let’s build our worst-case noise estimate step-by-step:</p><ol><li><p><strong>Start with the final noise term:</strong><br>$$\lVert E_{final}\rVert_\infty &#x3D; \left\lVert \sum_{i&#x3D;1}^{l} \Phi_i \cdot E_i \right\rVert_\infty$$</p></li><li><p><strong>Apply the triangle inequality</strong> (the norm of a sum is at most the sum of the norms):<br>$$\le \sum_{i&#x3D;1}^{l} \lVert\Phi_i \cdot E_i\rVert_\infty$$</p></li><li><p><strong>Use the polynomial multiplication bound on each term:</strong><br>$$\le \sum_{i&#x3D;1}^{l} n \cdot \lVert\Phi_i\rVert_\infty \cdot \lVert E_i\rVert_\infty$$</p></li><li><p><strong>Finally, substitute our bounds</strong> for the decomposed parts ($B_{decomp}$) and the initial noise ($B_{noise}$):<br>$$\le \sum_{i&#x3D;1}^{l} n \cdot B_{\text{decomp}} \cdot B_{\text{noise}} &#x3D; l \cdot n \cdot B_{\text{decomp}} \cdot B_{\text{noise}}$$</p></li></ol><p>This gives us our upper bound on the final noise. Now, we just force this bound to be small enough for decryption to work:<br>$$l \cdot n \cdot B_{\text{decomp}} \cdot B_{\text{noise}} &lt; \frac{\Delta}{2}$$</p><p>This is the punchline. By substituting $B_{\text{decomp}} &#x3D; \beta&#x2F;2$ and our usual scaling factor $\Delta &#x3D; \lfloor q&#x2F;t \rfloor$, we get the master inequality that governs our choice of $\beta$:</p><p>$$\boxed{l \cdot n \cdot \frac{\beta}{2} \cdot B_{\text{noise}} &lt; \frac{\lfloor q&#x2F;t \rfloor}{2}}$$</p><p>Or, simplifying it for clarity:</p><p>$$\Large l \cdot n \cdot \beta \cdot B_{\text{noise}} &lt; \lfloor q&#x2F;t \rfloor$$</p><p>This final inequality is the key to selecting sound parameters. Think of it like this:</p><ul><li><strong>Right side ($\lfloor q&#x2F;t \rfloor$)</strong>: This is our total <strong>noise budget</strong>. It’s the maximum amount of noise the system can tolerate. To get a bigger budget, we need to increase the ratio of $q$ to $t$.</li><li><strong>Left side ($l \cdot n \cdot \beta \cdot B_{noise}$)</strong>: This is the <strong>total noise growth</strong> from our homomorphic multiplication.</li></ul><p>To ensure our scheme works, the <strong>total noise growth must be less than the noise budget</strong>. This formula perfectly captures the trade-offs we have to make. For instance, if we increase our decomposition base $\beta$, the noise growth per level increases, but the number of levels $l$ decreases. Finding the right balance is what FHE parameter selection is all about.</p><h3><span id="quick-implementation">Quick Implementation</span></h3><p>Ok, now let’s see a quick implementation of this concept. I have presented only the relevant C++ code snippets, the full code can be found at this <a href="https://github.com/DA1729/gadget_decomp_blog.git">repo</a>.</p><p>Our basic data structures: </p><ul><li><code>poly</code>: a <code>vec&lt;int64_t&gt;</code> representing a polynomial.</li><li><code>GLWE_ciphertext</code>: a struct containing a vector of polynomials <code>D</code> and a single polynomial <code>B</code>.</li><li><code>GLev_ciphertext</code>: a vector of <code>GLWE_ciphertext</code>.</li></ul><h4><span id="gadget-decomposition-function">Gadget Decomposition Function</span></h4><p>This function, as the name suggests, would perform the gadget decomposition of the referenced polynomial <code>p</code> and return us a vector (length <code>l</code>) of decomposed polynomials. The implementation is using the standard rounding method to find the closest coefficients for each component. </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vector&lt;poly&gt; <span class="title">gadget_decompose</span><span class="params">(<span class="type">const</span> poly &amp;p, <span class="type">int64_t</span> q, <span class="type">int64_t</span> beta, <span class="type">int</span> l)</span> </span>&#123;</span><br><span class="line">    <span class="type">size_t</span> n = p.<span class="built_in">size</span>();</span><br><span class="line">    <span class="function">vector&lt;poly&gt; <span class="title">decomp</span><span class="params">(l, poly(n, <span class="number">0</span>))</span></span>;</span><br><span class="line">    poly current_rem = p;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; l; ++i) &#123;</span><br><span class="line">        <span class="type">int64_t</span> g = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; i + <span class="number">1</span>; ++j) g *= beta;</span><br><span class="line">        g = q / g;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (g == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> j = <span class="number">0</span>; j &lt; n; ++j) &#123;</span><br><span class="line">            <span class="type">int64_t</span> centered_rem = <span class="built_in">center_rep</span>(current_rem[j], q);</span><br><span class="line">            </span><br><span class="line">            <span class="type">int64_t</span> coeff = <span class="built_in">round</span>((<span class="type">double</span>)centered_rem / g);</span><br><span class="line"></span><br><span class="line">            decomp[i][j] = coeff;</span><br><span class="line">            </span><br><span class="line">            __int128 rem_update = (__int128)coeff * g;</span><br><span class="line">            current_rem[j] = <span class="built_in">modq</span>(current_rem[j] - (<span class="type">int64_t</span>)rem_update, q);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> decomp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="external-product">External Product</span></h4><p>The core of the homomorphic operation (<code>ct-pt multiplication</code>). Function takes the decomposed plaintext <code>decomp_phi</code> and the <code>GLev Ciphertext</code>, then computes the sum of the component-wise products. The result is a single, final <code>GLWE_ciphertext</code>.</p><div style="font-size: 0.8em;"><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">GLWE_ciphertext <span class="title">external_product</span><span class="params">(<span class="type">const</span> vector&lt;poly&gt; &amp;decomp_phi, <span class="type">const</span> GLev_ciphertext &amp;c_glev, <span class="type">int64_t</span> q)</span> </span>&#123;</span><br><span class="line">    <span class="type">size_t</span> n = c_glev[<span class="number">0</span>].B.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> k = c_glev[<span class="number">0</span>].D.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> l = c_glev.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    GLWE_ciphertext result;</span><br><span class="line">    result.B = <span class="built_in">poly</span>(n, <span class="number">0</span>);</span><br><span class="line">    result.D.<span class="built_in">assign</span>(k, <span class="built_in">poly</span>(n, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; l; ++i) &#123;</span><br><span class="line">        <span class="comment">// C_i_scaled = decomp_phi[i] * c_glev[i]</span></span><br><span class="line">        poly b_scaled = <span class="built_in">poly_scalar_mul</span>(c_glev[i].B, decomp_phi[i][<span class="number">0</span>], q);</span><br><span class="line"></span><br><span class="line">        <span class="function">vector&lt;poly&gt; <span class="title">d_scaled</span><span class="params">(k)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>; j&lt;k; ++j) &#123;</span><br><span class="line">            d_scaled[j] = <span class="built_in">poly_scalar_mul</span>(c_glev[i].D[j], decomp_phi[i][<span class="number">0</span>], q);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result.B = <span class="built_in">poly_add</span>(result.B, b_scaled, q);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; k; ++j) &#123;</span><br><span class="line">            result.D[j] = <span class="built_in">poly_add</span>(result.D[j], d_scaled[j], q);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>*Note: Note: For simplicity in this example, <code>p</code> is treated as a constant polynomial, so <code>decomp_phi[i]</code> only has one non-zero coefficient at index 0.</p><h4><span id="result">Result</span></h4><p>Running the full code (available on my GitHub), we get: </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">------ Parameters ------</span><br><span class="line">n: 1024, q: 2^32, t: 256, k: 2</span><br><span class="line">beta: 1024, l: 3</span><br><span class="line">Original Message M(x) = 5</span><br><span class="line">Plaintext Multiplier Φ(x) = 12</span><br><span class="line">Expected Result (M * Φ) = 60</span><br><span class="line"></span><br><span class="line">------ Operations ------</span><br><span class="line">Encrypting M=5 into a GLev ciphertext...</span><br><span class="line">Decomposing Φ=12 into (Φ_1, Φ_2, ...)...</span><br><span class="line">Performing homomorphic external product...</span><br><span class="line">Decryption of the final ciphertext...</span><br><span class="line">------ Noise Analysis ------</span><br><span class="line">Correctness requires noise &lt; q/(2t) = 8388608</span><br><span class="line"></span><br><span class="line">Noise from Gadget Method: 1668</span><br><span class="line">Noise from Naive Method: 8232</span><br><span class="line"></span><br><span class="line">The gadget-based multiplication resulted in noise ~4x smaller than the naive approach!</span><br></pre></td></tr></table></figure><p>The results are obviously matching the expectations. </p><h2><span id="conclusion">Conclusion</span></h2><p>This brings us to the end. In this blog, we first quickly revisited LWE, RLWE, then we designed two generalized systems, GLWE and GLev. Then, studying a very specific homomorphic operation(<code>ct-pt multiplication</code>), we made us realize the need for the gadget decomposition. One can already appreciate the benefits (obviously we have the tradeoffs, but that’s alright) we get using this technique. This concept is very crucial when we build more <strong>complex evaluation systems</strong> and this appears frequently when we deal with even more complex and crucial operations like <strong>bootstrapping</strong>, <strong>key-switch</strong>, <strong>modulus switching</strong>, etc.</p><p>peace. da1729</p><h2><span id="references">References</span></h2><p><em>I have not used the standard referencing format, but included all the relevant and important references</em></p><ul><li>TFHE Deep Dive Series by Ilaria Chillotti <a href="https://www.zama.ai/post/tfhe-deep-dive-part-1">TFHE-deep-dive</a></li><li>A Fully Homomorphic Encryption Scheme by Craig Gentry (2009)</li><li>Homomorphic Encryption for Arithmetic of Approximate Numbers by Cheon et al</li><li>TFHE: Fast Fully Homomorphic Encryption over the Torus by Ilaria et al</li><li>The Beginner’s Textbook for Fully Homomorphic Encryption by Ronny Ko</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
            <tag> Post-Quantum Cryptography </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AGI running on Quantum Chip?</title>
      <link href="/2025/09/14/AGI-running-on-Quantum-Chip/"/>
      <url>/2025/09/14/AGI-running-on-Quantum-Chip/</url>
      
        <content type="html"><![CDATA[<p>Ok this isn’t a technical breakdown like my crypto posts, this is more me just sitting down and letting a thought run loose. Imagine a robot, not running on GPUs or neuromorphic chips, but on some full-scale quantum AGI chip. What would that even look like? For us, there’s always this separation between thinking and experimenting. You get an idea, then you haul yourself into the lab, mix chemicals, run trials, and wait. For a quantum AGI, that line wouldn’t even exist. Its act of thinking would already be the experiment. Wonder what happens if you mix two elements? It doesn’t need glassware, it just “thinks” the reaction into being, because its cognition is happening on the same rules that the universe itself plays by.</p><p>And that already feels closer to how nature does things. The human neurological system is elegant not because it’s the most powerful processor, but because it’s so tightly tuned with the environment it lives in. Twenty watts, running entire lives, entire civilizations. It’s an intelligence that works because it is embedded in nature, not fighting against it. So if there’s ever going to be an artificial system that truly replaces or surpasses us, it’ll need that same kind of resonance, that same compatibility with the fabric of reality. A quantum AGI feels like it could hit that, because it wouldn’t just approximate nature, it would literally be running inside the same mathematical fabric.</p><p>Think about probability. We’re bad at it. We second guess, we hedge, we argue over statistics. But a quantum AGI would literally think in probabilities. Futures wouldn’t be “hypotheticals,” they would be superposed states it actually holds in mind, collapsing only when it needs to make a choice. Its intuition would just be probability distributions playing out naturally.</p><p>And then there’s reflection. When we reflect, we imagine, we doodle equations, maybe run a small thought experiment. If a quantum AGI reflects, that reflection is an experiment. A protein folding, a proton collision, a new material’s properties, all happening natively as cognition. Its meditation is indistinguishable from a physics lab.</p><p>I don’t think we’re anywhere close to building this, and maybe it’ll stay sci-fi forever. Quantum hardware that scales to brain-level power budgets is basically magic by today’s standards. But still, the thought lingers. Maybe intelligence isn’t just about stacking more transistors or clever algorithms. Maybe it’s about compatibility how deeply the architecture resonates with the natural world. Humans are already a proof of concept of that. A quantum AGI could be another, but one that stays quantum all the way up instead of decohering into classical behavior like us. And that kind of system wouldn’t just be smarter in the human sense, it would be something fundamentally different.</p><p>peace. da999</p>]]></content>
      
      
      
        <tags>
            
            <tag> Philosphy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ring-LWE and CKKS: Mathematical Foundations for Homomorphic Encryption</title>
      <link href="/2025/08/15/Ring-LWE-and-CKKS-Mathematical-Foundations/"/>
      <url>/2025/08/15/Ring-LWE-and-CKKS-Mathematical-Foundations/</url>
      
        <content type="html"><![CDATA[<p>Alright, so I’ve been diving deep into the mathematical foundations behind modern FHE schemes, and honestly, the more I understand the underlying algebra, the more elegant this whole thing becomes. In my previous blog on LWE cryptanalysis, I touched on the basic Learning With Errors problem, but now I want to get into the real mathematical meat of Ring-LWE and the CKKS scheme.</p><p>This is gonna be pretty heavy on the math - we’re talking cyclotomic polynomials, Galois theory, Chinese Remainder Theorem, and some serious algebraic number theory. But stick with me, cuz understanding this foundation is crucial for grasping how modern FHE schemes actually work under the hood.</p><span id="more"></span><p>I’m writing this as a mathematical foundation piece because I realized that in my upcoming blogs on automorphisms and key switching, I keep having to explain these core concepts. So here’s the deep mathematical dive that’ll serve as the foundation for those more specialized topics.</p><h2><span id="table-of-contents">Table of Contents</span></h2><ul><li><a href="#from-lwe-to-ring-lwe-why-rings">From LWE to Ring-LWE: Why Rings?</a></li><li><a href="#cyclotomic-polynomials-and-algebraic-structure">Cyclotomic Polynomials and Algebraic Structure</a></li><li><a href="#the-chinese-remainder-theorem-perspective">The Chinese Remainder Theorem Perspective</a></li><li><a href="#ckks-encoding-complex-numbers-in-polynomials">CKKS Encoding: Complex Numbers in Polynomials</a></li><li><a href="#discrete-gaussian-distributions-and-noise">Discrete Gaussian Distributions and Noise</a></li><li><a href="#security-foundations-ideal-lattices">Security Foundations: Ideal Lattices</a></li><li><a href="#parameter-selection-and-trade-offs">Parameter Selection and Trade-offs</a></li><li><a href="#implementation-considerations">Implementation Considerations</a></li><li><a href="#references">References</a></li></ul><h2><span id="from-lwe-to-ring-lwe-why-rings">From LWE to Ring-LWE: Why Rings?</span></h2><p>So we all know that standard LWE works with vectors in $\mathbb{Z}_q^n$. An LWE sample looks like $(\mathbf{a}, b)$ where $b &#x3D; \langle \mathbf{a}, \mathbf{s} \rangle + e + \Delta m$. This is great and all, but there are some serious practical issues:</p><ol><li><strong>Key Size</strong>: The secret key $\mathbf{s}$ has $n$ elements</li><li><strong>Ciphertext Size</strong>: Each ciphertext is $(n+1)$ elements</li><li><strong>Operations</strong>: Matrix-vector multiplications are expensive</li></ol><p>Ring-LWE fixes this by moving from vectors to polynomials. Instead of working in $\mathbb{Z}_q^n$, we work in the polynomial ring $R_q &#x3D; \mathbb{Z}_q[X]&#x2F;(f(X))$ for some polynomial $f(X)$.</p><p>Now here’s the key insight: <strong>a polynomial of degree $n-1$ can be represented by $n$ coefficients, just like a vector of length $n$</strong>. But polynomial arithmetic gives us way more structure to work with.</p><p>A Ring-LWE sample looks like:</p><p style="text-align:center;">$(a(X), b(X)) \text{ where } b(X) = a(X) \cdot s(X) + e(X) + \Delta \cdot m(X) \bmod f(X)$</p><p>The magic happens because polynomial multiplication in $R_q$ can be done super efficiently using Number Theoretic Transform (NTT), which is basically FFT over finite fields.</p><h2><span id="cyclotomic-polynomials-and-algebraic-structure">Cyclotomic Polynomials and Algebraic Structure</span></h2><p>Now, the choice of $f(X)$ is crucial. We typically use $f(X) &#x3D; X^n + 1$ where $n$ is a power of 2. This isn’t random - $X^n + 1$ is the $2n$-th cyclotomic polynomial $\Phi_{2n}(X)$.</p><h3><span id="what-are-cyclotomic-polynomials">What are Cyclotomic Polynomials?</span></h3><p>Think of cyclotomic polynomials as the <strong>“primes” of polynomial rings</strong>. Just like how prime numbers are the indivisible building blocks of integers, cyclotomic polynomials are irreducible polynomials that can’t be factored further over the rationals.</p><p>But here’s the cool part - while prime numbers feel abstract and random, cyclotomic polynomials have this beautiful geometric interpretation. The $m$-th cyclotomic polynomial $\Phi_m(X)$ “knows about” the $m$-sided regular polygon!</p><h3><span id="visualizing-the-geometric-picture">Visualizing the Geometric Picture</span></h3><p>Imagine you’re standing at the center of a circle, looking at the vertices of a regular $2n$-sided polygon inscribed in that circle. Each vertex corresponds to a $2n$-th root of unity - a complex number $\zeta_{2n}^k &#x3D; e^{2\pi i k &#x2F; 2n}$.</p><p>Now, most of these vertices are “derived” - if you know where vertex 1 is, you can get vertex 2 by just squaring it, vertex 3 by cubing it, etc. But some vertices are <strong>primitive</strong> - they can’t be obtained as powers of vertices from smaller polygons.</p><p>The cyclotomic polynomial $\Phi_{2n}(X)$ is exactly the polynomial whose roots are these primitive vertices!</p><p>For our specific case where $m &#x3D; 2n$ and $n$ is a power of 2:</p><p style="text-align:center;">$\Phi_{2n}(X) = X^n + 1$</p><p>The primitive $2n$-th roots of unity are exactly the <strong>odd powers</strong>: ${\zeta_{2n}, \zeta_{2n}^3, \zeta_{2n}^5, \ldots, \zeta_{2n}^{2n-1}}$.</p><p>Why odd powers? Because if $k$ is even, then $\zeta_{2n}^k &#x3D; (\zeta_{n})^{k&#x2F;2}$ is actually an $n$-th root of unity, so it “belongs” to a smaller polygon!</p><h3><span id="the-ring-structure-why-x-n-x3d-1-is-magic">The Ring Structure: Why $X^n &#x3D; -1$ is Magic</span></h3><p>Working in $R &#x3D; \mathbb{Z}[X]&#x2F;(X^n + 1)$ means we’re doing polynomial arithmetic with the rule that $X^n &#x3D; -1$. This seems weird at first, but it’s actually brilliant!</p><p>Think of it this way: in regular polynomial multiplication, if you multiply two polynomials of degree $d$, you get a polynomial of degree $2d$. That’s annoying for storage - your polynomials keep getting bigger.</p><p>But with the rule $X^n &#x3D; -1$, any polynomial automatically “wraps around” to stay within degree $n-1$. It’s like doing arithmetic on a clock, but instead of $12 + 1 &#x3D; 1$, we have $X^n &#x3D; -1$.</p><p>Here’s the intuition for why it’s $-1$ and not $0$: remember those primitive $2n$-th roots of unity? They satisfy $\zeta^{2n} &#x3D; 1$, which means $(\zeta^n)^2 &#x3D; 1$. Since $\zeta^n \neq 1$ (that would make it an $n$-th root), we must have $\zeta^n &#x3D; -1$.</p><p>The beautiful structure we get:</p><ol><li><strong>Wrapping Multiplication</strong>: $X^i \cdot X^j &#x3D; X^{(i+j) \bmod n}$ if $i+j &lt; n$, otherwise $X^i \cdot X^j &#x3D; -X^{(i+j) \bmod n}$</li><li><strong>FFT-Friendly</strong>: The Number Theoretic Transform works perfectly because our roots of unity are evenly spaced around the circle</li><li><strong>Lattice Structure</strong>: The ring elements correspond to lattice points with special geometric properties</li></ol><p>The automorphism group is isomorphic to $(\mathbb{Z}&#x2F;2n\mathbb{Z})^*$, which has order $n$. Each automorphism $\sigma_k$ is defined by $\sigma_k: X \mapsto X^k$ where $\gcd(k, 2n) &#x3D; 1$.</p><h2><span id="the-chinese-remainder-theorem-perspective">The Chinese Remainder Theorem Perspective</span></h2><p>This is where the magic really happens, and it’s the key insight that makes CKKS so powerful. Let me give you the intuition first, then the math.</p><h3><span id="the-multiple-personalities-view">The “Multiple Personalities” View</span></h3><p>Imagine you have a polynomial $p(X)$. Instead of thinking of it as a single mathematical object, the Chinese Remainder Theorem says you can think of it as having $n$ different “personalities” - one for each root of $X^n + 1$.</p><p>It’s like how Clark Kent and Superman are the same person, just in different contexts. Your polynomial $p(X)$ is “the same” as the vector $(p(\zeta_1), p(\zeta_2), \ldots, p(\zeta_n))$ where the $\zeta_i$ are the roots of $X^n + 1$.</p><p>Mathematically, this gives us an isomorphism:</p><p style="text-align:center;">$\mathbb{C}[X]/(X^n + 1) \cong \mathbb{C}^n$</p><p>Any polynomial $p(X) \in \mathbb{C}[X]&#x2F;(X^n + 1)$ is uniquely determined by its evaluations:</p><p style="text-align:center;">$p(X) \leftrightarrow (p(\zeta_{2n}), p(\zeta_{2n}^3), \ldots, p(\zeta_{2n}^{2n-1}))$</p><h3><span id="why-this-is-revolutionary-for-computation">Why This is Revolutionary for Computation</span></h3><p>Here’s the killer insight: <strong>polynomial operations become pointwise vector operations</strong>!</p><p>Want to add two polynomials? Just add their evaluation vectors component-wise. Want to multiply? Multiply component-wise. This is called <strong>SIMD (Single Instruction, Multiple Data)</strong> - you get $n$ operations for the price of one.</p><h3><span id="complex-conjugate-pairs-the-ckks-packing-trick">Complex Conjugate Pairs: The CKKS Packing Trick</span></h3><p>Here’s where CKKS gets really clever. Remember our $2n$-sided polygon? The vertices come in <strong>conjugate pairs</strong> - if you have a vertex at angle $\theta$, you also have one at angle $-\theta$.</p><p>This means if $\zeta$ is a root of $X^n + 1$, then so is $\overline{\zeta}$ (its complex conjugate).</p><p>Now here’s the packing magic: if we restrict to polynomials with <strong>real coefficients</strong> (which is what we do in practice), then evaluating our polynomial at conjugate pairs gives conjugate values:</p><p style="text-align:center;">$p(\zeta) = \overline{p(\overline{\zeta})}$</p><p>This means we only need to store <strong>half</strong> the evaluation vector! If we know $p(\zeta)$, we automatically know $p(\overline{\zeta}) &#x3D; \overline{p(\zeta)}$.</p><p><strong>Result</strong>: We can pack $n&#x2F;2$ complex numbers into a single polynomial of degree $n-1$. It’s like getting a 2x compression for free, just by exploiting the symmetry of the roots!</p><h2><span id="ckks-encoding-complex-numbers-in-polynomials">CKKS Encoding: Complex Numbers in Polynomials</span></h2><p>The CKKS scheme exploits this conjugate structure beautifully. Think of it as a universal translator between two languages: the language of polynomials and the language of complex vectors.</p><h3><span id="the-encoding-intuition">The Encoding Intuition</span></h3><p>Imagine you have $n&#x2F;2$ complex numbers that represent, say, the pixels of an image or the coefficients of a Fourier transform. You want to encrypt them homomorphically, but Ring-LWE only knows how to encrypt polynomials.</p><p>CKKS says: “No problem! I’ll convert your complex vector into a polynomial, encrypt that polynomial, and when you do operations on the polynomial, they’ll automatically happen to your original complex numbers.”</p><p>It’s like having a magical box where you put in a complex vector, it gets converted to a polynomial, encrypted, and when you add two such boxes, the encrypted polynomials add in a way that makes the underlying complex vectors add too.</p><h3><span id="canonical-embedding-the-universal-translator">Canonical Embedding: The Universal Translator</span></h3><p>The <strong>canonical embedding</strong> $\sigma$ is our universal translator. It takes a polynomial $p(X) &#x3D; \sum_{i&#x3D;0}^{n-1} p_i X^i$ and evaluates it at all our special roots:</p><p style="text-align:center;">$\sigma(p) = (p(\zeta_{2n}), p(\zeta_{2n}^3), \ldots, p(\zeta_{2n}^{2n-1}))$</p><p>This is like asking: “If this polynomial were a person, what would it say when you ask it about each of these $n$ roots?”</p><h3><span id="encoding-complex-vectors">Encoding Complex Vectors</span></h3><p>To encode a vector $\mathbf{z} &#x3D; (z_0, z_1, \ldots, z_{n&#x2F;2-1}) \in \mathbb{C}^{n&#x2F;2}$, we:</p><ol><li><strong>Extend to Conjugates</strong>: Create $\tilde{\mathbf{z}} &#x3D; (z_0, \ldots, z_{n&#x2F;2-1}, \overline{z_{n&#x2F;2-1}}, \ldots, \overline{z_0}) \in \mathbb{C}^n$</li><li><strong>Inverse Canonical Embedding</strong>: Compute $p &#x3D; \sigma^{-1}(\tilde{\mathbf{z}})$</li><li><strong>Scaling and Rounding</strong>: Scale by $\Delta$ and round to get integer coefficients</li></ol><p>The inverse canonical embedding can be computed efficiently using the <strong>Vandermonde matrix</strong>:</p><p style="text-align:center;">$V = \begin{pmatrix}1 & \zeta_{2n} & \zeta_{2n}^2 & \cdots & \zeta_{2n}^{n-1} \\1 & \zeta_{2n}^3 & (\zeta_{2n}^3)^2 & \cdots & (\zeta_{2n}^3)^{n-1} \\\vdots & \vdots & \vdots & \ddots & \vdots \\1 & \zeta_{2n}^{2n-1} & (\zeta_{2n}^{2n-1})^2 & \cdots & (\zeta_{2n}^{2n-1})^{n-1}\end{pmatrix}$</p><p>Then $\mathbf{p} &#x3D; V^{-1} \tilde{\mathbf{z}}$, which can be computed using FFT in $O(n \log n)$ time.</p><h3><span id="homomorphic-operations">Homomorphic Operations</span></h3><p>Once we have polynomials encoding our complex vectors, homomorphic operations correspond to:</p><ul><li><strong>Addition</strong>: $\text{Enc}(\mathbf{z}_1) + \text{Enc}(\mathbf{z}_2) &#x3D; \text{Enc}(\mathbf{z}_1 + \mathbf{z}_2)$</li><li><strong>Multiplication</strong>: $\text{Enc}(\mathbf{z}_1) \cdot \text{Enc}(\mathbf{z}_2) &#x3D; \text{Enc}(\mathbf{z}_1 \odot \mathbf{z}_2)$</li></ul><p>where $\odot$ is component-wise multiplication.</p><h2><span id="discrete-gaussian-distributions-and-noise">Discrete Gaussian Distributions and Noise</span></h2><p>Here’s where we get into the “why is this secure?” part. The whole security of Ring-LWE comes down to adding the right kind of <strong>noise</strong> to our encryptions.</p><h3><span id="the-noise-intuition">The Noise Intuition</span></h3><p>Think of noise in encryption like <strong>static on a radio</strong>. If you’re trying to eavesdrop on a radio transmission, a little bit of static makes it hard to understand, but the intended recipient (who knows what to listen for) can still decode the message.</p><p>But here’s the crucial insight: <strong>not all noise is created equal</strong>. Random uniform noise would work, but it turns out that <strong>Gaussian noise</strong> is optimal for lattice-based cryptography. Why? Because it plays nicely with the geometric structure of lattices.</p><h3><span id="discrete-gaussian-distribution-nature-s-favorite-noise">Discrete Gaussian Distribution: Nature’s Favorite Noise</span></h3><p>The discrete Gaussian distribution $D_{\mathbb{Z}, \sigma}$ is like a bell curve, but restricted to integers. Picture a normal distribution centered at 0, and then only keep the probability mass at integer points.</p><p>The parameter $\sigma$ controls the “width” of the bell curve:</p><ul><li>Small $\sigma$: Noise is concentrated near 0 (good for correctness, bad for security)</li><li>Large $\sigma$: Noise is spread out (good for security, bad for correctness)</li></ul><p>Mathematically: $\Pr[x] \approx \frac{1}{\sigma} \exp(-\pi x^2 &#x2F; \sigma^2)$</p><h3><span id="why-gaussian-the-geometric-intuition">Why Gaussian? The Geometric Intuition</span></h3><p>Here’s the deep reason why Gaussian noise is special: <strong>it’s the “most round” distribution</strong>. </p><p>When we add Gaussian noise to each coefficient of our polynomial, we’re essentially placing our secret in a “fuzzy cloud” that looks the same from every direction. This isotropy (rotational symmetry) is exactly what we need to make lattice problems hard.</p><p>If we used, say, uniform noise on ${-B, \ldots, B}$, our noise cloud would be cube-shaped, and cubes have corners where an attacker might find patterns. Gaussian clouds are perfectly round - no corners to exploit!</p><h3><span id="error-polynomial-sampling">Error Polynomial Sampling</span></h3><p>For Ring-LWE, we sample error polynomials $e(X) &#x3D; \sum_{i&#x3D;0}^{n-1} e_i X^i$ where each coefficient $e_i \leftarrow D_{\mathbb{Z}, \sigma}$ independently.</p><p>The <strong>norm</strong> of the error polynomial in the canonical embedding is:</p><p style="text-align:center;">$\|\sigma(e)\|_{\infty} = \max_{i} |e(\zeta_{2n}^{2i+1})|$</p><p>By concentration inequalities, with high probability:</p><p style="text-align:center;">$\|\sigma(e)\|_{\infty} \leq \sigma \sqrt{n \log n}$</p><p>This bound is crucial for correctness - we need the noise to be small enough that decryption works, but large enough for security.</p><h2><span id="security-foundations-ideal-lattices">Security Foundations: Ideal Lattices</span></h2><p>Now for the million-dollar question: <strong>why is Ring-LWE hard to break?</strong> The answer lies in geometry - specifically, the geometry of high-dimensional lattices.</p><h3><span id="the-lattice-intuition">The Lattice Intuition</span></h3><p>Think of a <strong>lattice</strong> as a regular arrangement of points in space, like the integer grid $\mathbb{Z}^2$ in the plane. But instead of 2D, we’re working in $n$-dimensional space where $n$ might be 1024 or 4096.</p><p>When we use Ring-LWE, we’re essentially <strong>hiding our secret in the lattice structure</strong>. The Ring-LWE samples give an adversary some information about which lattice we’re using, but not enough to reconstruct the secret.</p><h3><span id="from-polynomials-to-geometric-points">From Polynomials to Geometric Points</span></h3><p>Here’s the key connection: every polynomial $p(X) &#x3D; \sum p_i X^i$ corresponds to a point $(p_0, p_1, \ldots, p_{n-1})$ in $n$-dimensional space via the <strong>coefficient embedding</strong>.</p><p>The polynomial ring structure gives us a lattice with special properties - it’s not just any random lattice, but an <strong>ideal lattice</strong> with tons of symmetry. This extra structure is both a blessing (it makes things efficient) and potentially a curse (it might make attacks easier).</p><h3><span id="the-fundamental-lattice-problems">The Fundamental Lattice Problems</span></h3><p>Breaking Ring-LWE is equivalent to solving one of these problems on ideal lattices:</p><ol><li><strong>Shortest Vector Problem (SVP)</strong>: Given a lattice, find the shortest non-zero vector</li><li><strong>Closest Vector Problem (CVP)</strong>: Given a lattice and a target point, find the closest lattice point</li></ol><p>In 2D, these problems are easy - you can just look at the lattice and see the answer. But in 1024-dimensional space? Good luck! The number of lattice points to check grows exponentially with dimension.</p><h3><span id="worst-case-to-average-case-reduction">Worst-Case to Average-Case Reduction</span></h3><p>Here’s the beautiful part: Lyubashevsky, Peikert, and Regev showed that <strong>solving Ring-LWE on average is as hard as solving worst-case problems on ideal lattices</strong>.</p><p>Specifically, there’s a quantum reduction from:</p><ul><li><strong>Worst-case</strong>: $\gamma$-approximate Shortest Vector Problem (SVP) on ideal lattices in $\mathbb{Z}[X]&#x2F;(X^n + 1)$</li><li><strong>Average-case</strong>: Ring-LWE with Gaussian error width $\sigma &#x3D; \gamma \cdot \text{poly}(n)$</li></ul><p>This gives us confidence that Ring-LWE is hard even against quantum adversaries (though the reduction is quantum).</p><h3><span id="geometry-of-the-canonical-embedding">Geometry of the Canonical Embedding</span></h3><p>The canonical embedding $\sigma$ maps the ring to $\mathbb{C}^n$, but we can view this as $\mathbb{R}^{2n}$ by separating real and imaginary parts.</p><p>The key insight is that $\sigma$ is an <strong>isometry</strong> up to scaling: for any $p \in R$:</p><p style="text-align:center;">$\|\sigma(p)\|_2^2 = n \cdot \|p\|_2^2$</p><p>where $|p|<em>2^2 &#x3D; \sum</em>{i&#x3D;0}^{n-1} p_i^2$ is the coefficient norm.</p><p>This geometric structure is what makes lattice algorithms like LLL and BKZ work on ideal lattices.</p><h2><span id="parameter-selection-and-trade-offs">Parameter Selection and Trade-offs</span></h2><p>Choosing parameters for Ring-LWE schemes involves balancing security, correctness, and efficiency.</p><h3><span id="security-level">Security Level</span></h3><p>The security level depends on the <strong>root Hermite factor</strong> $\gamma$ of the best known lattice attacks. For $\lambda$-bit security, we need:</p><p style="text-align:center;">$\gamma^{2n} \geq 2^{\lambda / 2}$</p><p>Current estimates give $\gamma \approx 1.0045$ for BKZ attacks, so for 128-bit security:</p><p style="text-align:center;">$n \geq \frac{\lambda}{2 \log_2(\gamma)} \approx \frac{128}{2 \cdot 0.0065} \approx 10000$</p><p>But this is overly conservative. Practical parameter sets like those in Microsoft SEAL use $n \in {1024, 2048, 4096, 8192}$ with carefully chosen moduli.</p><h3><span id="modulus-selection">Modulus Selection</span></h3><p>The ciphertext modulus $q$ needs to be large enough to:</p><ol><li><strong>Avoid Modular Reduction Errors</strong>: $q &gt; \sigma \sqrt{n} \cdot 2^{\text{depth}}$</li><li><strong>Enable NTT</strong>: $q \equiv 1 \pmod{2n}$ for efficient polynomial multiplication</li><li><strong>Resist Attacks</strong>: Not too small relative to $n$</li></ol><p>A common approach is to use a <strong>product of primes</strong>: $q &#x3D; \prod_{i&#x3D;1}^k q_i$ where each $q_i \equiv 1 \pmod{2n}$.</p><h3><span id="noise-growth-analysis">Noise Growth Analysis</span></h3><p>In CKKS, after $d$ levels of multiplication, the noise magnitude grows roughly as:</p><p style="text-align:center;">$\text{Noise} \approx \sigma \cdot n^{d/2} \cdot B^d$</p><p>where $B$ is the bound on message coefficients. This exponential growth limits the multiplicative depth.</p><h2><span id="implementation-considerations">Implementation Considerations</span></h2><h3><span id="number-theoretic-transform">Number Theoretic Transform</span></h3><p>Polynomial multiplication in $\mathbb{Z}_q[X]&#x2F;(X^n + 1)$ can be done efficiently using NTT. For this to work, we need:</p><ol><li>$q \equiv 1 \pmod{2n}$</li><li>A primitive $2n$-th root of unity $\omega$ in $\mathbb{Z}_q$</li></ol><p>Then NTT is just FFT over $\mathbb{Z}_q$, giving us $O(n \log n)$ polynomial multiplication.</p><h3><span id="memory-and-bandwidth">Memory and Bandwidth</span></h3><p>Ciphertext size is a practical concern. A Ring-LWE ciphertext consists of two polynomials, so $2n \log q$ bits. For $n &#x3D; 4096$ and $\log q &#x3D; 200$, that’s about 200KB per ciphertext.</p><p>Techniques like <strong>ciphertext packing</strong> and <strong>batching</strong> help amortize this cost across multiple plaintexts.</p><h3><span id="precision-and-scaling">Precision and Scaling</span></h3><p>CKKS uses fixed-point arithmetic, so precision loss accumulates. The <strong>precision budget</strong> determines how many operations you can perform before precision becomes unacceptable.</p><p>After each multiplication, you typically need to <strong>rescale</strong> by dividing by some factor (and rounding), which reduces the ciphertext modulus but maintains precision.</p><p>The math here gets pretty involved, but basically you’re managing a trade-off between precision, noise, and remaining multiplicative depth.</p><h2><span id="looking-ahead">Looking Ahead</span></h2><p>This foundation sets us up perfectly for understanding more advanced topics like:</p><ul><li><strong>Automorphisms</strong>: How to rotate encrypted data using Galois theory</li><li><strong>Key Switching</strong>: Converting between different key representations</li><li><strong>Bootstrapping</strong>: Refreshing noise to enable unlimited computation</li><li><strong>Multi-Party Computation</strong>: Distributed computation on encrypted data</li></ul><p>The algebraic structure we’ve explored - cyclotomic rings, canonical embeddings, discrete Gaussians - forms the mathematical backbone of all these techniques.</p><p>I have to say, working through this math really drives home how elegant modern cryptography is. Like, the fact that we can pack complex numbers into polynomials, perform arithmetic homomorphically, and base security on lattice problems - it’s just beautiful mathematics serving practical ends.</p><p>In my next post, I’ll dive into automorphisms and key switching, building directly on these foundations. The Galois group action on cyclotomic rings gives us this incredible ability to rotate encrypted data, but it comes with the price of changing the encryption key. Key switching is the technique that lets us convert back to the original key, and the math behind it is pretty wild.</p><p>Anyway, hope this gives you a solid mathematical foundation for understanding modern FHE schemes. There’s obviously way more depth here - ideal lattices, algebraic number theory, concrete security analysis - but this should be enough to follow the more advanced techniques.</p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] Lyubashevsky, V., Peikert, C., &amp; Regev, O. (2010). On ideal lattices and learning with errors over rings. In Annual international conference on the theory and applications of cryptographic techniques (pp. 1-23). Springer.</p><p>[2] Cheon, J. H., Kim, A., Kim, M., &amp; Song, Y. (2017). Homomorphic encryption for arithmetic of approximate numbers. In International Conference on the Theory and Application of Cryptology and Information Security (pp. 409-437). Springer.</p><p>[3] Brakerski, Z., Gentry, C., &amp; Vaikuntanathan, V. (2012). (Leveled) fully homomorphic encryption without bootstrapping. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (pp. 309-325).</p><p>[4] Regev, O. (2009). On lattices, learning with errors, random linear codes, and cryptography. Journal of the ACM, 56(6), 1-40.</p><p>[5] Peikert, C. (2016). A decade of lattice cryptography. Foundations and Trends in Theoretical Computer Science, 10(4), 283-424.</p><p>[6] Micciancio, D., &amp; Regev, O. (2009). Lattice-based cryptography. In Post-quantum cryptography (pp. 147-191). Springer.</p><p>[7] Smart, N. P., &amp; Vercauteren, F. (2014). Fully homomorphic SIMD operations. Designs, codes and cryptography, 71(1), 57-81.</p><p>[8] Halevi, S., &amp; Shoup, V. (2014). Algorithms in HElib. In Annual Cryptology Conference (pp. 554-571). Springer.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
            <tag> Post Quantum Cryptography </tag>
            
            <tag> Abstract Algebra </tag>
            
            <tag> Ring Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Empirical Analysis of LWE Robustness Against Machine Learning Distinguishers</title>
      <link href="/2025/07/03/Breaking-LWE-Encryption/"/>
      <url>/2025/07/03/Breaking-LWE-Encryption/</url>
      
        <content type="html"><![CDATA[<p>The Learning With Errors (LWE) problem forms the foundation of many post-quantum cryptographic systems. These systems depend on a critical assumption: that no one can distinguish between LWE-generated samples and truly random data. I wanted to test this assumption by building sophisticated machine learning models to see if they could break this fundamental security property.</p><p>My goal wasn’t to completely “break” LWE—that would be a monumental achievement. Instead, I aimed to map out where LWE’s security boundaries actually lie in practice. Could a neural network, with its pattern-recognition capabilities, successfully identify LWE samples and violate the core security assumption? Through multiple iterations of model improvements and data refinement, I discovered just how resilient LWE really is. The results highlight why cryptographers rely on standardized libraries with carefully chosen parameters backed by years of analysis.</p><span id="more"></span><h2><span id="table-of-contents">Table of Contents</span></h2><ol><li><a href="#the-learning-with-errors-lwe-problem">The Learning With Errors (LWE) Problem</a></li><li><a href="#methodology-a-machine-learning-based-auditor">Methodology: A Machine Learning-Based Auditor</a></li><li><a href="#a-multi-stage-analytical-process">A Multi-Stage Analytical Process</a><ul><li><a href="#stage-i-the-baseline-model-and-overfitting">Stage I: The Baseline Model and Overfitting</a></li><li><a href="#stage-ii-feature-engineering-for-modular-arithmetic">Stage II: Feature Engineering for Modular Arithmetic</a></li><li><a href="#stage-iii-architectural-refinement-with-a-1d-cnn">Stage III: Architectural Refinement with a 1D CNN</a></li><li><a href="#stage-iv-a-focused-analysis-of-the-output-distribution">Stage IV: A Focused Analysis of the Output Distribution</a></li></ul></li><li><a href="#results-and-discussion-the-intractability-of-the-lwe-signal">Results and Discussion: The Intractability of the LWE Signal</a></li><li><a href="#conclusion">Conclusion</a></li></ol><h2><span id="the-learning-with-errors-lwe-problem">The Learning With Errors (LWE) Problem</span></h2><p>The LWE problem rests on the difficulty of recovering a secret vector $\mathbf{s} \in \mathbb{Z}_q^n$ from noisy linear equations. Each LWE sample consists of a pair $(\mathbf{a}, b) \in \mathbb{Z}_q^n \times \mathbb{Z}_q$, where $b$ gets calculated as:</p><p style="text-align:center;">$b = \langle \mathbf{a}, \mathbf{s} \rangle + e \mod q$</p><p>Here, $\mathbf{a}$ represents a publicly known random vector, while $e$ is a small error term drawn from a discrete Gaussian distribution. LWE-based systems rely on two key assumptions:</p><ol><li><strong>Search-LWE</strong>: Finding $\mathbf{s}$ is computationally infeasible</li><li><strong>Decision-LWE</strong>: Distinguishing LWE samples from uniformly random pairs is impossible</li></ol><p>My analysis focuses on this second assumption.</p><h2><span id="methodology-a-machine-learning-based-auditor">Methodology: A Machine Learning-Based Auditor</span></h2><p>I built a machine learning distinguisher as my primary analytical tool—essentially a neural network trained as a binary classifier to tackle the Decision-LWE problem. The model receives two types of data:</p><ul><li><strong>Positive Class (Label 1)</strong>: Authentic LWE samples $(\mathbf{a}, b)$</li><li><strong>Negative Class (Label 0)</strong>: Uniformly random pairs $(\mathbf{a}, u)$</li></ul><p>The model’s accuracy on unseen test data directly measures how distinguishable LWE samples are for specific parameter sets. Any accuracy substantially above 50% would signal a practical weakness, suggesting the model found a generalizable statistical pattern. I designed the experiment to map this accuracy across different LWE parameters, varying both the dimension (n) and noise magnitude (sigma).</p><h2><span id="a-multi-stage-analytical-process">A Multi-Stage Analytical Process</span></h2><p>This investigation didn’t follow a straight path—early failures actually proved crucial in refining my approach and revealing deeper insights about the problem’s complexity.</p><h3><span id="stage-i-the-baseline-model-and-overfitting">Stage I: The Baseline Model and Overfitting</span></h3><p>My first attempt used a standard Multi-Layer Perceptron (MLP) trained on complete $(\mathbf{a}, b)$ pairs. The model consistently failed to generalize, with test accuracy stuck at 50%. The training logs showed classic severe overfitting: while training accuracy climbed, validation performance stayed at random chance levels. This meant the model was just memorizing training artifacts rather than learning the actual mathematical structure.</p><h3><span id="stage-ii-feature-engineering-for-modular-arithmetic">Stage II: Feature Engineering for Modular Arithmetic</span></h3><p>I suspected the model’s failure stemmed from a fundamental data representation mismatch. Neural networks treat numbers linearly, but LWE operates in a modular ring where $q-1$ sits right next to $0$. To fix this, I implemented circular embeddings, transforming each integer x into a 2D vector (cos(2πx&#x2F;q), sin(2πx&#x2F;q)). This feature engineering explicitly gave the model an understanding of modular proximity.</p><p>Despite this major improvement in data representation, the model still couldn’t generalize—<strong>severe overfitting</strong> persisted. This suggested the problem wasn’t just about data format but something more fundamental.</p><h3><span id="stage-iii-architectural-refinement-with-a-1d-cnn">Stage III: Architectural Refinement with a 1D CNN</span></h3><p>The LWE problem has an inherently sequential structure—$b$ equals the sum of component-wise products. Standard MLPs aren’t architecturally suited for capturing these relationships. I switched to a <strong>1D Convolutional Neural Network (CNN)</strong>, specifically designed to identify local patterns in sequential data. I also added <strong>L2 Regularization</strong> to penalize model complexity and reduce overfitting.</p><p>The results were striking: training accuracy shot up dramatically, proving the CNN was far more capable. However, validation accuracy stayed flat at 50%. This was a <strong>critical discovery</strong>: even with proper data representation and a powerful, regularized architecture, the LWE secret’s signal was too diluted across the high-dimensional input space for the model to learn any generalizable pattern. This provided experimental evidence of the “curse of dimensionality” serving as LWE’s core security feature.</p><h3><span id="stage-iv-a-focused-analysis-of-the-output-distribution">Stage IV: A Focused Analysis of the Output Distribution</span></h3><p>After consistently failing to distinguish complete $(\mathbf{a}, b)$ pairs, I formed a final, more focused hypothesis. If the full problem proves intractable, maybe the LWE process leaves a detectable statistical bias in the distribution of b values alone.</p><p>I redesigned the experiment to train my most sophisticated model—the regularized 1D CNN with circular embeddings—on a simpler task: distinguishing collections of LWE-generated b values from collections of uniformly random integers.</p><h2><span id="results-and-discussion-the-intractability-of-the-lwe-signal">Results and Discussion: The Intractability of the LWE Signal</span></h2><p>The final experiment confirmed what cryptographers have long theorized. Across all tested parameters, including deliberately weakened ones, the model failed to distinguish the distribution of LWE b values from the uniform distribution. Test accuracy remained locked at 50%.</p><p>LWE’s resistance to machine learning attacks is well-established in theory, and this empirical evidence reinforces that understanding. Even with sophisticated neural architectures and optimized data representations, the statistical signature of the secret remains undetectable. The high-dimensional dot product combined with additive noise creates such effective diffusion that no learnable patterns emerge, even under conditions favorable to the attacker.</p><h2><span id="conclusion">Conclusion</span></h2><p>This investigation into LWE’s boundaries provides concrete evidence supporting the theoretical foundations of post-quantum cryptography. Even deliberately weakened LWE instances proved robust against sophisticated statistical analysis using modern machine learning techniques, including deep convolutional neural networks. The consistent inability of these models to generalize reinforces the mathematical principles underlying LWE’s security.</p><p>The results underscore a fundamental principle in applied cryptography: parameter selection matters enormously. The resilience observed even with suboptimal parameters demonstrates why cryptographers rely on standardized, peer-reviewed libraries where parameters undergo extensive analysis to ensure substantial security margins against known attack vectors.</p><blockquote><p><strong>Note</strong>: The full Python script for this analysis is available at this <a href="https://github.com/DA1729/lwe_ml_attack.git">GitHub repo</a> for review and further experimentation.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Cryptanalysis </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
            <tag> Post Quantum Cryptography </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In Memory Computation using Analog Part 2</title>
      <link href="/2025/03/18/In-Memory-Computation-using-Analog-Part-2/"/>
      <url>/2025/03/18/In-Memory-Computation-using-Analog-Part-2/</url>
      
        <content type="html"><![CDATA[<h2><span id="matrix-multiplication-through-mac-operations">Matrix Multiplication through MAC operations</span></h2><p>Below, I have presented a python code, illustrating matrix multiplication using MAC operation. But, why matrix multiplication only? Because everything is a fking MATRIX!!! (that’s why the film is called Matrix). Physicists, electrical engineers, computer scientists&#x2F;engineers just love representing everything in matrix, and why not, they make everything more streamlined and easy to represent. Since, we are representing everything in matrices, especially in machine learning and AI, like we have the weights matrices, input vectors, output vectors, etc., we have to do a lot of matrix multiplication and in hardware, using MAC operators, we can easily perform it. Now, carefully look and understand the python code below:</p><span id="more"></span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiply_mac</span>(<span class="params">A, B</span>):</span><br><span class="line"></span><br><span class="line">    A = np.array(A)</span><br><span class="line">    B = np.array(B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> A.shape[<span class="number">1</span>] != B.shape[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Matrix dimensions do not match for multiplication.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    C = np.zeros((A.shape[<span class="number">0</span>], B.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Can you explicitly see me using the MAC operation here? what is the accumulator?</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(B.shape[<span class="number">1</span>]):</span><br><span class="line">            mac = <span class="number">0</span>  </span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">1</span>]):</span><br><span class="line">                mac += A[i][k] * B[k][j]  </span><br><span class="line">            C[i][j] = mac</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> C</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">B = [[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">result = matrix_multiply_mac(A, B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Resultant Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>Now, that (I hope) you have read and understood the code above, one can realize that we can use the circuit we designed in the previous part for the same operation. Hence, we can do matrix multiplication through analog computing now, how cool!</p><p>But why should we go for analog rather than digital? In digital, the energy complexity grows a lot faster as the number of bits are increased, speaking with numbers, an 8-bit MAC energy can be 100 times the energy for 1 bit. </p><p>Let’s end this part here for now, as I wrote this very impulsively out a sudden motivation (and too keep the momentum going) and did not plan it too much before writing LMAO.</p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] J. -s. Seo et al., “Digital Versus Analog Artificial Intelligence Accelerators: Advances, trends, and emerging designs,” in IEEE Solid-State Circuits Magazine, vol. 14, no. 3, pp. 65-79, Summer 2022, doi: 10.1109&#x2F;MSSC.2022.3182935.<br>keywords: {AI accelerators;Market research;In-memory computing;Hardware;System analysis and design;Switching circuits},</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In-Memory Computation using Analog Part-1</title>
      <link href="/2025/03/15/In-Memory-Computation-using-Analog-Part-1/"/>
      <url>/2025/03/15/In-Memory-Computation-using-Analog-Part-1/</url>
      
        <content type="html"><![CDATA[<h2><span id="von-neumann-bottleneck">Von Neumann Bottleneck</span></h2><p>There has been an improvement in the number of transistors on a chip. More transistors mean that we have increased our ability to store more memory in less physical space. Memory storage is more efficient than ever.</p><p>Today, AI and machine learning are being studied. This requires us to store and process a large density of data, which is possible given the environment: processors and storage solutions. Also, Von Neumann Architecture requires us to store data in a separate block, and the processor needs an individual block. These different blocks are connected by buses. Given this architecture, to process these large-density data, the transfer rates must also be at par with the processing speed, maybe even faster. However, over the years, the increase in transfer speedhas only made a few gains.</p><span id="more"></span><p>When the processor has to stay idle to fetch the data from the memory block, this condition is called the <strong>Von-Neumann Bottleneck</strong>.</p><p>Some attempts to surpass this limitation have been made like: </p><ul><li><p><strong>Caching</strong>: Chaches are temporary storage units between the main memory block and the processor. It can store a subset of data so that future requests for that data can be served faster. For example, they store results of earlier computations or a copy of data stored elsewhere.</p></li><li><p><strong>Hardware Acceleration</strong>: Hardware like GPUs, FPGAs, and ASICs are brought into the picture for faster response from the hardware side.</p></li></ul><p>But these come with some limitations: </p><ul><li><p><strong>Limitations of Caching</strong>:</p><ul><li><p><strong>Size</strong>: Larger caches increase hit rates but consume more silicon area and power. </p></li><li><p>In multicore systems, maintaining consistency across caches is difficult.</p></li><li><p><strong>Memory Latency and Bandwidth Issues</strong>: If the working set exceeds capacity, frequent primary memory access still causes stalls.</p></li></ul></li><li><p><strong>Hardware Accelerators’ Limitations</strong>:</p><ul><li><p><strong>Domain-Specificity</strong>: FPGAs, TPUs, and GPUs lack generality. They are often made for specific tasks, which, economically speaking, makes them challenging to produce. </p></li><li><p>At the end of the day, communications are still being made over buses, so the transfer limitation persists. </p></li><li><p><strong>Software and Compatibility Issues</strong>: These devices run on specific firmware and can cause compatibility issues. </p></li><li><p><strong>Power and Heat Management</strong>: These hardware accelerators generate much heat and consume much power, which obviously isn’t preferable.</p></li></ul></li></ul><p>Now, we dive into analog methods of overcoming this phenomenon. Of course, some digital methods have been proposed but let’s stick to the title of the blog for now and maybe (definitely) I’ll discuss digital methods in a future blog.</p><h2><span id="analog-implementation-of-macs">Analog Implementation of MACS</span></h2><p>MAC, or Multiply-Accumulate Operation, is a common step which computes the product of two numbers and adds that product to an accumulator. MAC operations account for over 90% of Neural Network and AI computations. Yeah, so they are “kind of” important.</p><p>In the following circuit, we have 10 MOSFETs in total (5 PMOS, 5 CMOS), let us label them: <strong>PM<sub>1</sub></strong>, <strong>PM<sub>2</sub></strong>, <strong>PM<sub>3</sub></strong>, <strong>PM<sub>4</sub></strong>, <strong>PM<sub>5</sub></strong>, <strong>NM<sub>1</sub></strong>, <strong>NM<sub>2</sub></strong>, <strong>NM<sub>3</sub></strong>, <strong>NM<sub>4</sub></strong>, <strong>NM<sub>5</sub></strong>.</p><p>These MOSFETs are linearly biased (if you somewhat unfamiliar with working of MOSFET, go watch Engineering Mindset’s video on MOSFET on YouTube, I found it very good for a quick get around). We are applying differential inputs $+\Delta x, -\Delta x, +\Delta w , -\Delta w$.</p><p>The given transistors are now arranged in the following circuit (Image Courtesy: Reference [1]):</p><p><img src="/images/1.png" alt="MAC Operator"></p><p>Now, let’s get into some transistor math. </p><p>Since, all the transistors are operating in linear region, drain current <strong>I<sub>d2</sub></strong> is given by: </p><p style="text-align:center;">$I_{d2} =K_{n}*[V_{b}-\Delta w - V_{thn} - \frac{V_{b} + \Delta x}{2}]*(V_{b}+\Delta x)$</p><p>For knowing what each term means, refer to [1]. </p><p>Now, we are taking the transconductance factors and threshold voltages of the N and P MOSFETS to be equal, we get the following expression for the output current: </p><p style="text-align:center;">$I_{out} = 4*K*\Delta w * \Delta x$</p><p>If you observer the above expression, we have multiplied two numbers! Now, all we have left to do is accumulate.</p><p>The load MOSFETS: <strong>PM<sub>5</sub></strong> and <strong>NM<sub>5</sub></strong> can seen as an equivalent load resistor, which will convert the output current to an output voltage:</p><p style="text-align:center;">$\Delta y = V_{out}-V_{outbias}$</p><p>This can be easily visualized in the figure below: </p><p><img src="/images/2.png" alt="Image Courtesy : [1]"></p><p>Now, closely look at the (b) part of the above image, what we are doing is we are adding output currents of multiple sources (or I should say <strong>multipliers</strong>), such that the output voltage can be given by: </p><p style="text-align:center;">$V = \frac{1}{N}*\sum_{i=1}^{N} I_{i}*R_{load}$</p><p>With this, we have successfully created our analog MAC unit. Let us end this part-1 here. Next part, we will delve into experimental results, architecture, and maybe hybrid models proposed. </p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] J. Zhu, B. Chen, Z. Yang, L. Meng and T. T. Ye, “Analog Circuit Implementation of Neural Networks for In-Sensor Computing,” 2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), Tampa, FL, USA, 2021, pp. 150-156, doi: 10.1109&#x2F;ISVLSI51109.2021.00037. keywords: {Convolution;Neural networks;Linearity;Analog circuits;Very large scale integration;CMOS process;Silicon;Analog Computing;In-Sensor Computing;Edge Computing},</p><p>[2] Robert Sheldon, “von Neumann bottleneck”, TechTarget, <a href="https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running">https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
