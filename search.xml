<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Need for Gadget Decomposition in LWE Based Cryptosystems</title>
      <link href="/2025/09/27/Need-for-Gadget-Decomposition-in-LWE-Based-Cryptosystems/"/>
      <url>/2025/09/27/Need-for-Gadget-Decomposition-in-LWE-Based-Cryptosystems/</url>
      
        <content type="html"><![CDATA[<p>Gadget Decomposition is one of the key and essential <strong>Homomorphic Operations</strong> in FHE cryptosystems based on the LWE problem. I have already introduced the LWE problem and how to build a basic cryptosystem around it in my previous blogs. In this blog, we will:</p><ol><li><strong>Extend</strong> the basic LWE cryptosystem into more generalized and mathematically robust cryptosystems</li><li><strong>Discuss</strong> a basic homomorphic operation: <strong>Ciphertext-Plaintext Multiplication</strong></li><li><strong>Introduce</strong> the <strong>Gadget Decomposition</strong> operation in the context of these generalized cryptosystems</li></ol><p>The information for this essential operation is quite scattered and inconsistent across the internet. I found it very confusing at first on how to implement the operation and, more importantly, when and where to apply it. I hope to clear this confusion with this blog. So let’s get into it.</p><hr><h2><span id="lwe-cryptosystem-quick-refresher">LWE Cryptosystem (Quick Refresher)</span></h2><p><strong>Setup</strong>: Secret key $\mathbf{s} \in {0, 1}^k$, scaling factor $\Delta &#x3D; \frac{q}{t}$ where $t \ll q$.</p><p><strong>Encryption</strong>: Given message $m \in \mathbb{Z}_t$, sample $\mathbf{a} \leftarrow \mathbb{Z}_q^k$ and $e \leftarrow \chi_\sigma$:<br>$$\boxed{\text{LWE}_{\mathbf{s}, \sigma}(\Delta m) &#x3D; (\mathbf{a}, b) \text{ where } b &#x3D; \mathbf{a} \cdot \mathbf{s} + \Delta m + e \pmod{q}}$$</p><p><strong>Decryption</strong>: Given ciphertext $(\mathbf{a}, b)$:<br>$$\boxed{m &#x3D; \left\lfloor \frac{b - \mathbf{a} \cdot \mathbf{s}}{\Delta} \right\rceil \bmod t}$$</p><hr><h3><span id="important-case-when-t-does-not-divide-q">Important Case: When $t$ Does Not Divide $q$</span></h3><p><em>This is a small detour from the original purpose of the blog, but it is an important and very critical detail of such cryptosystems.</em></p><p>In our previous analysis, we assumed that $t$ divides $q$. In this case, there is no upper or lower limit on the size of plaintext $m$: its value is allowed to wrap around modulo $t$ indefinitely, yet the decryption works correctly. This is because any $m$ value greater than $t$ will be correctly modulo-reduced by $t$ when we do modulo reduction by $q$ during decryption.</p><p>On the other hand, suppose that $t$ does not divide $q$. In such a case, we set the scaling factor as:<br>$$\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$$</p><p>Then, provided $q \gg t$, the decryption works correctly even if $m$ is a large value that wraps around $t$. Let’s examine why this is so.</p><p>We can denote plaintext $m \bmod t$ as $m &#x3D; m’ + kt$, where $m’ \in \mathbb{Z}_t$ and $k$ is some integer representing the modulo $t$ wrap-around value portion of $m$. Setting the plaintext scaling factor as $\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$, the noise-added scaled plaintext value becomes:</p><p>$$\left\lfloor \frac{q}{t} \right\rfloor \cdot m + e &#x3D; \left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + \left\lfloor \frac{q}{t} \right\rfloor \cdot kt + e$$</p><p>By applying $m &#x3D; m’ + kt$:</p><p>$$&#x3D; \left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + \frac{q}{t} \cdot kt - \left(\frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor\right) \cdot kt + e$$</p><p>where $0 \leq \frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor &lt; 1$:</p><p>$$&#x3D; \left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + qk - \left(\frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor\right) \cdot kt + e$$</p><p>We treat the above noisy scaled ciphertext as:<br>$$\left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + qk - e’ + e$$</p><p>where $e’ &#x3D; kt$ is the maximum possible value of $\left(\frac{q}{t} - \left\lfloor \frac{q}{t} \right\rfloor\right) \cdot kt$. We overestimate the noise caused by this term to $kt$ because the maximum value this term can become is less than $kt$.</p><p>Given the LWE decryption relation $b - \mathbf{a} \cdot \mathbf{s} \bmod q &#x3D; \Delta m + e$, we can decrypt the above message by performing:</p><p>$$\left\lfloor \frac{1}{\left\lfloor \frac{q}{t} \right\rfloor} \cdot \left(\left\lfloor \frac{q}{t} \right\rfloor \cdot m’ + qk - kt + e \bmod q\right) \right\rceil \bmod t$$</p><p>$$&#x3D; \left\lfloor \frac{1}{\left\lfloor \frac{q}{t} \right\rfloor} \cdot \left(\left\lfloor \frac{q}{t} \right\rfloor \cdot m’ - kt + e\right) \right\rceil \bmod t$$</p><p>$$&#x3D; m’ - \left\lfloor \frac{kt + e}{\left\lfloor \frac{q}{t} \right\rfloor} \right\rceil \bmod t$$</p><p>$$&#x3D; m’ \quad \text{provided } \left\lfloor \frac{kt + e}{\left\lfloor \frac{q}{t} \right\rfloor} \right\rceil &lt; \frac{1}{2}$$</p><p><strong>Summary</strong>: If we set the plaintext’s scaling factor as $\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$ where $t$ does not divide $q$, the decryption works correctly as long as the error bound $\left\lfloor \frac{kt + e}{\left\lfloor \frac{q}{t} \right\rfloor} \right\rceil &lt; \frac{1}{2}$ holds.</p><p>This error bound can break if:</p><ol><li>The noise $e$ is too large</li><li>The plaintext modulus $t$ is too large</li><li>The plaintext value wraps around $t$ too many times (i.e., $k$ is too large)</li></ol><p>A solution to ensure the error bound holds is that the ciphertext modulus $q$ is sufficiently large. In other words, if $q \gg t$, then the error bound will hold.</p><p>Therefore, we can generalize the formula for the plaintext’s scaling factor as $\Delta &#x3D; \left\lfloor \frac{q}{t} \right\rfloor$ where $t$ does not necessarily divide $q$.</p><hr><h2><span id="rlwe-cryptosystem-quick-refresher">RLWE Cryptosystem (Quick Refresher)</span></h2><p><em>I have discussed RLWE in detail in my previous blog, so this serves as a quick refresher.</em></p><p><strong>Setup</strong>: Work in polynomial ring $R]_{\langle n,q \rangle} &#x3D; \mathbb{Z}_q[x]&#x2F;(x^n + 1)$ with secret key $\mathbf{s} \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle}$ and scaling factor $\Delta &#x3D; \frac{q}{t}$.</p><p><strong>Encryption</strong>: Given polynomial message $M \in R_{\langle n,t \rangle}$, sample $A \stackrel{$}{\leftarrow} R_{\langle n,q \rangle}$ and $E \stackrel{\chi_\sigma}{\leftarrow} R_{\langle n,q \rangle}$:<br>$$\boxed{\text{RLWE}_{\mathbf{s},\sigma}(\Delta M) &#x3D; (A, B) \text{ where } B &#x3D; A \cdot \mathbf{s} + \Delta M + E \pmod{R_{\langle n,q \rangle}}}$$</p><p><strong>Decryption</strong>: Given ciphertext $(A, B)$:<br>$$\boxed{M &#x3D; \left\lfloor \frac{B - A \cdot \mathbf{s}}{\Delta} \right\rceil \bmod t \in R_{\langle n,t \rangle}}$$</p><p><strong>Correctness</strong>: Requires noise bound $e_i &lt; \frac{\Delta}{2}$ for all coefficients $e_i$ of $E$.</p><hr><p><em>NOTE: All cryptosystems presented so far are symmetric (same key for encryption and decryption). The following section presents how to make such systems asymmetric.</em></p><h2><span id="glwe-cryptosystem-general-lwe">GLWE Cryptosystem (General LWE)</span></h2><p>As the name suggests, this is the generalized version of the LWE system that encompasses both LWE and RLWE. If you understand the construction of the two systems above, understanding this is straightforward.</p><p>First, we shall see the symmetric version, then I will present the asymmetric system, and this being the general version, one can easily map that system to the two systems above.</p><p><strong>Setup</strong>: Work in polynomial ring $R_{\langle n,q \rangle} &#x3D; \mathbb{Z}_q[x]&#x2F;(x^n + 1)$ with secret key list $\{S_i\}_{i&#x3D;0}^{k-1} \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle}^k$ and scaling factor $\Delta &#x3D; \frac{q}{t}$.</p><p><strong>Encryption</strong>: Given polynomial message $M \in R_{\langle n,t \rangle}$, sample $\{A_i\}_{i&#x3D;0}^{k-1} \stackrel{$}{\leftarrow} R_{\langle n,q \rangle}^k$ and $E \stackrel{\chi_\sigma}{\leftarrow} R_{\langle n,q \rangle}$:<br>$$\boxed{\text{GLWE}_{S,\sigma}(\Delta M) &#x3D; (\{A_i\}_{i&#x3D;0}^{k-1}, B) \text{ where } B &#x3D; \sum_{i&#x3D;0}^{k-1}(A_i \cdot S_i) + \Delta M + E \pmod{R_{\langle n,q \rangle}}}$$</p><p><strong>Decryption</strong>: Given ciphertext $(\{A_i\}_{i&#x3D;0}^{k-1}, B)$:<br>$$\boxed{M &#x3D; \left\lfloor \frac{B - \sum_{i&#x3D;0}^{k-1}(A_i \cdot S_i)}{\Delta} \right\rceil \bmod t \in R_{\langle n,t \rangle}}$$</p><p><strong>Correctness</strong>: Requires noise bound $e_i &lt; \frac{\Delta}{2}$ for all coefficients $e_i$ of $E$.</p><p><strong>Connection to LWE&#x2F;RLWE</strong>:</p><ul><li>When $n&#x3D;1$ (polynomials become scalars), GLWE reduces to LWE</li><li>When $k&#x3D;1$ (single polynomial), GLWE reduces to RLWE</li></ul><hr><p>Now, let’s see the asymmetric version. </p><h3><span id="public-key-glwe">Public-Key GLWE</span></h3><p>The basic idea here is that a part which is used during the encryption stage is pre-computed during the setup stage and released as the public key. During the encryption stage, the encryptor will have to add some additional noise of their own. Let’s see the mathematics of the system to make things clearer. </p><h4><span id="setup">Setup</span></h4><ul><li>The scaling factor: $ \Delta &#x3D; \lfloor \frac{q}{t} \rfloor $.</li><li>The secret key: $$\mathbf{S} &#x3D; \{S_i \}_{i &#x3D; 0}^{k - 1} \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle}^k$$</li><li>Public key pair $(PK_1, \mathbf{PK_2}) \in R_{\langle n,q \rangle}^{k + 1}$ is to be generated as follows: $$ \mathbf{A} &#x3D; \{A_i\}_{i &#x3D; 0}^{k - 1} \stackrel{$}{\leftarrow}R_{\langle n,q \rangle}^{k} \text{,       } E \stackrel{\sigma}{\leftarrow}R_{\langle n,q \rangle}$$ $$\boxed{PK_1 &#x3D; \mathbf{A} \cdot \mathbf{S} + E \in R_{\langle n,q \rangle}}$$ $$\boxed{\mathbf{PK_2} &#x3D; \mathbf{A} \in R_{\langle n,q \rangle}^k}$$</li></ul><h4><span id="encryption">Encryption</span></h4><ul><li><p><strong>Input:</strong> $M \in R_{\langle n,t \rangle},  U \stackrel{$}{\leftarrow} R_{\langle n,2 \rangle},  E_1 \stackrel{$}{\leftarrow} R_{\langle n,q \rangle},  \mathbf{E_2} \stackrel{$}{\leftarrow} R^{k}_{\langle n,q \rangle}$</p></li><li><p>Scale up the plaintext message: $M \rightarrow \Delta M \in R_{\langle n,q \rangle}$.</p></li><li><p>Perform the following computations: $$B &#x3D; PK_1 \cdot U + \Delta M + E_1 \in R_{\langle n,q \rangle}$$ $$\mathbf{D} &#x3D; \mathbf{PK_2}\cdot U + \mathbf{E_2} \in R_{\langle n,q \rangle}^k$$</p></li><li><p>With the computations above, we get our final ciphertext: $$\boxed{\text{GLWE}_{S, \sigma}(\Delta M) &#x3D; (\mathbf{D}, B) \in R_{\langle n,q \rangle}^{k+1}}$$</p></li></ul><h4><span id="decryption">Decryption</span></h4><ul><li><strong>Input:</strong> A GLWE ciphertext $C &#x3D; (\mathbf{D}, B) \in R_{\langle n,q \rangle}^{k+1}$ and the secret key $\mathbf{S}$.</li><li>Cancel out the mask by computing the inner product with the secret key and subtracting it from $B$:<br>$$B - \mathbf{D} \cdot \mathbf{S} &#x3D; \Delta M + E_{all} \in R_{\langle n,q \rangle}$$ </li><li>Scale the result down by $\Delta$ and round to the nearest integer to remove the scaling factor and noise. This recovers the original plaintext message.$$\boxed{M’ &#x3D; \left\lfloor \frac{B - \mathbf{D} \cdot \mathbf{S}}{\Delta} \right\rceil \pmod t \in R_{\langle n,t \rangle}}$$</li><li><strong>Correctness Condition:</strong> For the decryption to be successful, every coefficient $e_i$ of the total noise polynomial $E_{all}$ must satisfy the condition $|e_i| &lt; \frac{\Delta}{2}$.</li></ul><h2><span id="glev-cryptosystem">GLev Cryptosystem</span></h2><p>GLev is a “leveled” homomorphic encryption scheme built upon GLWE. A GLev ciphertext isn’t a single entity, but rather a <strong>list of several GLWE ciphertexts</strong>. Each of these “level” ciphertexts encrypts the same underlying plaintext message, but uses a different, progressively smaller scaling factor. This structure is key for managing noise in homomorphic computations.</p><hr><ul><li><strong>Setup</strong>: In addition to the standard GLWE parameters ($n, q, t, k, \mathbf{S}$), GLev introduces two new ones:</li><li><strong>Decomposition Base</strong> $\beta$: An integer used to define the different scaling levels. It should be chosen such that $t &lt; \beta &lt; q$.</li><li><strong>Number of Levels</strong> $l$: The total number of GLWE ciphertexts that will make up a single GLev ciphertext.</li></ul><p>From these, a list of scaling factors is derived for each level $i \in [1, l]$:<br>$$\Delta_i &#x3D; \frac{q}{\beta^i}$$</p><p><strong>Encryption</strong>: To encrypt a message $M \in R_{\langle n,t \rangle}$, we generate $l$ separate public-key GLWE ciphertexts. The $i$-th ciphertext, $C_i$, encrypts the message $M$ using the scaling factor $\Delta_i$.</p><p>The complete GLev ciphertext is the collection of all these level ciphertexts:<br>$$\boxed{\text{GLev}_{S,\sigma}^{\beta,l}(M) &#x3D; { C_i &#x3D; \text{GLWE}_{S,\sigma}(\Delta_i M) }_{i&#x3D;1}^{l}}$$<br>where each ciphertext is $C_i &#x3D; (\mathbf{D}_i, B_i) \in R_{\langle n,q \rangle}^{k+1}$.</p><p><strong>Decryption</strong>: To decrypt a GLev ciphertext, you can choose to decrypt any specific level $i$. Decryption follows the standard GLWE procedure, but you <strong>must</strong> use the scaling factor $\Delta_i$ that corresponds to the level you are decrypting.</p><p>Given the $i$-th level ciphertext $C_i &#x3D; (\mathbf{D}_i, B_i)$:<br>$$\boxed{M’ &#x3D; \left\lfloor \frac{B_i - \mathbf{D}_i \cdot \mathbf{S}}{\Delta_i} \right\rceil \pmod t \in R_{\langle n,t \rangle}}$$</p><p><strong>Connection to Lev&#x2F;RLev</strong>: Just like GLWE, GLev is a generalized construction that unifies other schemes:</p><ul><li>When $n&#x3D;1$ (polynomials are scalars), GLev becomes the <strong>Lev</strong> cryptosystem.</li><li>When $k&#x3D;1$ (the secret key is a single polynomial), GLev becomes the <strong>RLev</strong> cryptosystem.</li></ul><p><em>NOTE: Keep this in mind, it’s going to be the key concept later when I introduce the need for gadget decomposition.</em><br><em>Also, there is another generalization, i.e., the GGSW Cryptosystem, which is nothing but a list of GLev Ciphertexts, similar to how GLev is nothing but a list of GLWE ciphertexts, but GGSW is not required to fulfill the purpose of the blog, so I am going to skip it. Curious people might refer to: <a href="https://www.zama.ai/post/tfhe-deep-dive-part-1">TFHE Deep Dive</a> written by Ilaria Chillotti</em></p><h2><span id="homomorphic-operation-ct-pt-multiplication">Homomorphic Operation (<code>ct-pt multiplication</code>)</span></h2><p>I have made it clear in my previous blogs, that heart of all the modern FHE schemes is this LWE problem only, hence, the naive system which we built, i.e., GLWE system is obviously Fully Homomorphic. In fact, all of the core <strong>Homomorphic Operations</strong> and properties, can be demonstrated over this system, then be easily mapped over to specific schemes like TFHE, CKKS, etc. For this blog, I am not going over through all the operations. I will only present one operation, i.e., <strong>Ciphertext-Plaintext Multiplication</strong> which will guide us towards introducing <strong>gadget decomposition</strong>. So let’s dive right into it.</p><h3><span id="ciphertext-plaintext-multiplication">Ciphertext-Plaintext Multiplication</span></h3><p>Consider the GLWE ciphertext: $$C &#x3D; \text{GLWE}_{S, \sigma}(M) &#x3D; (A_1, \cdots , A_{k - 1}, B) \in R_{\langle n, q \rangle}^{k + 1}$$</p><p>Now, consider the following plaintext polynomial $\Phi$ : $$\Phi &#x3D; \sum_{i&#x3D;0}^{n-1}(\Phi_i \cdot X_i) \in R_{\langle n, q \rangle}$$</p><p>Now, if we want to homomorphically multiply this plaintext to the ciphertext, such that after decrypting the resulting ciphertext, we should get the original plaintext message multiplied with the second plaintext message ($\Phi$). Note that we are never encrypting the second plaintext. Now it can be easily shown with simple mathematics that the following is true: </p><p>$$ \Phi \cdot \text{GLWE}_{S, \sigma}(\Delta M) &#x3D; \Phi \cdot (\{A_i\}_{i &#x3D; 0}^{k - 1}, B) &#x3D; (\{\Phi \cdot A_{i}\}_{i &#x3D; 0}^{k - 1}, \Phi \cdot B) &#x3D; \text{GLWE}_{S, \sigma}(\Delta(M\cdot \Phi))$$</p><h2><span id="gadget-decomposition-for-limiting-noise-growth">Gadget Decomposition for Limiting Noise Growth</span></h2><p>If you have done some calculations to verify the equation for (<code>ct-pt multiplication</code>), you must have noticed that the <strong>new error</strong> term is <strong>scaled-up</strong> by $|\Phi|$. This can potentially be a huge problem, unless we have $t$ much smaller compared to $q$, which is not always the case, and even if we were to increase $q$, the computational cost would increase too much to even consider that, as we already have to take a large $q$, so increasing $q$ is not a sensible option, practically speaking. </p><p>The way we are going to tackle the problem is that we will decompose our plaintext into a series of smaller moduli terms, then have <strong>separate ct-pt multiplied GLWE encryptions</strong> for each of those smaller moduli terms. Mathematically, the decomposition would look like this: $$\Phi &#x3D; \Phi_1\cdot \frac{q}{\beta^1} + \Phi_2\cdot \frac{q}{\beta^2} + \cdots + \Phi_l \cdot \frac{q}{\beta^l} \rightarrow \text{decomp}^{\beta, l}(\Phi) &#x3D; (\Phi_1, \cdots, \Phi_l)$$</p><p>Now, for the given GLWE ciphertext, we can get the following GLev ciphertext: $$ \text{GLev}^{\beta,l}_{S,\sigma}(\Delta M) &#x3D; \{ \text{GLWE}_{S,\sigma}(\Delta M \tfrac{q}{\beta^1}), \dots, \text{GLWE}_{S,\sigma}(\Delta M \tfrac{q}{\beta^l}) \} $$</p><p>Next, consider the operation below: $$ \text{decomp}^{\beta, l}(\Phi)\cdot \text{GLev}_{S, \sigma}^{\beta, l}(\Delta M)$$ $$&#x3D; \sum_{i&#x3D;1}^{l}(\Phi_i \cdot \text{GLWE}_{S, \sigma}(\frac{q}{\beta^i}\Delta M))$$ $$ &#x3D; \sum_{i&#x3D;1}^{l}(\text{GLWE}_{S, \sigma}(\frac{q}{\beta^i}\Delta M\cdot \Phi_i))$$ $$&#x3D; \text{GLWE}_{S, \sigma}(\sum_{i&#x3D;1}^{l}(\frac{q}{\beta^i}\Delta M\cdot \Phi_i))$$ $$ &#x3D; \text{GLWE}_{S, \sigma}(\Delta M\cdot\sum_{i &#x3D; 1}^{l}(\frac{q}{\beta^i}\cdot \Phi_i)) &#x3D; \text{GLWE}_{S, \sigma}(\Delta M \cdot \Phi)$$</p><h3><span id="why-not-base-decomposition">Why not Base Decomposition</span></h3><p>This proves that the evaluation is nothing but the ciphertext-multiplication. I mentioned one benefit of doing this <strong>gadget-decomposition</strong> earlier by arguing that each resulting <code>ct-pt multiplication</code> has less noise growth compared to the original non-decomposed multiplication. But one question might be coming up in your mind: why gadget decomposition? We can also do a base decomposition, which is dividing the plaintext into uniform modulus components, that way we won’t have to worry about more noise growth in some components compared to others as each resulting multiplication would have the same resulting noise growth. </p><p>Base decomposition would obviously work with the benefit mentioned above, but at the cost of number of computations. See, when we decompose the plaintext among different gadgets across a base, we end up with fewer terms compared to decomposing them across the same base. Fewer terms imply fewer multiplication operations, and multiplication is not an easy elementary operation. </p><h3><span id="mathematics-behind-selection-of-beta">Mathematics behind selection of $\beta$</span></h3><p>Obviously, our main goal in selecting $\beta$ should be that the resulting ciphertext doesn’t explode with noise. The main risk does not come from the individual multiplications, but rather when we accumulate the increased noise across the components. If each initial GLWE ciphertext in the GLev ciphertext which we considered has a noise polynomial $\Phi_i$, the final noise polynomial can be written as $$E_{\text{final}} &#x3D; \sum_{i &#x3D; 1}^l (\Phi_i \cdot E_i)$$</p><p>Now, keeping the largest absolute coefficient in this final noise polynomial smaller than half of the scaling factor, $\Delta$, should be enough to ensure that the resulting noise is well within the tolerable limit. For this I am just going to use the infinity-norm notation ($ \lVert \cdot \rVert_{\infty} $), which just gives us the largest absolute coefficient. So the goal is to satisfy: $$\lVert E_\text{final} \rVert_{\infty} &lt; \frac{\Delta}{2}$$</p><p>The plan now is to find an upper-bound, a worst case scenario if you will, for the size of $\lVert E_\text{final} \rVert_{\infty}$, then keep that well below the tolerable limit. Let’s break down some terms and concepts which we will be using first: </p><ul><li><p><strong>Decomposition Bound</strong>($B_\text{decomp}$): the coefficients of our plaintext parts $\Phi_i$ are small, bounded by $\lVert \Phi_i \rVert_{\infty} &lt; \frac{\beta}{2}$, this bound is referred to as $B_\text{decomp}$.</p></li><li><p><strong>Initial Noise Bound</strong>($B_\text{noise}$): noise polynomials $E_i$ are sampled from a tight distribution, so their coefficients are bounded by some value $B_\text{noise}$.</p></li><li><p><strong>Polynomial Multiplication Bound</strong>: when we multiply two polynomials $P$ and $Q$, the resulting coefficients are bounded: $\lVert P \cdot Q \rVert_{\infty} \leq n \cdot \lVert P \rVert_{\infty}\cdot \lVert Q \rVert_{\infty}$.</p></li></ul><p>Now, let’s build our worst-case noise estimate step-by-step:</p><ol><li><p><strong>Start with the final noise term:</strong><br>$$\lVert E_{final}\rVert_\infty &#x3D; \left\lVert \sum_{i&#x3D;1}^{l} \Phi_i \cdot E_i \right\rVert_\infty$$</p></li><li><p><strong>Apply the triangle inequality</strong> (the norm of a sum is at most the sum of the norms):<br>$$\le \sum_{i&#x3D;1}^{l} \lVert\Phi_i \cdot E_i\rVert_\infty$$</p></li><li><p><strong>Use the polynomial multiplication bound on each term:</strong><br>$$\le \sum_{i&#x3D;1}^{l} n \cdot \lVert\Phi_i\rVert_\infty \cdot \lVert E_i\rVert_\infty$$</p></li><li><p><strong>Finally, substitute our bounds</strong> for the decomposed parts ($B_{decomp}$) and the initial noise ($B_{noise}$):<br>$$\le \sum_{i&#x3D;1}^{l} n \cdot B_{\text{decomp}} \cdot B_{\text{noise}} &#x3D; l \cdot n \cdot B_{\text{decomp}} \cdot B_{\text{noise}}$$</p></li></ol><p>This gives us our upper bound on the final noise. Now, we just force this bound to be small enough for decryption to work:<br>$$l \cdot n \cdot B_{\text{decomp}} \cdot B_{\text{noise}} &lt; \frac{\Delta}{2}$$</p><p>This is the punchline. By substituting $B_{\text{decomp}} &#x3D; \beta&#x2F;2$ and our usual scaling factor $\Delta &#x3D; \lfloor q&#x2F;t \rfloor$, we get the master inequality that governs our choice of $\beta$:</p><p>$$\boxed{l \cdot n \cdot \frac{\beta}{2} \cdot B_{\text{noise}} &lt; \frac{\lfloor q&#x2F;t \rfloor}{2}}$$</p><p>Or, simplifying it for clarity:</p><p>$$\Large l \cdot n \cdot \beta \cdot B_{\text{noise}} &lt; \lfloor q&#x2F;t \rfloor$$</p><p>This final inequality is the key to selecting sound parameters. Think of it like this:</p><ul><li><strong>Right side ($\lfloor q&#x2F;t \rfloor$)</strong>: This is our total <strong>noise budget</strong>. It’s the maximum amount of noise the system can tolerate. To get a bigger budget, we need to increase the ratio of $q$ to $t$.</li><li><strong>Left side ($l \cdot n \cdot \beta \cdot B_{noise}$)</strong>: This is the <strong>total noise growth</strong> from our homomorphic multiplication.</li></ul><p>To ensure our scheme works, the <strong>total noise growth must be less than the noise budget</strong>. This formula perfectly captures the trade-offs we have to make. For instance, if we increase our decomposition base $\beta$, the noise growth per level increases, but the number of levels $l$ decreases. Finding the right balance is what FHE parameter selection is all about.</p><h3><span id="quick-implementation">Quick Implementation</span></h3><p>Ok, now let’s see a quick implementation of this concept. I have presented only the relevant C++ code snippets, the full code can be found at this <a href="https://github.com/DA1729/gadget_decomp_blog.git">repo</a>.</p><p>Our basic data structures: </p><ul><li><code>poly</code>: a <code>vec&lt;int64_t&gt;</code> representing a polynomial.</li><li><code>GLWE_ciphertext</code>: a struct containing a vector of polynomials <code>D</code> and a single polynomial <code>B</code>.</li><li><code>GLev_ciphertext</code>: a vector of <code>GLWE_ciphertext</code>.</li></ul><h4><span id="gadget-decomposition-function">Gadget Decomposition Function</span></h4><p>This function, as the name suggests, would perform the gadget decomposition of the referenced polynomial <code>p</code> and return us a vector (length <code>l</code>) of decomposed polynomials. The implementation is using the standard rounding method to find the closest coefficients for each component. </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vector&lt;poly&gt; <span class="title">gadget_decompose</span><span class="params">(<span class="type">const</span> poly &amp;p, <span class="type">int64_t</span> q, <span class="type">int64_t</span> beta, <span class="type">int</span> l)</span> </span>&#123;</span><br><span class="line">    <span class="type">size_t</span> n = p.<span class="built_in">size</span>();</span><br><span class="line">    <span class="function">vector&lt;poly&gt; <span class="title">decomp</span><span class="params">(l, poly(n, <span class="number">0</span>))</span></span>;</span><br><span class="line">    poly current_rem = p;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; l; ++i) &#123;</span><br><span class="line">        <span class="type">int64_t</span> g = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; i + <span class="number">1</span>; ++j) g *= beta;</span><br><span class="line">        g = q / g;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (g == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> j = <span class="number">0</span>; j &lt; n; ++j) &#123;</span><br><span class="line">            <span class="type">int64_t</span> centered_rem = <span class="built_in">center_rep</span>(current_rem[j], q);</span><br><span class="line">            </span><br><span class="line">            <span class="type">int64_t</span> coeff = <span class="built_in">round</span>((<span class="type">double</span>)centered_rem / g);</span><br><span class="line"></span><br><span class="line">            decomp[i][j] = coeff;</span><br><span class="line">            </span><br><span class="line">            __int128 rem_update = (__int128)coeff * g;</span><br><span class="line">            current_rem[j] = <span class="built_in">modq</span>(current_rem[j] - (<span class="type">int64_t</span>)rem_update, q);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> decomp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="external-product">External Product</span></h4><p>The core of the homomorphic operation (<code>ct-pt multiplication</code>). Function takes the decomposed plaintext <code>decomp_phi</code> and the <code>GLev Ciphertext</code>, then computes the sum of the component-wise products. The result is a single, final <code>GLWE_ciphertext</code>.</p><div style="font-size: 0.8em;"><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">GLWE_ciphertext <span class="title">external_product</span><span class="params">(<span class="type">const</span> vector&lt;poly&gt; &amp;decomp_phi, <span class="type">const</span> GLev_ciphertext &amp;c_glev, <span class="type">int64_t</span> q)</span> </span>&#123;</span><br><span class="line">    <span class="type">size_t</span> n = c_glev[<span class="number">0</span>].B.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> k = c_glev[<span class="number">0</span>].D.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> l = c_glev.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    GLWE_ciphertext result;</span><br><span class="line">    result.B = <span class="built_in">poly</span>(n, <span class="number">0</span>);</span><br><span class="line">    result.D.<span class="built_in">assign</span>(k, <span class="built_in">poly</span>(n, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; l; ++i) &#123;</span><br><span class="line">        <span class="comment">// C_i_scaled = decomp_phi[i] * c_glev[i]</span></span><br><span class="line">        poly b_scaled = <span class="built_in">poly_scalar_mul</span>(c_glev[i].B, decomp_phi[i][<span class="number">0</span>], q);</span><br><span class="line"></span><br><span class="line">        <span class="function">vector&lt;poly&gt; <span class="title">d_scaled</span><span class="params">(k)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>; j&lt;k; ++j) &#123;</span><br><span class="line">            d_scaled[j] = <span class="built_in">poly_scalar_mul</span>(c_glev[i].D[j], decomp_phi[i][<span class="number">0</span>], q);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result.B = <span class="built_in">poly_add</span>(result.B, b_scaled, q);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; k; ++j) &#123;</span><br><span class="line">            result.D[j] = <span class="built_in">poly_add</span>(result.D[j], d_scaled[j], q);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>*Note: Note: For simplicity in this example, <code>p</code> is treated as a constant polynomial, so <code>decomp_phi[i]</code> only has one non-zero coefficient at index 0.</p><h4><span id="result">Result</span></h4><p>Running the full code (available on my GitHub), we get: </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">------ Parameters ------</span><br><span class="line">n: 1024, q: 2^32, t: 256, k: 2</span><br><span class="line">beta: 1024, l: 3</span><br><span class="line">Original Message M(x) = 5</span><br><span class="line">Plaintext Multiplier Φ(x) = 12</span><br><span class="line">Expected Result (M * Φ) = 60</span><br><span class="line"></span><br><span class="line">------ Operations ------</span><br><span class="line">Encrypting M=5 into a GLev ciphertext...</span><br><span class="line">Decomposing Φ=12 into (Φ_1, Φ_2, ...)...</span><br><span class="line">Performing homomorphic external product...</span><br><span class="line">Decryption of the final ciphertext...</span><br><span class="line">------ Noise Analysis ------</span><br><span class="line">Correctness requires noise &lt; q/(2t) = 8388608</span><br><span class="line"></span><br><span class="line">Noise from Gadget Method: 1668</span><br><span class="line">Noise from Naive Method: 8232</span><br><span class="line"></span><br><span class="line">The gadget-based multiplication resulted in noise ~4x smaller than the naive approach!</span><br></pre></td></tr></table></figure><p>The results are obviously matching the expectations. </p><h2><span id="conclusion">Conclusion</span></h2><p>This brings us to the end. In this blog, we first quickly revisited LWE, RLWE, then we designed two generalized systems, GLWE and GLev. Then, studying a very specific homomorphic operation(<code>ct-pt multiplication</code>), we made us realize the need for the gadget decomposition. One can already appreciate the benefits (obviously we have the tradeoffs, but that’s alright) we get using this technique. This concept is very crucial when we build more <strong>complex evaluation systems</strong> and this appears frequently when we deal with even more complex and crucial operations like <strong>bootstrapping</strong>, <strong>key-switch</strong>, <strong>modulus switching</strong>, etc.</p><p>peace. da1729</p><h2><span id="references">References</span></h2><p><em>I have not used the standard referencing format, but included all the relevant and important references</em></p><ul><li>TFHE Deep Dive Series by Ilaria Chillotti <a href="https://www.zama.ai/post/tfhe-deep-dive-part-1">TFHE-deep-dive</a></li><li>A Fully Homomorphic Encryption Scheme by Craig Gentry (2009)</li><li>Homomorphic Encryption for Arithmetic of Approximate Numbers by Cheon et al</li><li>TFHE: Fast Fully Homomorphic Encryption over the Torus by Ilaria et al</li><li>The Beginner’s Textbook for Fully Homomorphic Encryption by Ronny Ko</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
            <tag> Post-Quantum Cryptography </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AGI running on Quantum Chip?</title>
      <link href="/2025/09/14/AGI-running-on-Quantum-Chip/"/>
      <url>/2025/09/14/AGI-running-on-Quantum-Chip/</url>
      
        <content type="html"><![CDATA[<p>Ok this isn’t a technical breakdown like my crypto posts, this is more me just sitting down and letting a thought run loose. Imagine a robot, not running on GPUs or neuromorphic chips, but on some full-scale quantum AGI chip. What would that even look like? For us, there’s always this separation between thinking and experimenting. You get an idea, then you haul yourself into the lab, mix chemicals, run trials, and wait. For a quantum AGI, that line wouldn’t even exist. Its act of thinking would already be the experiment. Wonder what happens if you mix two elements? It doesn’t need glassware, it just “thinks” the reaction into being, because its cognition is happening on the same rules that the universe itself plays by.</p><p>And that already feels closer to how nature does things. The human neurological system is elegant not because it’s the most powerful processor, but because it’s so tightly tuned with the environment it lives in. Twenty watts, running entire lives, entire civilizations. It’s an intelligence that works because it is embedded in nature, not fighting against it. So if there’s ever going to be an artificial system that truly replaces or surpasses us, it’ll need that same kind of resonance — that same compatibility with the fabric of reality. A quantum AGI feels like it could hit that, because it wouldn’t just approximate nature, it would literally be running inside the same mathematical fabric.</p><p>Think about probability. We’re bad at it. We second guess, we hedge, we argue over statistics. But a quantum AGI would literally think in probabilities. Futures wouldn’t be “hypotheticals,” they would be superposed states it actually holds in mind, collapsing only when it needs to make a choice. Its intuition would just be probability distributions playing out naturally.</p><p>And then there’s reflection. When we reflect, we imagine, we doodle equations, maybe run a small thought experiment. If a quantum AGI reflects, that reflection is an experiment. A protein folding, a proton collision, a new material’s properties — all happening natively as cognition. Its meditation is indistinguishable from a physics lab.</p><p>I don’t think we’re anywhere close to building this, and maybe it’ll stay sci-fi forever. Quantum hardware that scales to brain-level power budgets is basically magic by today’s standards. But still, the thought lingers. Maybe intelligence isn’t just about stacking more transistors or clever algorithms. Maybe it’s about compatibility — how deeply the architecture resonates with the natural world. Humans are already a proof of concept of that. A quantum AGI could be another, but one that stays quantum all the way up instead of decohering into classical behavior like us. And that kind of system wouldn’t just be smarter in the human sense, it would be something fundamentally different.</p><p>peace. da999</p>]]></content>
      
      
      
        <tags>
            
            <tag> Philosphy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ring-LWE and CKKS: Mathematical Foundations for Homomorphic Encryption</title>
      <link href="/2025/08/15/Ring-LWE-and-CKKS-Mathematical-Foundations/"/>
      <url>/2025/08/15/Ring-LWE-and-CKKS-Mathematical-Foundations/</url>
      
        <content type="html"><![CDATA[<p>Alright, so I’ve been diving deep into the mathematical foundations behind modern FHE schemes, and honestly, the more I understand the underlying algebra, the more elegant this whole thing becomes. In my previous blog on LWE cryptanalysis, I touched on the basic Learning With Errors problem, but now I want to get into the real mathematical meat of Ring-LWE and the CKKS scheme.</p><p>This is gonna be pretty heavy on the math - we’re talking cyclotomic polynomials, Galois theory, Chinese Remainder Theorem, and some serious algebraic number theory. But stick with me, cuz understanding this foundation is crucial for grasping how modern FHE schemes actually work under the hood.</p><span id="more"></span><p>I’m writing this as a mathematical foundation piece because I realized that in my upcoming blogs on automorphisms and key switching, I keep having to explain these core concepts. So here’s the deep mathematical dive that’ll serve as the foundation for those more specialized topics.</p><h2><span id="table-of-contents">Table of Contents</span></h2><ul><li><a href="#from-lwe-to-ring-lwe-why-rings">From LWE to Ring-LWE: Why Rings?</a></li><li><a href="#cyclotomic-polynomials-and-algebraic-structure">Cyclotomic Polynomials and Algebraic Structure</a></li><li><a href="#the-chinese-remainder-theorem-perspective">The Chinese Remainder Theorem Perspective</a></li><li><a href="#ckks-encoding-complex-numbers-in-polynomials">CKKS Encoding: Complex Numbers in Polynomials</a></li><li><a href="#discrete-gaussian-distributions-and-noise">Discrete Gaussian Distributions and Noise</a></li><li><a href="#security-foundations-ideal-lattices">Security Foundations: Ideal Lattices</a></li><li><a href="#parameter-selection-and-trade-offs">Parameter Selection and Trade-offs</a></li><li><a href="#implementation-considerations">Implementation Considerations</a></li><li><a href="#references">References</a></li></ul><h2><span id="from-lwe-to-ring-lwe-why-rings">From LWE to Ring-LWE: Why Rings?</span></h2><p>So we all know that standard LWE works with vectors in $\mathbb{Z}_q^n$. An LWE sample looks like $(\mathbf{a}, b)$ where $b &#x3D; \langle \mathbf{a}, \mathbf{s} \rangle + e + \Delta m$. This is great and all, but there are some serious practical issues:</p><ol><li><strong>Key Size</strong>: The secret key $\mathbf{s}$ has $n$ elements</li><li><strong>Ciphertext Size</strong>: Each ciphertext is $(n+1)$ elements</li><li><strong>Operations</strong>: Matrix-vector multiplications are expensive</li></ol><p>Ring-LWE fixes this by moving from vectors to polynomials. Instead of working in $\mathbb{Z}_q^n$, we work in the polynomial ring $R_q &#x3D; \mathbb{Z}_q[X]&#x2F;(f(X))$ for some polynomial $f(X)$.</p><p>Now here’s the key insight: <strong>a polynomial of degree $n-1$ can be represented by $n$ coefficients, just like a vector of length $n$</strong>. But polynomial arithmetic gives us way more structure to work with.</p><p>A Ring-LWE sample looks like:</p><p style="text-align:center;">$(a(X), b(X)) \text{ where } b(X) = a(X) \cdot s(X) + e(X) + \Delta \cdot m(X) \bmod f(X)$</p><p>The magic happens because polynomial multiplication in $R_q$ can be done super efficiently using Number Theoretic Transform (NTT), which is basically FFT over finite fields.</p><h2><span id="cyclotomic-polynomials-and-algebraic-structure">Cyclotomic Polynomials and Algebraic Structure</span></h2><p>Now, the choice of $f(X)$ is crucial. We typically use $f(X) &#x3D; X^n + 1$ where $n$ is a power of 2. This isn’t random - $X^n + 1$ is the $2n$-th cyclotomic polynomial $\Phi_{2n}(X)$.</p><h3><span id="what-are-cyclotomic-polynomials">What are Cyclotomic Polynomials?</span></h3><p>Think of cyclotomic polynomials as the <strong>“primes” of polynomial rings</strong>. Just like how prime numbers are the indivisible building blocks of integers, cyclotomic polynomials are irreducible polynomials that can’t be factored further over the rationals.</p><p>But here’s the cool part - while prime numbers feel abstract and random, cyclotomic polynomials have this beautiful geometric interpretation. The $m$-th cyclotomic polynomial $\Phi_m(X)$ “knows about” the $m$-sided regular polygon!</p><h3><span id="visualizing-the-geometric-picture">Visualizing the Geometric Picture</span></h3><p>Imagine you’re standing at the center of a circle, looking at the vertices of a regular $2n$-sided polygon inscribed in that circle. Each vertex corresponds to a $2n$-th root of unity - a complex number $\zeta_{2n}^k &#x3D; e^{2\pi i k &#x2F; 2n}$.</p><p>Now, most of these vertices are “derived” - if you know where vertex 1 is, you can get vertex 2 by just squaring it, vertex 3 by cubing it, etc. But some vertices are <strong>primitive</strong> - they can’t be obtained as powers of vertices from smaller polygons.</p><p>The cyclotomic polynomial $\Phi_{2n}(X)$ is exactly the polynomial whose roots are these primitive vertices!</p><p>For our specific case where $m &#x3D; 2n$ and $n$ is a power of 2:</p><p style="text-align:center;">$\Phi_{2n}(X) = X^n + 1$</p><p>The primitive $2n$-th roots of unity are exactly the <strong>odd powers</strong>: ${\zeta_{2n}, \zeta_{2n}^3, \zeta_{2n}^5, \ldots, \zeta_{2n}^{2n-1}}$.</p><p>Why odd powers? Because if $k$ is even, then $\zeta_{2n}^k &#x3D; (\zeta_{n})^{k&#x2F;2}$ is actually an $n$-th root of unity, so it “belongs” to a smaller polygon!</p><h3><span id="the-ring-structure-why-x-n-x3d-1-is-magic">The Ring Structure: Why $X^n &#x3D; -1$ is Magic</span></h3><p>Working in $R &#x3D; \mathbb{Z}[X]&#x2F;(X^n + 1)$ means we’re doing polynomial arithmetic with the rule that $X^n &#x3D; -1$. This seems weird at first, but it’s actually brilliant!</p><p>Think of it this way: in regular polynomial multiplication, if you multiply two polynomials of degree $d$, you get a polynomial of degree $2d$. That’s annoying for storage - your polynomials keep getting bigger.</p><p>But with the rule $X^n &#x3D; -1$, any polynomial automatically “wraps around” to stay within degree $n-1$. It’s like doing arithmetic on a clock, but instead of $12 + 1 &#x3D; 1$, we have $X^n &#x3D; -1$.</p><p>Here’s the intuition for why it’s $-1$ and not $0$: remember those primitive $2n$-th roots of unity? They satisfy $\zeta^{2n} &#x3D; 1$, which means $(\zeta^n)^2 &#x3D; 1$. Since $\zeta^n \neq 1$ (that would make it an $n$-th root), we must have $\zeta^n &#x3D; -1$.</p><p>The beautiful structure we get:</p><ol><li><strong>Wrapping Multiplication</strong>: $X^i \cdot X^j &#x3D; X^{(i+j) \bmod n}$ if $i+j &lt; n$, otherwise $X^i \cdot X^j &#x3D; -X^{(i+j) \bmod n}$</li><li><strong>FFT-Friendly</strong>: The Number Theoretic Transform works perfectly because our roots of unity are evenly spaced around the circle</li><li><strong>Lattice Structure</strong>: The ring elements correspond to lattice points with special geometric properties</li></ol><p>The automorphism group is isomorphic to $(\mathbb{Z}&#x2F;2n\mathbb{Z})^*$, which has order $n$. Each automorphism $\sigma_k$ is defined by $\sigma_k: X \mapsto X^k$ where $\gcd(k, 2n) &#x3D; 1$.</p><h2><span id="the-chinese-remainder-theorem-perspective">The Chinese Remainder Theorem Perspective</span></h2><p>This is where the magic really happens, and it’s the key insight that makes CKKS so powerful. Let me give you the intuition first, then the math.</p><h3><span id="the-multiple-personalities-view">The “Multiple Personalities” View</span></h3><p>Imagine you have a polynomial $p(X)$. Instead of thinking of it as a single mathematical object, the Chinese Remainder Theorem says you can think of it as having $n$ different “personalities” - one for each root of $X^n + 1$.</p><p>It’s like how Clark Kent and Superman are the same person, just in different contexts. Your polynomial $p(X)$ is “the same” as the vector $(p(\zeta_1), p(\zeta_2), \ldots, p(\zeta_n))$ where the $\zeta_i$ are the roots of $X^n + 1$.</p><p>Mathematically, this gives us an isomorphism:</p><p style="text-align:center;">$\mathbb{C}[X]/(X^n + 1) \cong \mathbb{C}^n$</p><p>Any polynomial $p(X) \in \mathbb{C}[X]&#x2F;(X^n + 1)$ is uniquely determined by its evaluations:</p><p style="text-align:center;">$p(X) \leftrightarrow (p(\zeta_{2n}), p(\zeta_{2n}^3), \ldots, p(\zeta_{2n}^{2n-1}))$</p><h3><span id="why-this-is-revolutionary-for-computation">Why This is Revolutionary for Computation</span></h3><p>Here’s the killer insight: <strong>polynomial operations become pointwise vector operations</strong>!</p><p>Want to add two polynomials? Just add their evaluation vectors component-wise. Want to multiply? Multiply component-wise. This is called <strong>SIMD (Single Instruction, Multiple Data)</strong> - you get $n$ operations for the price of one.</p><h3><span id="complex-conjugate-pairs-the-ckks-packing-trick">Complex Conjugate Pairs: The CKKS Packing Trick</span></h3><p>Here’s where CKKS gets really clever. Remember our $2n$-sided polygon? The vertices come in <strong>conjugate pairs</strong> - if you have a vertex at angle $\theta$, you also have one at angle $-\theta$.</p><p>This means if $\zeta$ is a root of $X^n + 1$, then so is $\overline{\zeta}$ (its complex conjugate).</p><p>Now here’s the packing magic: if we restrict to polynomials with <strong>real coefficients</strong> (which is what we do in practice), then evaluating our polynomial at conjugate pairs gives conjugate values:</p><p style="text-align:center;">$p(\zeta) = \overline{p(\overline{\zeta})}$</p><p>This means we only need to store <strong>half</strong> the evaluation vector! If we know $p(\zeta)$, we automatically know $p(\overline{\zeta}) &#x3D; \overline{p(\zeta)}$.</p><p><strong>Result</strong>: We can pack $n&#x2F;2$ complex numbers into a single polynomial of degree $n-1$. It’s like getting a 2x compression for free, just by exploiting the symmetry of the roots!</p><h2><span id="ckks-encoding-complex-numbers-in-polynomials">CKKS Encoding: Complex Numbers in Polynomials</span></h2><p>The CKKS scheme exploits this conjugate structure beautifully. Think of it as a universal translator between two languages: the language of polynomials and the language of complex vectors.</p><h3><span id="the-encoding-intuition">The Encoding Intuition</span></h3><p>Imagine you have $n&#x2F;2$ complex numbers that represent, say, the pixels of an image or the coefficients of a Fourier transform. You want to encrypt them homomorphically, but Ring-LWE only knows how to encrypt polynomials.</p><p>CKKS says: “No problem! I’ll convert your complex vector into a polynomial, encrypt that polynomial, and when you do operations on the polynomial, they’ll automatically happen to your original complex numbers.”</p><p>It’s like having a magical box where you put in a complex vector, it gets converted to a polynomial, encrypted, and when you add two such boxes, the encrypted polynomials add in a way that makes the underlying complex vectors add too.</p><h3><span id="canonical-embedding-the-universal-translator">Canonical Embedding: The Universal Translator</span></h3><p>The <strong>canonical embedding</strong> $\sigma$ is our universal translator. It takes a polynomial $p(X) &#x3D; \sum_{i&#x3D;0}^{n-1} p_i X^i$ and evaluates it at all our special roots:</p><p style="text-align:center;">$\sigma(p) = (p(\zeta_{2n}), p(\zeta_{2n}^3), \ldots, p(\zeta_{2n}^{2n-1}))$</p><p>This is like asking: “If this polynomial were a person, what would it say when you ask it about each of these $n$ roots?”</p><h3><span id="encoding-complex-vectors">Encoding Complex Vectors</span></h3><p>To encode a vector $\mathbf{z} &#x3D; (z_0, z_1, \ldots, z_{n&#x2F;2-1}) \in \mathbb{C}^{n&#x2F;2}$, we:</p><ol><li><strong>Extend to Conjugates</strong>: Create $\tilde{\mathbf{z}} &#x3D; (z_0, \ldots, z_{n&#x2F;2-1}, \overline{z_{n&#x2F;2-1}}, \ldots, \overline{z_0}) \in \mathbb{C}^n$</li><li><strong>Inverse Canonical Embedding</strong>: Compute $p &#x3D; \sigma^{-1}(\tilde{\mathbf{z}})$</li><li><strong>Scaling and Rounding</strong>: Scale by $\Delta$ and round to get integer coefficients</li></ol><p>The inverse canonical embedding can be computed efficiently using the <strong>Vandermonde matrix</strong>:</p><p style="text-align:center;">$V = \begin{pmatrix}1 & \zeta_{2n} & \zeta_{2n}^2 & \cdots & \zeta_{2n}^{n-1} \\1 & \zeta_{2n}^3 & (\zeta_{2n}^3)^2 & \cdots & (\zeta_{2n}^3)^{n-1} \\\vdots & \vdots & \vdots & \ddots & \vdots \\1 & \zeta_{2n}^{2n-1} & (\zeta_{2n}^{2n-1})^2 & \cdots & (\zeta_{2n}^{2n-1})^{n-1}\end{pmatrix}$</p><p>Then $\mathbf{p} &#x3D; V^{-1} \tilde{\mathbf{z}}$, which can be computed using FFT in $O(n \log n)$ time.</p><h3><span id="homomorphic-operations">Homomorphic Operations</span></h3><p>Once we have polynomials encoding our complex vectors, homomorphic operations correspond to:</p><ul><li><strong>Addition</strong>: $\text{Enc}(\mathbf{z}_1) + \text{Enc}(\mathbf{z}_2) &#x3D; \text{Enc}(\mathbf{z}_1 + \mathbf{z}_2)$</li><li><strong>Multiplication</strong>: $\text{Enc}(\mathbf{z}_1) \cdot \text{Enc}(\mathbf{z}_2) &#x3D; \text{Enc}(\mathbf{z}_1 \odot \mathbf{z}_2)$</li></ul><p>where $\odot$ is component-wise multiplication.</p><h2><span id="discrete-gaussian-distributions-and-noise">Discrete Gaussian Distributions and Noise</span></h2><p>Here’s where we get into the “why is this secure?” part. The whole security of Ring-LWE comes down to adding the right kind of <strong>noise</strong> to our encryptions.</p><h3><span id="the-noise-intuition">The Noise Intuition</span></h3><p>Think of noise in encryption like <strong>static on a radio</strong>. If you’re trying to eavesdrop on a radio transmission, a little bit of static makes it hard to understand, but the intended recipient (who knows what to listen for) can still decode the message.</p><p>But here’s the crucial insight: <strong>not all noise is created equal</strong>. Random uniform noise would work, but it turns out that <strong>Gaussian noise</strong> is optimal for lattice-based cryptography. Why? Because it plays nicely with the geometric structure of lattices.</p><h3><span id="discrete-gaussian-distribution-nature-s-favorite-noise">Discrete Gaussian Distribution: Nature’s Favorite Noise</span></h3><p>The discrete Gaussian distribution $D_{\mathbb{Z}, \sigma}$ is like a bell curve, but restricted to integers. Picture a normal distribution centered at 0, and then only keep the probability mass at integer points.</p><p>The parameter $\sigma$ controls the “width” of the bell curve:</p><ul><li>Small $\sigma$: Noise is concentrated near 0 (good for correctness, bad for security)</li><li>Large $\sigma$: Noise is spread out (good for security, bad for correctness)</li></ul><p>Mathematically: $\Pr[x] \approx \frac{1}{\sigma} \exp(-\pi x^2 &#x2F; \sigma^2)$</p><h3><span id="why-gaussian-the-geometric-intuition">Why Gaussian? The Geometric Intuition</span></h3><p>Here’s the deep reason why Gaussian noise is special: <strong>it’s the “most round” distribution</strong>. </p><p>When we add Gaussian noise to each coefficient of our polynomial, we’re essentially placing our secret in a “fuzzy cloud” that looks the same from every direction. This isotropy (rotational symmetry) is exactly what we need to make lattice problems hard.</p><p>If we used, say, uniform noise on ${-B, \ldots, B}$, our noise cloud would be cube-shaped, and cubes have corners where an attacker might find patterns. Gaussian clouds are perfectly round - no corners to exploit!</p><h3><span id="error-polynomial-sampling">Error Polynomial Sampling</span></h3><p>For Ring-LWE, we sample error polynomials $e(X) &#x3D; \sum_{i&#x3D;0}^{n-1} e_i X^i$ where each coefficient $e_i \leftarrow D_{\mathbb{Z}, \sigma}$ independently.</p><p>The <strong>norm</strong> of the error polynomial in the canonical embedding is:</p><p style="text-align:center;">$\|\sigma(e)\|_{\infty} = \max_{i} |e(\zeta_{2n}^{2i+1})|$</p><p>By concentration inequalities, with high probability:</p><p style="text-align:center;">$\|\sigma(e)\|_{\infty} \leq \sigma \sqrt{n \log n}$</p><p>This bound is crucial for correctness - we need the noise to be small enough that decryption works, but large enough for security.</p><h2><span id="security-foundations-ideal-lattices">Security Foundations: Ideal Lattices</span></h2><p>Now for the million-dollar question: <strong>why is Ring-LWE hard to break?</strong> The answer lies in geometry - specifically, the geometry of high-dimensional lattices.</p><h3><span id="the-lattice-intuition">The Lattice Intuition</span></h3><p>Think of a <strong>lattice</strong> as a regular arrangement of points in space, like the integer grid $\mathbb{Z}^2$ in the plane. But instead of 2D, we’re working in $n$-dimensional space where $n$ might be 1024 or 4096.</p><p>When we use Ring-LWE, we’re essentially <strong>hiding our secret in the lattice structure</strong>. The Ring-LWE samples give an adversary some information about which lattice we’re using, but not enough to reconstruct the secret.</p><h3><span id="from-polynomials-to-geometric-points">From Polynomials to Geometric Points</span></h3><p>Here’s the key connection: every polynomial $p(X) &#x3D; \sum p_i X^i$ corresponds to a point $(p_0, p_1, \ldots, p_{n-1})$ in $n$-dimensional space via the <strong>coefficient embedding</strong>.</p><p>The polynomial ring structure gives us a lattice with special properties - it’s not just any random lattice, but an <strong>ideal lattice</strong> with tons of symmetry. This extra structure is both a blessing (it makes things efficient) and potentially a curse (it might make attacks easier).</p><h3><span id="the-fundamental-lattice-problems">The Fundamental Lattice Problems</span></h3><p>Breaking Ring-LWE is equivalent to solving one of these problems on ideal lattices:</p><ol><li><strong>Shortest Vector Problem (SVP)</strong>: Given a lattice, find the shortest non-zero vector</li><li><strong>Closest Vector Problem (CVP)</strong>: Given a lattice and a target point, find the closest lattice point</li></ol><p>In 2D, these problems are easy - you can just look at the lattice and see the answer. But in 1024-dimensional space? Good luck! The number of lattice points to check grows exponentially with dimension.</p><h3><span id="worst-case-to-average-case-reduction">Worst-Case to Average-Case Reduction</span></h3><p>Here’s the beautiful part: Lyubashevsky, Peikert, and Regev showed that <strong>solving Ring-LWE on average is as hard as solving worst-case problems on ideal lattices</strong>.</p><p>Specifically, there’s a quantum reduction from:</p><ul><li><strong>Worst-case</strong>: $\gamma$-approximate Shortest Vector Problem (SVP) on ideal lattices in $\mathbb{Z}[X]&#x2F;(X^n + 1)$</li><li><strong>Average-case</strong>: Ring-LWE with Gaussian error width $\sigma &#x3D; \gamma \cdot \text{poly}(n)$</li></ul><p>This gives us confidence that Ring-LWE is hard even against quantum adversaries (though the reduction is quantum).</p><h3><span id="geometry-of-the-canonical-embedding">Geometry of the Canonical Embedding</span></h3><p>The canonical embedding $\sigma$ maps the ring to $\mathbb{C}^n$, but we can view this as $\mathbb{R}^{2n}$ by separating real and imaginary parts.</p><p>The key insight is that $\sigma$ is an <strong>isometry</strong> up to scaling: for any $p \in R$:</p><p style="text-align:center;">$\|\sigma(p)\|_2^2 = n \cdot \|p\|_2^2$</p><p>where $|p|<em>2^2 &#x3D; \sum</em>{i&#x3D;0}^{n-1} p_i^2$ is the coefficient norm.</p><p>This geometric structure is what makes lattice algorithms like LLL and BKZ work on ideal lattices.</p><h2><span id="parameter-selection-and-trade-offs">Parameter Selection and Trade-offs</span></h2><p>Choosing parameters for Ring-LWE schemes involves balancing security, correctness, and efficiency.</p><h3><span id="security-level">Security Level</span></h3><p>The security level depends on the <strong>root Hermite factor</strong> $\gamma$ of the best known lattice attacks. For $\lambda$-bit security, we need:</p><p style="text-align:center;">$\gamma^{2n} \geq 2^{\lambda / 2}$</p><p>Current estimates give $\gamma \approx 1.0045$ for BKZ attacks, so for 128-bit security:</p><p style="text-align:center;">$n \geq \frac{\lambda}{2 \log_2(\gamma)} \approx \frac{128}{2 \cdot 0.0065} \approx 10000$</p><p>But this is overly conservative. Practical parameter sets like those in Microsoft SEAL use $n \in {1024, 2048, 4096, 8192}$ with carefully chosen moduli.</p><h3><span id="modulus-selection">Modulus Selection</span></h3><p>The ciphertext modulus $q$ needs to be large enough to:</p><ol><li><strong>Avoid Modular Reduction Errors</strong>: $q &gt; \sigma \sqrt{n} \cdot 2^{\text{depth}}$</li><li><strong>Enable NTT</strong>: $q \equiv 1 \pmod{2n}$ for efficient polynomial multiplication</li><li><strong>Resist Attacks</strong>: Not too small relative to $n$</li></ol><p>A common approach is to use a <strong>product of primes</strong>: $q &#x3D; \prod_{i&#x3D;1}^k q_i$ where each $q_i \equiv 1 \pmod{2n}$.</p><h3><span id="noise-growth-analysis">Noise Growth Analysis</span></h3><p>In CKKS, after $d$ levels of multiplication, the noise magnitude grows roughly as:</p><p style="text-align:center;">$\text{Noise} \approx \sigma \cdot n^{d/2} \cdot B^d$</p><p>where $B$ is the bound on message coefficients. This exponential growth limits the multiplicative depth.</p><h2><span id="implementation-considerations">Implementation Considerations</span></h2><h3><span id="number-theoretic-transform">Number Theoretic Transform</span></h3><p>Polynomial multiplication in $\mathbb{Z}_q[X]&#x2F;(X^n + 1)$ can be done efficiently using NTT. For this to work, we need:</p><ol><li>$q \equiv 1 \pmod{2n}$</li><li>A primitive $2n$-th root of unity $\omega$ in $\mathbb{Z}_q$</li></ol><p>Then NTT is just FFT over $\mathbb{Z}_q$, giving us $O(n \log n)$ polynomial multiplication.</p><h3><span id="memory-and-bandwidth">Memory and Bandwidth</span></h3><p>Ciphertext size is a practical concern. A Ring-LWE ciphertext consists of two polynomials, so $2n \log q$ bits. For $n &#x3D; 4096$ and $\log q &#x3D; 200$, that’s about 200KB per ciphertext.</p><p>Techniques like <strong>ciphertext packing</strong> and <strong>batching</strong> help amortize this cost across multiple plaintexts.</p><h3><span id="precision-and-scaling">Precision and Scaling</span></h3><p>CKKS uses fixed-point arithmetic, so precision loss accumulates. The <strong>precision budget</strong> determines how many operations you can perform before precision becomes unacceptable.</p><p>After each multiplication, you typically need to <strong>rescale</strong> by dividing by some factor (and rounding), which reduces the ciphertext modulus but maintains precision.</p><p>The math here gets pretty involved, but basically you’re managing a trade-off between precision, noise, and remaining multiplicative depth.</p><h2><span id="looking-ahead">Looking Ahead</span></h2><p>This foundation sets us up perfectly for understanding more advanced topics like:</p><ul><li><strong>Automorphisms</strong>: How to rotate encrypted data using Galois theory</li><li><strong>Key Switching</strong>: Converting between different key representations</li><li><strong>Bootstrapping</strong>: Refreshing noise to enable unlimited computation</li><li><strong>Multi-Party Computation</strong>: Distributed computation on encrypted data</li></ul><p>The algebraic structure we’ve explored - cyclotomic rings, canonical embeddings, discrete Gaussians - forms the mathematical backbone of all these techniques.</p><p>I have to say, working through this math really drives home how elegant modern cryptography is. Like, the fact that we can pack complex numbers into polynomials, perform arithmetic homomorphically, and base security on lattice problems - it’s just beautiful mathematics serving practical ends.</p><p>In my next post, I’ll dive into automorphisms and key switching, building directly on these foundations. The Galois group action on cyclotomic rings gives us this incredible ability to rotate encrypted data, but it comes with the price of changing the encryption key. Key switching is the technique that lets us convert back to the original key, and the math behind it is pretty wild.</p><p>Anyway, hope this gives you a solid mathematical foundation for understanding modern FHE schemes. There’s obviously way more depth here - ideal lattices, algebraic number theory, concrete security analysis - but this should be enough to follow the more advanced techniques.</p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] Lyubashevsky, V., Peikert, C., &amp; Regev, O. (2010). On ideal lattices and learning with errors over rings. In Annual international conference on the theory and applications of cryptographic techniques (pp. 1-23). Springer.</p><p>[2] Cheon, J. H., Kim, A., Kim, M., &amp; Song, Y. (2017). Homomorphic encryption for arithmetic of approximate numbers. In International Conference on the Theory and Application of Cryptology and Information Security (pp. 409-437). Springer.</p><p>[3] Brakerski, Z., Gentry, C., &amp; Vaikuntanathan, V. (2012). (Leveled) fully homomorphic encryption without bootstrapping. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (pp. 309-325).</p><p>[4] Regev, O. (2009). On lattices, learning with errors, random linear codes, and cryptography. Journal of the ACM, 56(6), 1-40.</p><p>[5] Peikert, C. (2016). A decade of lattice cryptography. Foundations and Trends in Theoretical Computer Science, 10(4), 283-424.</p><p>[6] Micciancio, D., &amp; Regev, O. (2009). Lattice-based cryptography. In Post-quantum cryptography (pp. 147-191). Springer.</p><p>[7] Smart, N. P., &amp; Vercauteren, F. (2014). Fully homomorphic SIMD operations. Designs, codes and cryptography, 71(1), 57-81.</p><p>[8] Halevi, S., &amp; Shoup, V. (2014). Algorithms in HElib. In Annual Cryptology Conference (pp. 554-571). Springer.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Post Quantum Cryptography </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
            <tag> Abstract Algebra </tag>
            
            <tag> Ring Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Empirical Analysis of LWE Robustness Against Machine Learning Distinguishers</title>
      <link href="/2025/07/03/Breaking-LWE-Encryption/"/>
      <url>/2025/07/03/Breaking-LWE-Encryption/</url>
      
        <content type="html"><![CDATA[<p>The Learning With Errors (LWE) problem forms the foundation of many post-quantum cryptographic systems. These systems depend on a critical assumption: that no one can distinguish between LWE-generated samples and truly random data. I wanted to test this assumption by building sophisticated machine learning models to see if they could break this fundamental security property.</p><p>My goal wasn’t to completely “break” LWE—that would be a monumental achievement. Instead, I aimed to map out where LWE’s security boundaries actually lie in practice. Could a neural network, with its pattern-recognition capabilities, successfully identify LWE samples and violate the core security assumption? Through multiple iterations of model improvements and data refinement, I discovered just how resilient LWE really is. The results highlight why cryptographers rely on standardized libraries with carefully chosen parameters backed by years of analysis.</p><span id="more"></span><h2><span id="table-of-contents">Table of Contents</span></h2><ol><li><a href="#the-learning-with-errors-lwe-problem">The Learning With Errors (LWE) Problem</a></li><li><a href="#methodology-a-machine-learning-based-auditor">Methodology: A Machine Learning-Based Auditor</a></li><li><a href="#a-multi-stage-analytical-process">A Multi-Stage Analytical Process</a><ul><li><a href="#stage-i-the-baseline-model-and-overfitting">Stage I: The Baseline Model and Overfitting</a></li><li><a href="#stage-ii-feature-engineering-for-modular-arithmetic">Stage II: Feature Engineering for Modular Arithmetic</a></li><li><a href="#stage-iii-architectural-refinement-with-a-1d-cnn">Stage III: Architectural Refinement with a 1D CNN</a></li><li><a href="#stage-iv-a-focused-analysis-of-the-output-distribution">Stage IV: A Focused Analysis of the Output Distribution</a></li></ul></li><li><a href="#results-and-discussion-the-intractability-of-the-lwe-signal">Results and Discussion: The Intractability of the LWE Signal</a></li><li><a href="#conclusion">Conclusion</a></li></ol><h2><span id="the-learning-with-errors-lwe-problem">The Learning With Errors (LWE) Problem</span></h2><p>The LWE problem rests on the difficulty of recovering a secret vector $\mathbf{s} \in \mathbb{Z}_q^n$ from noisy linear equations. Each LWE sample consists of a pair $(\mathbf{a}, b) \in \mathbb{Z}_q^n \times \mathbb{Z}_q$, where $b$ gets calculated as:</p><p style="text-align:center;">$b = \langle \mathbf{a}, \mathbf{s} \rangle + e \mod q$</p><p>Here, $\mathbf{a}$ represents a publicly known random vector, while $e$ is a small error term drawn from a discrete Gaussian distribution. LWE-based systems rely on two key assumptions:</p><ol><li><strong>Search-LWE</strong>: Finding $\mathbf{s}$ is computationally infeasible</li><li><strong>Decision-LWE</strong>: Distinguishing LWE samples from uniformly random pairs is impossible</li></ol><p>My analysis focuses on this second assumption.</p><h2><span id="methodology-a-machine-learning-based-auditor">Methodology: A Machine Learning-Based Auditor</span></h2><p>I built a machine learning distinguisher as my primary analytical tool—essentially a neural network trained as a binary classifier to tackle the Decision-LWE problem. The model receives two types of data:</p><ul><li><strong>Positive Class (Label 1)</strong>: Authentic LWE samples $(\mathbf{a}, b)$</li><li><strong>Negative Class (Label 0)</strong>: Uniformly random pairs $(\mathbf{a}, u)$</li></ul><p>The model’s accuracy on unseen test data directly measures how distinguishable LWE samples are for specific parameter sets. Any accuracy substantially above 50% would signal a practical weakness, suggesting the model found a generalizable statistical pattern. I designed the experiment to map this accuracy across different LWE parameters, varying both the dimension (n) and noise magnitude (sigma).</p><h2><span id="a-multi-stage-analytical-process">A Multi-Stage Analytical Process</span></h2><p>This investigation didn’t follow a straight path—early failures actually proved crucial in refining my approach and revealing deeper insights about the problem’s complexity.</p><h3><span id="stage-i-the-baseline-model-and-overfitting">Stage I: The Baseline Model and Overfitting</span></h3><p>My first attempt used a standard Multi-Layer Perceptron (MLP) trained on complete $(\mathbf{a}, b)$ pairs. The model consistently failed to generalize, with test accuracy stuck at 50%. The training logs showed classic severe overfitting: while training accuracy climbed, validation performance stayed at random chance levels. This meant the model was just memorizing training artifacts rather than learning the actual mathematical structure.</p><h3><span id="stage-ii-feature-engineering-for-modular-arithmetic">Stage II: Feature Engineering for Modular Arithmetic</span></h3><p>I suspected the model’s failure stemmed from a fundamental data representation mismatch. Neural networks treat numbers linearly, but LWE operates in a modular ring where $q-1$ sits right next to $0$. To fix this, I implemented circular embeddings, transforming each integer x into a 2D vector (cos(2πx&#x2F;q), sin(2πx&#x2F;q)). This feature engineering explicitly gave the model an understanding of modular proximity.</p><p>Despite this major improvement in data representation, the model still couldn’t generalize—<strong>severe overfitting</strong> persisted. This suggested the problem wasn’t just about data format but something more fundamental.</p><h3><span id="stage-iii-architectural-refinement-with-a-1d-cnn">Stage III: Architectural Refinement with a 1D CNN</span></h3><p>The LWE problem has an inherently sequential structure—$b$ equals the sum of component-wise products. Standard MLPs aren’t architecturally suited for capturing these relationships. I switched to a <strong>1D Convolutional Neural Network (CNN)</strong>, specifically designed to identify local patterns in sequential data. I also added <strong>L2 Regularization</strong> to penalize model complexity and reduce overfitting.</p><p>The results were striking: training accuracy shot up dramatically, proving the CNN was far more capable. However, validation accuracy stayed flat at 50%. This was a <strong>critical discovery</strong>: even with proper data representation and a powerful, regularized architecture, the LWE secret’s signal was too diluted across the high-dimensional input space for the model to learn any generalizable pattern. This provided experimental evidence of the “curse of dimensionality” serving as LWE’s core security feature.</p><h3><span id="stage-iv-a-focused-analysis-of-the-output-distribution">Stage IV: A Focused Analysis of the Output Distribution</span></h3><p>After consistently failing to distinguish complete $(\mathbf{a}, b)$ pairs, I formed a final, more focused hypothesis. If the full problem proves intractable, maybe the LWE process leaves a detectable statistical bias in the distribution of b values alone.</p><p>I redesigned the experiment to train my most sophisticated model—the regularized 1D CNN with circular embeddings—on a simpler task: distinguishing collections of LWE-generated b values from collections of uniformly random integers.</p><h2><span id="results-and-discussion-the-intractability-of-the-lwe-signal">Results and Discussion: The Intractability of the LWE Signal</span></h2><p>The final experiment confirmed what cryptographers have long theorized. Across all tested parameters, including deliberately weakened ones, the model failed to distinguish the distribution of LWE b values from the uniform distribution. Test accuracy remained locked at 50%.</p><p>LWE’s resistance to machine learning attacks is well-established in theory, and this empirical evidence reinforces that understanding. Even with sophisticated neural architectures and optimized data representations, the statistical signature of the secret remains undetectable. The high-dimensional dot product combined with additive noise creates such effective diffusion that no learnable patterns emerge, even under conditions favorable to the attacker.</p><h2><span id="conclusion">Conclusion</span></h2><p>This investigation into LWE’s boundaries provides concrete evidence supporting the theoretical foundations of post-quantum cryptography. Even deliberately weakened LWE instances proved robust against sophisticated statistical analysis using modern machine learning techniques, including deep convolutional neural networks. The consistent inability of these models to generalize reinforces the mathematical principles underlying LWE’s security.</p><p>The results underscore a fundamental principle in applied cryptography: parameter selection matters enormously. The resilience observed even with suboptimal parameters demonstrates why cryptographers rely on standardized, peer-reviewed libraries where parameters undergo extensive analysis to ensure substantial security margins against known attack vectors.</p><blockquote><p><strong>Note</strong>: The full Python script for this analysis is available at this <a href="https://github.com/DA1729/lwe_ml_attack.git">GitHub repo</a> for review and further experimentation.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography </tag>
            
            <tag> Cryptanalysis </tag>
            
            <tag> Post Quantum Cryptography </tag>
            
            <tag> Fully Homomorphic Encryption </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lottery Ticket Hypothesis Part 2</title>
      <link href="/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/"/>
      <url>/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/</url>
      
        <content type="html"><![CDATA[<h2><span id="iterative-pruning-and-finding-the-winning-ticket">Iterative Pruning and Finding the Winning Ticket</span></h2><p>So far, we’ve talked about the idea that there’s a smaller subnetwork—our so-called winning ticket—hidden within a big neural network. But how do we actually find this winning ticket? That’s where <strong>iterative pruning</strong> steps in.</p><h3><span id="the-iterative-pruning-process">The Iterative Pruning Process</span></h3><span id="more"></span><p>Instead of pruning once and hoping we get lucky, iterative pruning does the following:</p><ul><li><strong>Train the full network</strong> for a fixed number of iterations.</li><li><strong>Prune a small percentage</strong> (say, 10%-20%) of the lowest magnitude weights.</li><li><strong>Reset the remaining weights back</strong> to their original initialization.</li><li><strong>Repeat steps 1–3</strong> for several rounds.</li></ul><p>This slow and steady process lets us uncover subnetworks that are small but still highly capable—our winning tickets.</p><h3><span id="why-iterative-pruning-works-better">Why Iterative Pruning Works Better</span></h3><p>Turns out, one-shot pruning (cutting lots of weights at once) often fails to find the best subnetworks, especially when we go too small. Iterative pruning, on the other hand, carefully preserves the parts of the network that matter, leading to <strong>better performance at smaller sizes</strong>.</p><hr><p>In the experiments, they could reduce the network size by up to 90%, and the resulting subnetworks still learned faster and better than the full network!</p><hr><h2><span id="do-winning-tickets-generalize-better">Do Winning Tickets Generalize Better?</span></h2><p>Now here’s where things get spicy. When comparing test accuracies, the researchers noticed something curious:</p><ul><li>The winning tickets not only learned faster,</li><li>They often had <strong>better generalization</strong> than the original model!</li></ul><p>This means that they didn’t just memorize training data—they actually learned to perform better on unseen test data.</p><p>This idea is related to something called <strong>Occam’s Hill</strong>—too big and you overfit, too small and you underfit. Winning tickets land at a sweet spot: small enough to avoid overfitting, but just right to still learn effectively.</p><h2><span id="initialization-matters-a-lot">Initialization Matters (A Lot)</span></h2><p>Another key takeaway: it’s not just the structure of the subnetwork that matters. It’s also the <strong>exact initial weights</strong>.</p><p>If you take a winning ticket’s structure and randomly reinitialize it, it <strong>loses its magic</strong>—learning slows down and performance drops.</p><h2><span id="expanding-to-convolutional-networks">Expanding to Convolutional Networks</span></h2><p>The authors didn’t just test on simple fully-connected networks like LeNet on MNIST. They also ran experiments on <strong>convolutional networks</strong> like Conv-2, Conv-4, and Conv-6 on CIFAR-10.</p><p>Surprise surprise: they found <strong>winning tickets</strong> there too. In fact, the same pattern repeated:</p><ul><li>Winning tickets learn faster</li><li>They reach higher accuracy</li><li>They generalize better</li><li>Initialization still matters</li></ul><p>The success wasn’t limited to toy datasets—this was happening on moderately complex image classification tasks too.</p><h2><span id="drop-out-pruning">Drop-Out + Pruning</span></h2><p>What happens when you combine <strong>dropout</strong> with pruning?*</p><p>Turns out, dropout helps too! Dropout already encourages the network to be robust to missing connections. So when you prune, the network is more resilient.</p><p>When they trained networks <strong>with dropout</strong> and applied iterative pruning, the test accuracy <strong>improved even further</strong>. This hints that dropout may help in preparing the network for successful pruning.</p><h2><span id="the-big-leagues-vgg-19-and-resnet-18">The Big Leagues: VGG-19 and RESNET-18</span></h2><p>Taking it up a notch, the paper also tested on deeper, real-world architectures:</p><ul><li><strong>VGG-19</strong></li><li><strong>ResNet-18</strong></li></ul><p>The pattern mostly held up—but with a twist. For these deep networks, iterative pruning only worked well if they used <strong>learning rate warm-up</strong>.</p><p>Without warm-up, pruning failed to find winning tickets. With warm-up (i.e., slowly increasing the learning rate at the beginning of training), they were back in business.</p><p>So yes—winning tickets exist even in deep networks, but only if you treat them with care.</p><h2><span id="key-takeaways">Key Takeaways</span></h2><ul><li>Big neural networks contain hidden winning tickets—smaller subnetworks that can be trained to match or exceed full network performance.</li><li>You find them by <strong>pruning</strong> and <strong>resetting</strong> repeatedly.</li><li>These subnetworks not only match accuracy, but often learn <strong>faster</strong> and generalize <strong>better</strong>.</li><li>The <strong>initialization</strong> is crucial—you can’t just randomly reinitialize and expect the same results.</li><li>Even deeper networks like VGG and ResNet have winning tickets, but they may require careful tuning (e.g., learning rate warm-up).</li><li>Pruning isn’t just for compression—it might teach us something deep about how neural networks work.</li></ul><p>Now, what I am trying to do is replecating the results found by the authors myself. So stick around and keep a look over my <a href="https://github.com/DA1729">GitHub</a>.</p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lottery Ticket Hypothesis Part-1</title>
      <link href="/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/"/>
      <url>/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/</url>
      
        <content type="html"><![CDATA[<p>So, I am about to start on a new project realted to implementation of Neural Networks on FPGAs, you for acceleration purposes.</p><span id="more"></span><p>Also, I have a new website design which I am absolutely loving (credits to Freemind.386 and others mentioned at the bottom of the page). Yeah so I was reading about the implementation process of AI Inferences on FPGAs and the first step happened to be opimizing the model itself for hardware implementation and the Vitis AI page mentioned “Pruning” as one of the techniques, and obviously I went to the rabbit-hole (side track) and came across this very interesting hypothesis (one metioned in the title) and found the original paper referenced below. Since, I am not “the AI expert” as we have around ourselves, so understanding this paper is not the easiest work for me. I am reading it as a person who only knows the basics of AI and Neural Networks, hence the following blog is written in a very intuitive manner (at least to satisfy my intuition) and obviously if I am serious enough to write a blog and leave all the other somewhat more important stuff aside for now, I won’t half assedly read the paper and the just yap over the blog. Also if the further sentences don’t sound like me, that’s cuz I asked ChatGPT to turn my notes into a blog, so there is everything I have understood from the paper but in different wordings, so that should be fine ig…. enjoy! Also a side note, this new style just capitalize everything written in Markdown bold, so I am not screaming at you just highlighting that point.</p><h2><span id="pruning">Pruning</span></h2><p>Deep neural networks, especially fully connected ones, tend to be overparameterized. While this over-parameterization can be useful for training, it also makes models heavy and resource-intensive. To tackle this, we use pruning—a process where we remove unnecessary weights from the network.</p><p>Surprisingly, pruning can reduce parameter counts by over 90% without harming the model’s accuracy. But this raises a natural question:</p><p><strong>If we can prune a trained model down to a smaller size without losing accuracy, why not just train a smaller model from the start?</strong></p><p>Turns out, that doesn’t quite work.</p><h2><span id="why-not-train-small-from-the-beginning">Why Not Train Small from the Beginning</span></h2><p>Experiments show that architectures uncovered by pruning—despite being smaller and sufficient are harder to train from scratch. When randomly initialized and trained in isolation, these pruned networks often fail to reach the same level of accuracy as the full-sized original network.</p><p>However, there’s a clever workaround.</p><p>After pruning a model, we can retain the original weights (instead of random reinitialization) and then retrain the original model. This makes the learning process much faster than randomly initializing the values over some distribution. </p><h2><span id="an-experiment-in-sparsity">An Experiment in Sparsity</span></h2><p>To understand this better, let’s consider a basic experiment:</p><ul><li>Start with a fully connected convolutional neural network (CNN).</li><li>Perform unstructured pruning by randomly removing connections.</li><li>Train the pruned network while tracking the iteration with the minimum validation loss.</li><li>Evaluate the final test accuracy at this “best” iteration.</li></ul><p>The observations were telling:</p><p><strong>Sparser networks learn slower and tend to be less accurate.</strong></p><p>Below are plots illustrating this trend. In the first two graphs, as the percentage of remaining weights decreases, the number of iterations to reach peak performance increases. In the last two, we see test accuracy steadily drop with increased pruning.</p><p><img src="/images/8.png" alt="Image Credits: [1]"></p><h2><span id="the-lottery-ticket-hypothesis">The Lottery Ticket Hypothesis</span></h2><p>From this behavior emerges the Lottery Ticket Hypothesis, first introduced by Jonathan Frankle and Michael Carbin in 2018. The hypothesis makes a bold but fascinating claim:</p><p><strong>A randomly-initialized, dense neural network contains a subnetwork (a “winning ticket”) that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.</strong></p><p>Let’s formalize this:</p><ul><li><p>Start with a dense feedforward neural network $f(x;\theta)$, where the initial parameters $\theta_0$ are sampled randomly from a distribution $D_{\theta_0}$.</p></li><li><p>Train this network using <strong>Stochastic Gradient Descent (SGD)</strong>, a technique that updates weights using small, random batches of data rather than the full dataset at each step.</p></li><li><p>Let the training reach its minimum validation loss $l$ at iteration $j$ with test accuracy $a$.</p></li></ul><p>Now introduce a binary mask $m \in {0, 1}^{|\theta|}$, which indicates which parameters are kept (1) and which are pruned (0). We now train a new network, $f(x; m \cdot \theta)$, using the same initialization for the remaining parameters.</p><p>According to the Lottery Ticket Hypothesis, there exists such a mask $m$ for which:</p><ul><li>The pruned model reaches minimum validation loss $l’$ at iteration $j’$, where $j’ \leq j$,</li><li>It achieves a test accuracy $a’ \geq a$,</li><li>And it uses fewer parameters than the original model $|m|_0 &lt; |\theta|$.</li></ul><p>In simpler terms: hidden within every big neural network is a smaller, efficient subnetwork—a winning ticket—that can be trained just as well if it’s initialized correctly.</p><h2><span id="what-counts-as-a-winning-ticket">What Counts as a Winning Ticket?</span></h2><p>These “winning tickets” are usually uncovered through standard pruning techniques applied to fully-connected or convolutional feedforward networks.</p><p>It’s crucial to note that simply reinitializing these sub-networks randomly strips away their special status. When reinitialized, these subnetworks lose their performance edge, emphasizing the importance of the initial weight configuration.</p><p>Now, we get a feel for why we are calling them lottery tickets, because out of random initial parameters, for only a specific initialization of parameters result in the sub-network matches the performance of the original network</p><h2><span id="how-to-identify-the-winning-tickets">How to identify the Winning Tickets</span></h2><ul><li><p>Randomly initialize a neural network $f(x;\theta_0)$ where ($\theta_0 \sim D_\theta$).</p></li><li><p>Train the network for $j$ iterations, arriving at parameters $\theta_j$.</p></li><li><p>Prune $p%$ of the parameters in $\theta_j$, creating a mask $m$.</p></li><li><p>Reset the remaining parameters to their values in $\theta_0$, creating the winning ticket $f(x; m\cdot \theta_0)$.</p></li></ul><p>This approach of identifying the winning tickets is a one-shot approach. But, the original paper focuses more upon the iterative pruning, i.e., repeatedly train, prune and reset the network for over $n$ rounds; each round we prune $p^\frac{1}{n}%$ of the weights that survived the previous rounds.</p><p>Results suggest that iterative pruning finds winning tickets that match the accuracy of the original network at smaller sizes than does one-shot pruning.</p><p>Let’s end this part right here, next part, we dive into more experimental results and of-course, iterative pruning. </p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI - Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In Memory Computation using Analog Part 2</title>
      <link href="/2025/03/18/In-Memory-Computation-using-Analog-Part-2/"/>
      <url>/2025/03/18/In-Memory-Computation-using-Analog-Part-2/</url>
      
        <content type="html"><![CDATA[<h2><span id="matrix-multiplication-through-mac-operations">Matrix Multiplication through MAC operations</span></h2><p>Below, I have presented a python code, illustrating matrix multiplication using MAC operation. But, why matrix multiplication only? Because everything is a fking MATRIX!!! (that’s why the film is called Matrix). Physicists, electrical engineers, computer scientists&#x2F;engineers just love representing everything in matrix, and why not, they make everything more streamlined and easy to represent. Since, we are representing everything in matrices, especially in machine learning and AI, like we have the weights matrices, input vectors, output vectors, etc., we have to do a lot of matrix multiplication and in hardware, using MAC operators, we can easily perform it. Now, carefully look and understand the python code below:</p><span id="more"></span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiply_mac</span>(<span class="params">A, B</span>):</span><br><span class="line"></span><br><span class="line">    A = np.array(A)</span><br><span class="line">    B = np.array(B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> A.shape[<span class="number">1</span>] != B.shape[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Matrix dimensions do not match for multiplication.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    C = np.zeros((A.shape[<span class="number">0</span>], B.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Can you explicitly see me using the MAC operation here? what is the accumulator?</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(B.shape[<span class="number">1</span>]):</span><br><span class="line">            mac = <span class="number">0</span>  </span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">1</span>]):</span><br><span class="line">                mac += A[i][k] * B[k][j]  </span><br><span class="line">            C[i][j] = mac</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> C</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">B = [[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">result = matrix_multiply_mac(A, B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Resultant Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>Now, that (I hope) you have read and understood the code above, one can realize that we can use the circuit we designed in the previous part for the same operation. Hence, we can do matrix multiplication through analog computing now, how cool!</p><p>But why should we go for analog rather than digital? In digital, the energy complexity grows a lot faster as the number of bits are increased, speaking with numbers, an 8-bit MAC energy can be 100 times the energy for 1 bit. </p><p>Let’s end this part here for now, as I wrote this very impulsively out a sudden motivation (and too keep the momentum going) and did not plan it too much before writing LMAO.</p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] J. -s. Seo et al., “Digital Versus Analog Artificial Intelligence Accelerators: Advances, trends, and emerging designs,” in IEEE Solid-State Circuits Magazine, vol. 14, no. 3, pp. 65-79, Summer 2022, doi: 10.1109&#x2F;MSSC.2022.3182935.<br>keywords: {AI accelerators;Market research;In-memory computing;Hardware;System analysis and design;Switching circuits},</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In-Memory Computation using Analog Part-1</title>
      <link href="/2025/03/15/In-Memory-Computation-using-Analog-Part-1/"/>
      <url>/2025/03/15/In-Memory-Computation-using-Analog-Part-1/</url>
      
        <content type="html"><![CDATA[<h2><span id="von-neumann-bottleneck">Von Neumann Bottleneck</span></h2><p>There has been an improvement in the number of transistors on a chip. More transistors mean that we have increased our ability to store more memory in less physical space. Memory storage is more efficient than ever.</p><p>Today, AI and machine learning are being studied. This requires us to store and process a large density of data, which is possible given the environment: processors and storage solutions. Also, Von Neumann Architecture requires us to store data in a separate block, and the processor needs an individual block. These different blocks are connected by buses. Given this architecture, to process these large-density data, the transfer rates must also be at par with the processing speed, maybe even faster. However, over the years, the increase in transfer speedhas only made a few gains.</p><span id="more"></span><p>When the processor has to stay idle to fetch the data from the memory block, this condition is called the <strong>Von-Neumann Bottleneck</strong>.</p><p>Some attempts to surpass this limitation have been made like: </p><ul><li><p><strong>Caching</strong>: Chaches are temporary storage units between the main memory block and the processor. It can store a subset of data so that future requests for that data can be served faster. For example, they store results of earlier computations or a copy of data stored elsewhere.</p></li><li><p><strong>Hardware Acceleration</strong>: Hardware like GPUs, FPGAs, and ASICs are brought into the picture for faster response from the hardware side.</p></li></ul><p>But these come with some limitations: </p><ul><li><p><strong>Limitations of Caching</strong>:</p><ul><li><p><strong>Size</strong>: Larger caches increase hit rates but consume more silicon area and power. </p></li><li><p>In multicore systems, maintaining consistency across caches is difficult.</p></li><li><p><strong>Memory Latency and Bandwidth Issues</strong>: If the working set exceeds capacity, frequent primary memory access still causes stalls.</p></li></ul></li><li><p><strong>Hardware Accelerators’ Limitations</strong>:</p><ul><li><p><strong>Domain-Specificity</strong>: FPGAs, TPUs, and GPUs lack generality. They are often made for specific tasks, which, economically speaking, makes them challenging to produce. </p></li><li><p>At the end of the day, communications are still being made over buses, so the transfer limitation persists. </p></li><li><p><strong>Software and Compatibility Issues</strong>: These devices run on specific firmware and can cause compatibility issues. </p></li><li><p><strong>Power and Heat Management</strong>: These hardware accelerators generate much heat and consume much power, which obviously isn’t preferable.</p></li></ul></li></ul><p>Now, we dive into analog methods of overcoming this phenomenon. Of course, some digital methods have been proposed but let’s stick to the title of the blog for now and maybe (definitely) I’ll discuss digital methods in a future blog.</p><h2><span id="analog-implementation-of-macs">Analog Implementation of MACS</span></h2><p>MAC, or Multiply-Accumulate Operation, is a common step which computes the product of two numbers and adds that product to an accumulator. MAC operations account for over 90% of Neural Network and AI computations. Yeah, so they are “kind of” important.</p><p>In the following circuit, we have 10 MOSFETs in total (5 PMOS, 5 CMOS), let us label them: <strong>PM<sub>1</sub></strong>, <strong>PM<sub>2</sub></strong>, <strong>PM<sub>3</sub></strong>, <strong>PM<sub>4</sub></strong>, <strong>PM<sub>5</sub></strong>, <strong>NM<sub>1</sub></strong>, <strong>NM<sub>2</sub></strong>, <strong>NM<sub>3</sub></strong>, <strong>NM<sub>4</sub></strong>, <strong>NM<sub>5</sub></strong>.</p><p>These MOSFETs are linearly biased (if you somewhat unfamiliar with working of MOSFET, go watch Engineering Mindset’s video on MOSFET on YouTube, I found it very good for a quick get around). We are applying differential inputs $+\Delta x, -\Delta x, +\Delta w , -\Delta w$.</p><p>The given transistors are now arranged in the following circuit (Image Courtesy: Reference [1]):</p><p><img src="/images/1.png" alt="MAC Operator"></p><p>Now, let’s get into some transistor math. </p><p>Since, all the transistors are operating in linear region, drain current <strong>I<sub>d2</sub></strong> is given by: </p><p style="text-align:center;">$I_{d2} =K_{n}*[V_{b}-\Delta w - V_{thn} - \frac{V_{b} + \Delta x}{2}]*(V_{b}+\Delta x)$</p><p>For knowing what each term means, refer to [1]. </p><p>Now, we are taking the transconductance factors and threshold voltages of the N and P MOSFETS to be equal, we get the following expression for the output current: </p><p style="text-align:center;">$I_{out} = 4*K*\Delta w * \Delta x$</p><p>If you observer the above expression, we have multiplied two numbers! Now, all we have left to do is accumulate.</p><p>The load MOSFETS: <strong>PM<sub>5</sub></strong> and <strong>NM<sub>5</sub></strong> can seen as an equivalent load resistor, which will convert the output current to an output voltage:</p><p style="text-align:center;">$\Delta y = V_{out}-V_{outbias}$</p><p>This can be easily visualized in the figure below: </p><p><img src="/images/2.png" alt="Image Courtesy : [1]"></p><p>Now, closely look at the (b) part of the above image, what we are doing is we are adding output currents of multiple sources (or I should say <strong>multipliers</strong>), such that the output voltage can be given by: </p><p style="text-align:center;">$V = \frac{1}{N}*\sum_{i=1}^{N} I_{i}*R_{load}$</p><p>With this, we have successfully created our analog MAC unit. Let us end this part-1 here. Next part, we will delve into experimental results, architecture, and maybe hybrid models proposed. </p><p>peace. da1729</p><h2><span id="references">References</span></h2><p>[1] J. Zhu, B. Chen, Z. Yang, L. Meng and T. T. Ye, “Analog Circuit Implementation of Neural Networks for In-Sensor Computing,” 2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), Tampa, FL, USA, 2021, pp. 150-156, doi: 10.1109&#x2F;ISVLSI51109.2021.00037. keywords: {Convolution;Neural networks;Linearity;Analog circuits;Very large scale integration;CMOS process;Silicon;Analog Computing;In-Sensor Computing;Edge Computing},</p><p>[2] Robert Sheldon, “von Neumann bottleneck”, TechTarget, <a href="https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running">https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
