<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Lottery Ticket Hypothesis for Beginners Part 2</title>
      <link href="/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/"/>
      <url>/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/</url>
      
        <content type="html"><![CDATA[<h2 id="Iterative-Pruning-and-Finding-the-Winning-Ticket"><a href="#Iterative-Pruning-and-Finding-the-Winning-Ticket" class="headerlink" title="Iterative Pruning and Finding the Winning Ticket"></a>Iterative Pruning and Finding the Winning Ticket</h2><p>So far, we’ve talked about the idea that there’s a smaller subnetwork—our so-called winning ticket—hidden within a big neural network. But how do we actually find this winning ticket? That’s where <strong>iterative pruning</strong> steps in.</p><h3 id="The-Iterative-Pruning-Process"><a href="#The-Iterative-Pruning-Process" class="headerlink" title="The Iterative Pruning Process"></a>The Iterative Pruning Process</h3><span id="more"></span><p>Instead of pruning once and hoping we get lucky, iterative pruning does the following:</p><ul><li><strong>Train the full network</strong> for a fixed number of iterations.</li><li><strong>Prune a small percentage</strong> (say, 10%-20%) of the lowest magnitude weights.</li><li><strong>Reset the remaining weights back</strong> to their original initialization.</li><li><strong>Repeat steps 1–3</strong> for several rounds.</li></ul><p>This slow and steady process lets us uncover subnetworks that are small but still highly capable—our winning tickets.</p><h3 id="Why-Iterative-Pruning-Works-Better"><a href="#Why-Iterative-Pruning-Works-Better" class="headerlink" title="Why Iterative Pruning Works Better"></a>Why Iterative Pruning Works Better</h3><p>Turns out, one-shot pruning (cutting lots of weights at once) often fails to find the best subnetworks, especially when we go too small. Iterative pruning, on the other hand, carefully preserves the parts of the network that matter, leading to <strong>better performance at smaller sizes</strong>.</p><hr><p>In the experiments, they could reduce the network size by up to 90%, and the resulting subnetworks still learned faster and better than the full network!</p><hr><h2 id="Do-Winning-Tickets-Generalize-Better"><a href="#Do-Winning-Tickets-Generalize-Better" class="headerlink" title="Do Winning Tickets Generalize Better?"></a>Do Winning Tickets Generalize Better?</h2><p>Now here’s where things get spicy. When comparing test accuracies, the researchers noticed something curious:</p><ul><li>The winning tickets not only learned faster,</li><li>They often had <strong>better generalization</strong> than the original model!</li></ul><p>This means that they didn’t just memorize training data—they actually learned to perform better on unseen test data.</p><p>This idea is related to something called <strong>Occam’s Hill</strong>—too big and you overfit, too small and you underfit. Winning tickets land at a sweet spot: small enough to avoid overfitting, but just right to still learn effectively.</p><h2 id="Initialization-Matters-A-Lot"><a href="#Initialization-Matters-A-Lot" class="headerlink" title="Initialization Matters (A Lot)"></a>Initialization Matters (A Lot)</h2><p>Another key takeaway: it’s not just the structure of the subnetwork that matters. It’s also the <strong>exact initial weights</strong>.</p><p>If you take a winning ticket’s structure and randomly reinitialize it, it <strong>loses its magic</strong>—learning slows down and performance drops.</p><h2 id="Expanding-to-Convolutional-Networks"><a href="#Expanding-to-Convolutional-Networks" class="headerlink" title="Expanding to Convolutional Networks"></a>Expanding to Convolutional Networks</h2><p>The authors didn’t just test on simple fully-connected networks like LeNet on MNIST. They also ran experiments on <strong>convolutional networks</strong> like Conv-2, Conv-4, and Conv-6 on CIFAR-10.</p><p>Surprise surprise: they found <strong>winning tickets</strong> there too. In fact, the same pattern repeated:</p><ul><li>Winning tickets learn faster</li><li>They reach higher accuracy</li><li>They generalize better</li><li>Initialization still matters</li></ul><p>The success wasn’t limited to toy datasets—this was happening on moderately complex image classification tasks too.</p><h2 id="Drop-Out-Pruning"><a href="#Drop-Out-Pruning" class="headerlink" title="Drop-Out + Pruning"></a>Drop-Out + Pruning</h2><p>What happens when you combine <strong>dropout</strong> with pruning?*</p><p>Turns out, dropout helps too! Dropout already encourages the network to be robust to missing connections. So when you prune, the network is more resilient.</p><p>When they trained networks <strong>with dropout</strong> and applied iterative pruning, the test accuracy <strong>improved even further</strong>. This hints that dropout may help in preparing the network for successful pruning.</p><h2 id="The-Big-Leagues-VGG-19-and-RESNET-18"><a href="#The-Big-Leagues-VGG-19-and-RESNET-18" class="headerlink" title="The Big Leagues: VGG-19 and RESNET-18"></a>The Big Leagues: VGG-19 and RESNET-18</h2><p>Taking it up a notch, the paper also tested on deeper, real-world architectures:</p><ul><li><strong>VGG-19</strong></li><li><strong>ResNet-18</strong></li></ul><p>The pattern mostly held up—but with a twist. For these deep networks, iterative pruning only worked well if they used <strong>learning rate warm-up</strong>.</p><p>Without warm-up, pruning failed to find winning tickets. With warm-up (i.e., slowly increasing the learning rate at the beginning of training), they were back in business.</p><p>So yes—winning tickets exist even in deep networks, but only if you treat them with care.</p><h2 id="Key-Takeaways"><a href="#Key-Takeaways" class="headerlink" title="Key Takeaways"></a>Key Takeaways</h2><ul><li>Big neural networks contain hidden winning tickets—smaller subnetworks that can be trained to match or exceed full network performance.</li><li>You find them by <strong>pruning</strong> and <strong>resetting</strong> repeatedly.</li><li>These subnetworks not only match accuracy, but often learn <strong>faster</strong> and generalize <strong>better</strong>.</li><li>The <strong>initialization</strong> is crucial—you can’t just randomly reinitialize and expect the same results.</li><li>Even deeper networks like VGG and ResNet have winning tickets, but they may require careful tuning (e.g., learning rate warm-up).</li><li>Pruning isn’t just for compression—it might teach us something deep about how neural networks work.</li></ul><p>Now, what I am trying to do is replecating the results found by the authors myself. So stick around and keep a look over my <a href="https://github.com/DA1729">GitHub</a>.</p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lottery Ticket Hypothesis for Beginners Part-1</title>
      <link href="/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/"/>
      <url>/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/</url>
      
        <content type="html"><![CDATA[<p>So, I am about to start on a new project realted to implementation of Neural Networks on FPGAs, you for acceleration purposes. Also, I have a new website design which I am absolutely loving (credits to Freemind.386 and others mentioned at the bottom of the page). Yeah so I was reading about the implementation process of AI Inferences on FPGAs and the first step happened to be opimizing the model itself for hardware implementation and the Vitis AI page mentioned “Pruning” as one of the techniques, and obviously I went to the rabbit-hole (side track) and came across this very interesting hypothesis (one metioned in the title) and found the original paper referenced below. Since, I am not “the AI expert” as we have around ourselves, so understanding this paper is not the easiest work for me. I am reading it as a person who only knows the basics of AI and Neural Networks, hence the following blog is written in a very intuitive manner (at least to satisfy my intuition) and obviously if I am serious enough to write a blog and leave all the other somewhat more important stuff aside for now, I won’t half assedly read the paper and the just yap over the blog. Also if the further sentences don’t sound like me, that’s cuz I asked ChatGPT to turn my notes into a blog, so there is everything I have understood from the paper but in different wordings, so that should be fine ig…. enjoy! Also a side note, this new style just capitalize everything written in Markdown bold, so I am not screaming at you just highlighting that point.</p><span id="more"></span><h2 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h2><p>Deep neural networks, especially fully connected ones, tend to be overparameterized. While this over-parameterization can be useful for training, it also makes models heavy and resource-intensive. To tackle this, we use pruning—a process where we remove unnecessary weights from the network.</p><p>Surprisingly, pruning can reduce parameter counts by over 90% without harming the model’s accuracy. But this raises a natural question:</p><p><strong>If we can prune a trained model down to a smaller size without losing accuracy, why not just train a smaller model from the start?</strong></p><p>Turns out, that doesn’t quite work.</p><h2 id="Why-Not-Train-Small-from-the-Beginning"><a href="#Why-Not-Train-Small-from-the-Beginning" class="headerlink" title="Why Not Train Small from the Beginning"></a>Why Not Train Small from the Beginning</h2><p>Experiments show that architectures uncovered by pruning—despite being smaller and sufficient are harder to train from scratch. When randomly initialized and trained in isolation, these pruned networks often fail to reach the same level of accuracy as the full-sized original network.</p><p>However, there’s a clever workaround.</p><p>After pruning a model, we can retain the original weights (instead of random reinitialization) and then retrain the original model. This makes the learning process much faster than randomly initializing the values over some distribution. </p><h2 id="An-Experiment-in-Sparsity"><a href="#An-Experiment-in-Sparsity" class="headerlink" title="An Experiment in Sparsity"></a>An Experiment in Sparsity</h2><p>To understand this better, let’s consider a basic experiment:</p><ul><li>Start with a fully connected convolutional neural network (CNN).</li><li>Perform unstructured pruning by randomly removing connections.</li><li>Train the pruned network while tracking the iteration with the minimum validation loss.</li><li>Evaluate the final test accuracy at this “best” iteration.</li></ul><p>The observations were telling:</p><p><strong>Sparser networks learn slower and tend to be less accurate.</strong></p><p>Below are plots illustrating this trend. In the first two graphs, as the percentage of remaining weights decreases, the number of iterations to reach peak performance increases. In the last two, we see test accuracy steadily drop with increased pruning.</p><p><img src="/images/8.png" alt="Image Credits: [1]"></p><h2 id="The-Lottery-Ticket-Hypothesis"><a href="#The-Lottery-Ticket-Hypothesis" class="headerlink" title="The Lottery Ticket Hypothesis"></a>The Lottery Ticket Hypothesis</h2><p>From this behavior emerges the Lottery Ticket Hypothesis, first introduced by Jonathan Frankle and Michael Carbin in 2018. The hypothesis makes a bold but fascinating claim:</p><p><strong>A randomly-initialized, dense neural network contains a subnetwork (a “winning ticket”) that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.</strong></p><p>Let’s formalize this: </p><ul><li><p>Start with a dense feedforward neural network <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x;\theta) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>, where the initial parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\theta_0 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> are sampled randomly from a distribution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><msub><mi>θ</mi><mn>0</mn></msub></msub></mrow><annotation encoding="application/x-tex">D_{\theta_0} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9334em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>.</p></li><li><p>Train this network using <strong>Stochastic Gradient Descent (SGD)</strong>, a technique that updates weights using small, random batches of data rather than the full dataset at each step.</p></li><li><p>Let the training reach its minimum validation loss <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span> at iteration <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> with test accuracy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span>.</p></li></ul><p>Now introduce a binary mask <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>∈</mo><msup><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow><mrow><mi mathvariant="normal">∣</mi><mi>θ</mi><mi mathvariant="normal">∣</mi></mrow></msup></mrow><annotation encoding="application/x-tex">m \in {0, 1}^{|\theta|} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1168em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9223em;"><span style="top:-3.0973em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mord mtight">∣</span></span></span></span></span></span></span></span></span></span></span></span>, which indicates which parameters are kept (1) and which are pruned (0). We now train a new network, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>m</mi><mo>⋅</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x; m \cdot \theta) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>, using the same initialization for the remaining parameters.</p><p>According to the Lottery Ticket Hypothesis, there exists such a mask <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> for which:</p><ul><li>The pruned model reaches minimum validation loss <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>l</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">l&#x27; </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> at iteration <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">j&#x27; </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>≤</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">j&#x27; \leq j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>,</li><li>It achieves a test accuracy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>≥</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">a&#x27; \geq a </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879em;vertical-align:-0.136em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span>,</li><li>And it uses fewer parameters than the original model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>m</mi><msub><mi mathvariant="normal">∣</mi><mn>0</mn></msub><mo>&lt;</mo><mi mathvariant="normal">∣</mi><mi>θ</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|m|_0 &lt; |\theta| </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">m</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mord">∣</span></span></span></span>.</li></ul><p>In simpler terms: hidden within every big neural network is a smaller, efficient subnetwork—a winning ticket—that can be trained just as well if it’s initialized correctly.</p><h2 id="What-Counts-as-a-Winning-Ticket"><a href="#What-Counts-as-a-Winning-Ticket" class="headerlink" title="What Counts as a Winning Ticket?"></a>What Counts as a Winning Ticket?</h2><p>These “winning tickets” are usually uncovered through standard pruning techniques applied to fully-connected or convolutional feedforward networks.</p><p>It’s crucial to note that simply reinitializing these sub-networks randomly strips away their special status. When reinitialized, these subnetworks lose their performance edge, emphasizing the importance of the initial weight configuration.</p><p>Now, we get a feel for why we are calling them lottery tickets, because out of random initial parameters, for only a specific initialization of parameters result in the sub-network matches the performance of the original network</p><h2 id="How-to-identify-the-Winning-Tickets"><a href="#How-to-identify-the-Winning-Tickets" class="headerlink" title="How to identify the Winning Tickets"></a>How to identify the Winning Tickets</h2><ul><li><p>Randomly initialize a neural network <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x;\theta_0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> where (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>∼</mo><msub><mi>D</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\theta_0 \sim D_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>).</p></li><li><p>Train the network for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> iterations, arriving at parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\theta_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>.</p></li><li><p>Prune <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">p\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9444em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord">%</span></span></span></span> of the parameters in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\theta_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, creating a mask <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>. </p></li><li><p>Reset the remaining parameters to their values in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\theta_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, creating the winning ticket <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>m</mi><mo>⋅</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x; m\cdot \theta_0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.</p></li></ul><p>This approach of identifying the winning tickets is a one-shot approach. But, the original paper focuses more upon the iterative pruning, i.e., repeatedly train, prune and reset the network for over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> rounds; each round we prune <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mfrac><mn>1</mn><mi>n</mi></mfrac></msup><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">p^\frac{1}{n}\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1485em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954em;"><span style="top:-3.363em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span><span class="mord">%</span></span></span></span> of the weights that survived the previous rounds.</p><p>Results suggest that iterative pruning finds winning tickets that match the accuracy of the original network at smaller sizes than does one-shot pruning.</p><p>Let’s end this part right here, next part, we dive into more experimental results and of-course, iterative pruning. </p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI - Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vOld32!</title>
      <link href="/2025/04/11/vOld32/"/>
      <url>/2025/04/11/vOld32/</url>
      
        <content type="html"><![CDATA[<p>Alongside the 8-bit processor, I have started working on vOl32, the 32 bit version of vOld series.</p><span id="more"></span><p>Here is the github repo link: <a href="https://github.com/DA1729/vOld32">vOld32-repository</a>.</p><p>I have started this so that I can look into incorporating RISC-V in it. </p><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> VLSI </tag>
            
            <tag> digital design </tag>
            
            <tag> microprocessors </tag>
            
            <tag> verilog </tag>
            
            <tag> FPGA </tag>
            
            <tag> vOld </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vOld8 ALU</title>
      <link href="/2025/04/09/vOld32-ALU/"/>
      <url>/2025/04/09/vOld32-ALU/</url>
      
        <content type="html"><![CDATA[<p>You can find the relevant ALU files on this repo: <a href="https://github.com/DA1729/ALU_8BIT">vOld8 ALU</a>. </p><p>Working of an ALU is very easy to understand and easy for our intuitions. A basic ALU, which is used extensively even today, works on the following statement: “All the operations are evaluated for all the inputs, but are driven using the input Operation Code (OpCode) to get only one desired output.”</p><span id="more"></span><p>Reading the words driven using OpCode must strike the idea of using a Multiplexer (Mux). And indeed, a Mux is used to drive the desired output. </p><p>ALU by its name, Arithemetic and Logic Unit contains both Arithmetic Operations (Add, Subtract) and Logic Operations (AND, XOR, Comparators, etc.). For each Operation, we have a unique Operation Code (OpCode from now on). This OpCode is used as a select input for the Mux to get the desired Operator’s result. </p><p>Apart from the Arithmetic and Logical results, we also have four flag outputs: </p><ul><li><p>Overflow: Set when the result of a signed arithmetic operation is too large to fit in the destination register. This is a very very very critical flag. See the story of Ariane-5 Rocket Blast. </p></li><li><p>Negative: Set when the result of an operation is negative (most significant bit is 1).</p></li><li><p>Zero: Set when the result of an operation is exactly 0. </p></li><li><p>Carry: Set when an addition produces a carry out or a subtraction requires a borrow into the highest bit.</p></li></ul><h2 id="Arithmetic-Circuits"><a href="#Arithmetic-Circuits" class="headerlink" title="Arithmetic Circuits"></a>Arithmetic Circuits</h2><p>For arithmetic circuits, I have implemented only two modules (circuits), namely, Carry Propagation Adder (CPA_adder) and a Subtractor (sub). </p><p>I am not implementing Multiplier and Divide circuits (although I have reserved OpCodes for them, see later) as they are not the most efficient at hardware level. I can always implement them at the software level, which happens to be more efficient. I have reserved OpCodes for them as I aim to do a lot of experimentations with this microprocessor, like implementing a super efficient Multiplier Circuit which outperforms multiplication at software level. </p><p>I have refered [1] for the construction of CPA and the subtractor. Also note that the flags I mentioned above relevant only the case of Addition and Subtraction Operations.</p><p>Below are the block diagrams of the CPA_adder and sub modules: </p><p><img src="/images/3.png" alt="Carry Propagation Adder"></p><p>Below is the subtraction module’s block diagram: </p><p><img src="/images/4.png" alt="Subtractor"></p><h2 id="Logic-Circuits"><a href="#Logic-Circuits" class="headerlink" title="Logic Circuits"></a>Logic Circuits</h2><p>There are total of 11 modules in the logic_cicuits directory (see the repo). All of them are very simple to make and simple to understand. But, we shall look into the comp.sv file. This module(comparator) was created for comparisons. This single module, consists of 6 different operations: </p><ul><li>Equal to </li><li>Not equal to </li><li>Lesser than</li><li>Lesser than or equal to </li><li>Greater than</li><li>Greater than or equal to</li></ul><p>We will use a 6-to-1 mux to get only the desired comparison operation out of this module. </p><h2 id="Operation-Codes-OpCodes"><a href="#Operation-Codes-OpCodes" class="headerlink" title="Operation Codes (OpCodes)"></a>Operation Codes (OpCodes)</h2><p>Below is the Operation Code (OpCode) table designating a unique OpCode to each Operation. </p><table><thead><tr><th>Operation</th><th>OpCode</th></tr></thead><tbody><tr><td>ADD</td><td>00000</td></tr><tr><td>SUB</td><td>00001</td></tr><tr><td>MUL</td><td>00010</td></tr><tr><td>DIV</td><td>00011</td></tr><tr><td>AND</td><td>00100</td></tr><tr><td>OR</td><td>00101</td></tr><tr><td>XOR</td><td>00110</td></tr><tr><td>NOR</td><td>00111</td></tr><tr><td>NAND</td><td>01000</td></tr><tr><td>XNOR</td><td>01001</td></tr><tr><td>EQ (Equal)</td><td>01010</td></tr><tr><td>NEQ (Not Eq.)</td><td>01011</td></tr><tr><td>LT (Less Than)</td><td>01100</td></tr><tr><td>LTE (≤)</td><td>01101</td></tr><tr><td>GT (Greater)</td><td>01110</td></tr><tr><td>GTE (≥)</td><td>01111</td></tr><tr><td>Shift Left</td><td>10000</td></tr><tr><td>Shift Right</td><td>10001</td></tr><tr><td>Rotate Left</td><td>10010</td></tr><tr><td>Rotate Right</td><td>10011</td></tr></tbody></table><p>See, that there are two extra operations for Multiplication and Division, for which I have not made the modules of. These OpCodes are reserved for future experimentation purposes, as I have mentioned that I am making this processor for performing various Optimization and Complex experiments. </p><p>Till now, if the OpCodes 00010 (MUL) and 00011 (DIV) are chosen, all the outputs are driven to 0. </p><h2 id="Block-Diagram-of-the-ALU"><a href="#Block-Diagram-of-the-ALU" class="headerlink" title="Block Diagram of the ALU"></a>Block Diagram of the ALU</h2><p>I am deploying three Muxes (2-to-1, 6-to-1 and 20-to-1). I am using the 2-to-1 Mux to get the values of the flags from the Adder and Subtractor Operations.</p><p>6-to-1 Mux is being used to get the desired Comparator output. If you notice carefully, I can tell which comparator operator is being called using the last three bits of the whole OpCode. But note that we will still get the mux output even if the OpCode doesn’t call a comparison. There is no way to tell the mux given that we are using only the last three bits that the comparison isn’t being called so give the default output. It doesn’t cost us much of our efficieny.  Below is the block diagram for this 6-to-1 mux. Bit combination over the data input wire tells us the combination of the select input for which the given data will be steered towards the output of the mux. </p><p><img src="/images/5.png" alt="6-to-1 MUX for Comparison Operation Selection"></p><p>Now coming over to flag selection, for each flag I have deployed a 2-to-1 Mux, which outputs the flag from the chosen operator (ADD or SUB). Consider any flag say X, then let the value of X from ADD Operation be X_add and from the SUB Operation be X_sub. We can get X from the logic in the diagram below: </p><p><img src="/images/6.png" alt="2-to-1 MUX for Flag Outputs"></p><p>Note that for the select input of the mux, I am using the whole OpCode as I only need the meaningful values of flags for the ADD and SUB operators.</p><p>Now that we are done selecting these side details, we now use a 20-to-1 Mux for bringing all the Operations, Modules and Muxes together to form the full ALU. Below is the block diagram for it.</p><p><img src="/images/7.png" alt="20-to-1 Mux for ALU (other than flags) Output"></p><p>One must observe, that the input to the data input d10 (i.e. Comparison Mux Output) can be driven to the output on 6 different selection inputs (OpCodes). </p><p>This completes our ALU, further I’ll write it’s testbenches to identify any further logical bugs, but one can get the full idea of basic ALU construction from this. Refer to the repository along with the blog to get the full idea. </p><p>I have started working on the Memory of the processor now…</p><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> VLSI </tag>
            
            <tag> digital design </tag>
            
            <tag> microprocessors </tag>
            
            <tag> verilog </tag>
            
            <tag> FPGA </tag>
            
            <tag> vOld </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vOld8 microprocessor</title>
      <link href="/2025/04/09/vOld32-microprocessor/"/>
      <url>/2025/04/09/vOld32-microprocessor/</url>
      
        <content type="html"><![CDATA[<p>I have started working on making my own 8-bit microprocessor I am calling vOld32 (I’ll tell the history behind the name in the future). </p><p>I am doing 8-bits for now to understand the workings of the computers and microprocessors, later I am pretty sure I’ll make a super complex and optimized 32 (vOld32) or 64 (vOld64)  bits microprocessor. 8-bits feels like a good starting point. </p><p>Below is the github repository on which I’ll be taking forward this project: </p><span id="more"></span><p><a href="https://github.com/DA1729/vOld32">vOld32 Repository</a></p><p>I’ll be posting my progress, procedure and working of the blocks which I’ll be making in order to complete this microprocessor. </p><p>Also, I am writing the files in SystemVerilog. I’ll also mention the sources I refer, so it will be useful for someone else to make their own processor. </p><p>Wish me luck ig. </p><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> VLSI </tag>
            
            <tag> digital design </tag>
            
            <tag> microprocessors </tag>
            
            <tag> verilog </tag>
            
            <tag> FPGA </tag>
            
            <tag> vOld </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In Memory Computation using Analog Part 2</title>
      <link href="/2025/03/18/In-Memory-Computation-using-Analog-Part-2/"/>
      <url>/2025/03/18/In-Memory-Computation-using-Analog-Part-2/</url>
      
        <content type="html"><![CDATA[<h2 id="Matrix-Multiplication-through-MAC-operations"><a href="#Matrix-Multiplication-through-MAC-operations" class="headerlink" title="Matrix Multiplication through MAC operations"></a>Matrix Multiplication through MAC operations</h2><p>Below, I have presented a python code, illustrating matrix multiplication using MAC operation. But, why matrix multiplication only? Because everything is a fking MATRIX!!! (that’s why the film is called Matrix). Physicists, electrical engineers, computer scientists&#x2F;engineers just love representing everything in matrix, and why not, they make everything more streamlined and easy to represent. Since, we are representing everything in matrices, especially in machine learning and AI, like we have the weights matrices, input vectors, output vectors, etc., we have to do a lot of matrix multiplication and in hardware, using MAC operators, we can easily perform it. Now, carefully look and understand the python code below:</p><span id="more"></span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiply_mac</span>(<span class="params">A, B</span>):</span><br><span class="line"></span><br><span class="line">    A = np.array(A)</span><br><span class="line">    B = np.array(B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> A.shape[<span class="number">1</span>] != B.shape[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Matrix dimensions do not match for multiplication.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    C = np.zeros((A.shape[<span class="number">0</span>], B.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Can you explicitly see me using the MAC operation here? what is the accumulator?</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(B.shape[<span class="number">1</span>]):</span><br><span class="line">            mac = <span class="number">0</span>  </span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">1</span>]):</span><br><span class="line">                mac += A[i][k] * B[k][j]  </span><br><span class="line">            C[i][j] = mac</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> C</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">B = [[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">result = matrix_multiply_mac(A, B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Resultant Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>Now, that (I hope) you have read and understood the code above, one can realize that we can use the circuit we designed in the previous part for the same operation. Hence, we can do matrix multiplication through analog computing now, how cool!</p><p>But why should we go for analog rather than digital? In digital, the energy complexity grows a lot faster as the number of bits are increased, speaking with numbers, an 8-bit MAC energy can be 100 times the energy for 1 bit. </p><p>Let’s end this part here for now, as I wrote this very impulsively out a sudden motivation (and too keep the momentum going) and did not plan it too much before writing LMAO.</p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] J. -s. Seo et al., “Digital Versus Analog Artificial Intelligence Accelerators: Advances, trends, and emerging designs,” in IEEE Solid-State Circuits Magazine, vol. 14, no. 3, pp. 65-79, Summer 2022, doi: 10.1109&#x2F;MSSC.2022.3182935.<br>keywords: {AI accelerators;Market research;In-memory computing;Hardware;System analysis and design;Switching circuits},</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In-Memory Computation using Analog Part-1</title>
      <link href="/2025/03/15/In-Memory-Computation-using-Analog-Part-1/"/>
      <url>/2025/03/15/In-Memory-Computation-using-Analog-Part-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Von-Neumann-Bottleneck"><a href="#Von-Neumann-Bottleneck" class="headerlink" title="Von Neumann Bottleneck"></a>Von Neumann Bottleneck</h2><p>There has been an improvement in the number of transistors on a chip. More transistors mean that we have increased our ability to store more memory in less physical space. Memory storage is more efficient than ever.</p><p>Today, AI and machine learning are being studied. This requires us to store and process a large density of data, which is possible given the environment: processors and storage solutions. Also, Von Neumann Architecture requires us to store data in a separate block, and the processor needs an individual block. These different blocks are connected by buses. Given this architecture, to process these large-density data, the transfer rates must also be at par with the processing speed, maybe even faster. However, over the years, the increase in transfer speedhas only made a few gains.</p><span id="more"></span><p>When the processor has to stay idle to fetch the data from the memory block, this condition is called the <strong>Von-Neumann Bottleneck</strong>.</p><p>Some attempts to surpass this limitation have been made like: </p><ul><li><p><strong>Caching</strong>: Chaches are temporary storage units between the main memory block and the processor. It can store a subset of data so that future requests for that data can be served faster. For example, they store results of earlier computations or a copy of data stored elsewhere.</p></li><li><p><strong>Hardware Acceleration</strong>: Hardware like GPUs, FPGAs, and ASICs are brought into the picture for faster response from the hardware side.</p></li></ul><p>But these come with some limitations: </p><ul><li><p><strong>Limitations of Caching</strong>:</p><ul><li><p><strong>Size</strong>: Larger caches increase hit rates but consume more silicon area and power. </p></li><li><p>In multicore systems, maintaining consistency across caches is difficult.</p></li><li><p><strong>Memory Latency and Bandwidth Issues</strong>: If the working set exceeds capacity, frequent primary memory access still causes stalls.</p></li></ul></li><li><p><strong>Hardware Accelerators’ Limitations</strong>:</p><ul><li><p><strong>Domain-Specificity</strong>: FPGAs, TPUs, and GPUs lack generality. They are often made for specific tasks, which, economically speaking, makes them challenging to produce. </p></li><li><p>At the end of the day, communications are still being made over buses, so the transfer limitation persists. </p></li><li><p><strong>Software and Compatibility Issues</strong>: These devices run on specific firmware and can cause compatibility issues. </p></li><li><p><strong>Power and Heat Management</strong>: These hardware accelerators generate much heat and consume much power, which obviously isn’t preferable.</p></li></ul></li></ul><p>Now, we dive into analog methods of overcoming this phenomenon. Of course, some digital methods have been proposed but let’s stick to the title of the blog for now and maybe (definitely) I’ll discuss digital methods in a future blog.</p><h2 id="Analog-Implementation-of-MACS"><a href="#Analog-Implementation-of-MACS" class="headerlink" title="Analog Implementation of MACS"></a>Analog Implementation of MACS</h2><p>MAC, or Multiply-Accumulate Operation, is a common step which computes the product of two numbers and adds that product to an accumulator. MAC operations account for over 90% of Neural Network and AI computations. Yeah, so they are “kind of” important.</p><p>In the following circuit, we have 10 MOSFETs in total (5 PMOS, 5 CMOS), let us label them: <strong>PM<sub>1</sub></strong>, <strong>PM<sub>2</sub></strong>, <strong>PM<sub>3</sub></strong>, <strong>PM<sub>4</sub></strong>, <strong>PM<sub>5</sub></strong>, <strong>NM<sub>1</sub></strong>, <strong>NM<sub>2</sub></strong>, <strong>NM<sub>3</sub></strong>, <strong>NM<sub>4</sub></strong>, <strong>NM<sub>5</sub></strong>.</p><p>These MOSFETs are linearly biased (if you somewhat unfamiliar with working of MOSFET, go watch Engineering Mindset’s video on MOSFET on YouTube, I found it very good for a quick get around). We are applying differential inputs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo separator="true">,</mo><mo>−</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo separator="true">,</mo><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>w</mi><mo separator="true">,</mo><mo>−</mo><mi mathvariant="normal">Δ</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">+\Delta x, -\Delta x, +\Delta w , -\Delta w </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">+</span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">+</span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>.</p><p>The given transistors are now arranged in the following circuit (Image Courtesy: Reference [1]):</p><p><img src="/images/1.png" alt="MAC Operator"></p><p>Now, let’s get into some transistor math. </p><p>Since, all the transistors are operating in linear region, drain current <strong>I<sub>d2</sub></strong> is given by: </p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>d</mi><mn>2</mn></mrow></msub><mo>=</mo><msub><mi>K</mi><mi>n</mi></msub><mo>∗</mo><mo stretchy="false">[</mo><msub><mi>V</mi><mi>b</mi></msub><mo>−</mo><mi mathvariant="normal">Δ</mi><mi>w</mi><mo>−</mo><msub><mi>V</mi><mrow><mi>t</mi><mi>h</mi><mi>n</mi></mrow></msub><mo>−</mo><mfrac><mrow><msub><mi>V</mi><mi>b</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi></mrow><mn>2</mn></mfrac><mo stretchy="false">]</mo><mo>∗</mo><mo stretchy="false">(</mo><msub><mi>V</mi><mi>b</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I_{d2} =K_{n}*[V_{b}-\Delta w - V_{thn} - \frac{V_{b} + \Delta x}{2}]*(V_{b}+\Delta x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">hn</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2392em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8942em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4159em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.2222em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">Δ</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> </p><p>For knowing what each term means, refer to [1]. </p><p>Now, we are taking the transconductance factors and threshold voltages of the N and P MOSFETS to be equal, we get the following expression for the output current: </p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>4</mn><mo>∗</mo><mi>K</mi><mo>∗</mo><mi mathvariant="normal">Δ</mi><mi>w</mi><mo>∗</mo><mi mathvariant="normal">Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">I_{out} = 4*K*\Delta w * \Delta x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span></span></span></span></p><p>If you observer the above expression, we have multiplied two numbers! Now, all we have left to do is accumulate.</p><p>The load MOSFETS: <strong>PM<sub>5</sub></strong> and <strong>NM<sub>5</sub></strong> can seen as an equivalent load resistor, which will convert the output current to an output voltage:</p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>y</mi><mo>=</mo><msub><mi>V</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>−</mo><msub><mi>V</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Delta y = V_{out}-V_{outbias} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">bia</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><p>This can be easily visualized in the figure below: </p><p><img src="/images/2.png" alt="Image Courtesy : [1]"></p><p>Now, closely look at the (b) part of the above image, what we are doing is we are adding output currents of multiple sources (or I should say <strong>multipliers</strong>), such that the output voltage can be given by: </p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>∗</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>I</mi><mi>i</mi></msub><mo>∗</mo><msub><mi>R</mi><mrow><mi>l</mi><mi>o</mi><mi>a</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">V = \frac{1}{N}*\sum_{i=1}^{N} I_{i}*R_{load}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2809em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><p>With this, we have successfully created our analog MAC unit. Let us end this part-1 here. Next part, we will delve into experimental results, architecture, and maybe hybrid models proposed. </p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] J. Zhu, B. Chen, Z. Yang, L. Meng and T. T. Ye, “Analog Circuit Implementation of Neural Networks for In-Sensor Computing,” 2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), Tampa, FL, USA, 2021, pp. 150-156, doi: 10.1109&#x2F;ISVLSI51109.2021.00037. keywords: {Convolution;Neural networks;Linearity;Analog circuits;Very large scale integration;CMOS process;Silicon;Analog Computing;In-Sensor Computing;Edge Computing},</p><p>[2] Robert Sheldon, “von Neumann bottleneck”, TechTarget, <a href="https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running">https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
