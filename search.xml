<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Breaking LWE Encryption Part 1</title>
      <link href="/2025/07/03/Breaking-LWE-Encryption/"/>
      <url>/2025/07/03/Breaking-LWE-Encryption/</url>
      
        <content type="html"><![CDATA[<p>Ok, the title is an absolute clickbait. We are not “breaking” breaking the LWE Encryption, cuz it’s used a lot in <strong>Post-Quantum Cryptography</strong>, so it is supposed to remain intact even against a fully functioning Quantum Computer. In this blog, I am emphasizing the fact that whenever one has to encrypt something really important, or let’s say they own a startup and want to encrypt their company’s data, one must always use the standardized crypto libraries and tools like <strong>OpenSSL</strong>, <strong>RustCrypto</strong>, <strong>OQS</strong> (for post-quantum), etc. Cuz, by doing some clever thinking, your own code encoding an “Attack Proof” crypto scheme, can be broken. And, I’ll show that by breaking an LWE encryption which I coded in C++. Infact, I’ll break it in two different ways.</p><p>But before that, what is LWE?</p><span id="more"></span><h2 id="LWE-Learning-With-Errors"><a href="#LWE-Learning-With-Errors" class="headerlink" title="LWE (Learning With Errors)"></a>LWE (Learning With Errors)</h2><p>LWE or Learngin With Errors, is a <strong>hard</strong> mathematical problem. The mathematical definition from Wikipedia is as follows:</p><div style="border-left: 4px solid #007acc; padding: 10px; background-color: #f9f9f9; text-align: justify;"><p>Let <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="double-struck">Z</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\mathbb{Z}_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.975em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> denote the ring of integers modulo <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span> and let <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="double-struck">Z</mi><mi>q</mi><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbb{Z}_q^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.072em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span> denote the set of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>-vectors over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="double-struck">Z</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\mathbb{Z}_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.975em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>. There exists a certain unknown linear function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msubsup><mi mathvariant="double-struck">Z</mi><mi>q</mi><mi>n</mi></msubsup><mo>→</mo><msub><mi mathvariant="double-struck">Z</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">f : \mathbb{Z}_q^n \to \mathbb{Z}_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.072em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.975em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, and the input to the LWE problem is a sample of pairs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{x}, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>∈</mo><msubsup><mi mathvariant="double-struck">Z</mi><mi>q</mi><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{x} \in \mathbb{Z}_q^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathbf">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.072em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>∈</mo><msub><mi mathvariant="double-struck">Z</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">y \in \mathbb{Z}_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.975em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, so that with high probability <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = f(\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathbf">x</span><span class="mclose">)</span></span></span></span>. Furthermore, the deviation from the equality is according to some known noise model. The problem calls for finding the function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>, or some close approximation thereof, with high probability.</p></div><p>This seems complex, but tbh, still better than other mathematical definitions for other stuff. One can get the idea from the above definition, that LWE problem is to find a solution to a set of linear equations with noise added to the system. Now, one might ask why this is hard to solve, proving that is out of scope for this blog, but for the curious who don’t fear a long mathematical detour, here you go: <a href="https://arxiv.org/abs/1306.0281">Classical Hardness of LWE</a>.</p><h2 id="LWE-Encryption"><a href="#LWE-Encryption" class="headerlink" title="LWE Encryption"></a>LWE Encryption</h2><p>Now, that we know the crux of the problem, let’s jump straight into encryption using this problem.</p><p>So, an LWE ciphertext consists of a random public vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∈</mo><msubsup><mi mathvariant="double-struck">Z</mi><mi>q</mi><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">a \in \mathbb{Z}_q^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.072em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span> and an additional element <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mi>a</mi><mo>⋅</mo><mi>s</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>⋅</mo><mi>m</mi><mo>+</mo><mi>e</mi></mrow><annotation encoding="application/x-tex">b = a \cdot s + \Delta \cdot m + e </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">e</span></span></span></span> mod <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span>. Let’s break this down further:</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span> is the **secret key**. Here, it is being randomly sampled from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">}</mo><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\{0, 1\}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span>.</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> is the message. It is important to note that the message space has to be considerably small as compared to the integer ring <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="double-struck">Z</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\mathbb{Z}_q </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.975em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> in order to stop the ciphertext becoming random noise instead of encrypted data. In fact, we define another number <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span>, which is in general a power of 2. The power represents the number of bits required to represent any element from the message space.</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span></span></span></span> is the scaling factor, which equals <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>q</mi><mi>p</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{q}{p} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2286em;vertical-align:-0.4811em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7475em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4811em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>. So, by multiplying this scaling factor to our message, what is happening is, we are shifting the message bits to the most significant portions of an n bit structure.</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">e</span></span></span></span> is the noise term and it is sampeled from a discrete Gaussian distribution with small standard distribution. Now, noise should not be too large that it affects the Most Significant Bits of the encoded message during the addition.</li></ul><h2 id="LWE-Decryption"><a href="#LWE-Decryption" class="headerlink" title="LWE Decryption"></a>LWE Decryption</h2><p>Decryption process is fairly simple. First, assuming that the receiver obviously has the secret key (NOTE: We have discussed the symmetric scheme here, it is also possible to devise an assymetric scheme usng LWE). The receiver will re-compute the dot product of the secret key and the public vector. Then, subtract the result from b. After that, perform an operation known as ROUND, which removes the error. Then we can just rescale the end result to get our decrypted plain-text.</p><p>Now that we are done with encryption and decryption using LWE, we can move forward having a look at the code and attacking strategies. I know that I have not explained the above concepts in a hurry cuz there are way better resources for studying about LWE, than here. So I would highly recommend checking out some blogs and papers online on LWE for a better understanding. I’ll also link some here:</p><ul><li><p><a href="https://www.jeremykun.com/2024/05/04/fhe-overview/#lwe-and-rlwe">Jeremy Kun’s Blog on FHE Overview</a>: He does a great job in explaining LWE and it’s sibling RLWE through words. Really good for an intuitive understanding. His other blogs are also very good and interesting. They cover crypto, signal processing. ML, Category Theory, and many more very interesting topics.</p></li><li><p><a href="https://www.youtube.com/watch?v=npoHSR6-oRw&t=5958s">TFHE Deep Dive talk by Ilaria chillotti</a>: It’s a really great talk for someone interested in getting into Homomorphic Encryption from mathematical perspective. Ilaria is one of the co-authors of the original TFHE (an FHE scheme) paper, and she does a great job explaining LWE, RLWE and RGSW.</p></li><li><p><a href="https://web.stanford.edu/class/cs354/scribe/lecture14.pdf">Stanford Lecture on LWE</a>: For those serious about crypto. More math intensive, but more serious you are about crypto more you have to start dealing with rigrous abstract math.</p></li><li><p>Google, ChatGPT, Claude, what else do you want.</p></li></ul><h2 id="Code-which-we-are-going-to-BREAK"><a href="#Code-which-we-are-going-to-BREAK" class="headerlink" title="Code which we are going to BREAK"></a>Code which we are going to BREAK</h2><p>Now, let’s see the code which I will attempt to break. It encodes LWE encryption and decryption in a very naive manner or how I would imagine a person without a serious security team to code an LWE scheme. We’ll go block by block and you can find all the codes related to the blog in this <a href="https://github.com/DA1729/breaking-lwe">repo</a>.</p><h3 id="Defining-the-parameters"><a href="#Defining-the-parameters" class="headerlink" title="Defining the parameters"></a>Defining the parameters</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> n = <span class="number">512</span>;           <span class="comment">// security parameter</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> q = <span class="number">12289</span>;         <span class="comment">// large prime modulus</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> p = <span class="number">4</span>;             <span class="comment">// message space &#123;0,1,2,3&#125;</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> delta = q / p;     <span class="comment">// scaling factor for encoding</span></span><br></pre></td></tr></table></figure><p>This is pretty self explanatory, just refer to the above definitions of the labeled terms, things will make sense. One thing to note, is that our message space is only <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0, 1, 2, 3\} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">3</span><span class="mclose">}</span></span></span></span>, which might seem very impractical and something worth not break into. But trust me, give me a larger message space, I can either just scale my attack for these parameters, or not care as large message spaces don’t interact well with the added Gaussian noise. In other words, encryption is practically pure noise.</p><h3 id="Noise-sampling"><a href="#Noise-sampling" class="headerlink" title="Noise sampling"></a>Noise sampling</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// noise sampling</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">sample_discrete_gaussian</span><span class="params">(std::mt19937&amp; gen, <span class="type">double</span> sigma = <span class="number">3.2</span>)</span> </span>&#123;</span><br><span class="line">    std::normal_distribution&lt;&gt; <span class="built_in">dist</span>(<span class="number">0.0</span>, sigma);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(std::<span class="built_in">round</span>(<span class="built_in">dist</span>(gen))) % q;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, a confusion might arise at the return statement. So the sample from a Gaussian distribution comes out as a double value in C++, but I want it to be an integer instead, NOO floating point headache. And, we really do want an “integer” sample from the distribution, and aditionally, it has to be in the ring <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="double-struck">Z</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">\mathbb{Z}_q </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.975em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, so I casted the double as an integer by first rounding it of course. Then I reduced it modulo <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span>.</p><h3 id="Secret-key-generation"><a href="#Secret-key-generation" class="headerlink" title="Secret key generation"></a>Secret key generation</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// secret key</span></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">key_gen</span><span class="params">(std::mt19937&amp; gen)</span> </span>&#123;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">s</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="function">std::bernoulli_distribution <span class="title">bern</span><span class="params">(<span class="number">0.5</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span>&amp; si : s) si = <span class="built_in">bern</span>(gen);</span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Secret key, here, is just a vector of binary digits of length <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>. So, I just sampeled from a Bernoulli Distribution (with success probability equal to half), <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> times.</p><h3 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// encryption</span></span><br><span class="line">std::pair&lt;std::vector&lt;<span class="type">int</span>&gt;, <span class="type">int</span>&gt; <span class="built_in">encrypt</span>(<span class="type">int</span> m, <span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; s, std::mt19937&amp; gen) &#123;</span><br><span class="line">    std::uniform_int_distribution&lt;&gt; <span class="built_in">uniform_q</span>(<span class="number">0</span>, q - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">a</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span>&amp; ai : a) ai = <span class="built_in">uniform_q</span>(gen);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> e = <span class="built_in">sample_discrete_gaussian</span>(gen);</span><br><span class="line">    <span class="type">int</span> b = (<span class="built_in">dot_mod_q</span>(a, s) + delta * m + e) % q;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;a, b&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In this function, we first sample the public vector of length <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>. Then things are pretty straight forward and matches one-on-one with the definitions above. Also, <code>dot_mod_q</code> ain’t a standard C++ function. It’s a function I wrote and did not explain in detail cuz it’s very simple to understand.</p><h3 id="Decryption"><a href="#Decryption" class="headerlink" title="Decryption"></a>Decryption</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// decryption</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">decrypt</span><span class="params">(<span class="type">const</span> std::pair&lt;std::vector&lt;<span class="type">int</span>&gt;, <span class="type">int</span>&gt;&amp; ct, <span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; s)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; a = ct.first;</span><br><span class="line">    <span class="type">int</span> b = ct.second;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> phase = (b - <span class="built_in">dot_mod_q</span>(a, s) + q) % q;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(std::<span class="built_in">round</span>((<span class="type">double</span>)phase / delta)) % p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In this function, one can see the rounding step in the return statement. Rest, everything here too matches the one-on-one with the definitions above.</p><p>Now putting it all together, we have the following:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;random&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cassert&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> n = <span class="number">512</span>;           <span class="comment">// security parameter</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> q = <span class="number">12289</span>;         <span class="comment">// large prime modulus</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> p = <span class="number">4</span>;             <span class="comment">// message space &#123;0,1,2,3&#125;</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> delta = q / p;     <span class="comment">// scaling factor for encoding</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// noise sampling</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">sample_discrete_gaussian</span><span class="params">(std::mt19937&amp; gen, <span class="type">double</span> sigma = <span class="number">3.2</span>)</span> </span>&#123;</span><br><span class="line">    std::normal_distribution&lt;&gt; <span class="built_in">dist</span>(<span class="number">0.0</span>, sigma);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(std::<span class="built_in">round</span>(<span class="built_in">dist</span>(gen))) % q;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// secret key</span></span><br><span class="line"><span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">key_gen</span><span class="params">(std::mt19937&amp; gen)</span> </span>&#123;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">s</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="function">std::bernoulli_distribution <span class="title">bern</span><span class="params">(<span class="number">0.5</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span>&amp; si : s) si = <span class="built_in">bern</span>(gen);</span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">dot_mod_q</span><span class="params">(<span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; a, <span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; b)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">assert</span>(a.<span class="built_in">size</span>() == b.<span class="built_in">size</span>());</span><br><span class="line">    <span class="type">int64_t</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>(); ++i)</span><br><span class="line">        sum += <span class="built_in">static_cast</span>&lt;<span class="type">int64_t</span>&gt;(a[i]) * b[i];</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(sum % q);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// encryption</span></span><br><span class="line">std::pair&lt;std::vector&lt;<span class="type">int</span>&gt;, <span class="type">int</span>&gt; <span class="built_in">encrypt</span>(<span class="type">int</span> m, <span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; s, std::mt19937&amp; gen) &#123;</span><br><span class="line">    std::uniform_int_distribution&lt;&gt; <span class="built_in">uniform_q</span>(<span class="number">0</span>, q - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">a</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span>&amp; ai : a) ai = <span class="built_in">uniform_q</span>(gen);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> e = <span class="built_in">sample_discrete_gaussian</span>(gen);</span><br><span class="line">    <span class="type">int</span> b = (<span class="built_in">dot_mod_q</span>(a, s) + delta * m + e) % q;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;a, b&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// decryption</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">decrypt</span><span class="params">(<span class="type">const</span> std::pair&lt;std::vector&lt;<span class="type">int</span>&gt;, <span class="type">int</span>&gt;&amp; ct, <span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; s)</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;<span class="type">int</span>&gt;&amp; a = ct.first;</span><br><span class="line">    <span class="type">int</span> b = ct.second;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> phase = (b - <span class="built_in">dot_mod_q</span>(a, s) + q) % q;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(std::<span class="built_in">round</span>((<span class="type">double</span>)phase / delta)) % p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::random_device rd;</span><br><span class="line">    <span class="function">std::mt19937 <span class="title">gen</span><span class="params">(rd())</span></span>;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; s = <span class="built_in">key_gen</span>(gen);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Testing encryption over message space &#123;0,1,2,3&#125;:\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; p; ++m) &#123;</span><br><span class="line">        <span class="keyword">auto</span> ct = <span class="built_in">encrypt</span>(m, s, gen);</span><br><span class="line">        <span class="type">int</span> recovered = <span class="built_in">decrypt</span>(ct, s);</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Original: &quot;</span> &lt;&lt; m</span><br><span class="line">                  &lt;&lt; <span class="string">&quot; | Decrypted: &quot;</span> &lt;&lt; recovered</span><br><span class="line">                  &lt;&lt; ((m != recovered) ? <span class="string">&quot; ❌&quot;</span> : <span class="string">&quot; ✅&quot;</span>) &lt;&lt; <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>And here is the output of the code:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Testing encryption over message space &#123;0,1,2,3&#125;:</span><br><span class="line">Original: 0 | Decrypted: 0 ✅</span><br><span class="line">Original: 1 | Decrypted: 1 ✅</span><br><span class="line">Original: 2 | Decrypted: 2 ✅</span><br><span class="line">Original: 3 | Decrypted: 3 ✅</span><br></pre></td></tr></table></figure><p>Ok, so now that we are done with the basics, now actually get into the main stuff. Breaking it.</p><h2 id="Cryptanalysis-of-Lazily-Coded-LWE"><a href="#Cryptanalysis-of-Lazily-Coded-LWE" class="headerlink" title="Cryptanalysis of Lazily Coded LWE"></a>Cryptanalysis of Lazily Coded LWE</h2><p>So before we dive into this interesting topic. Let’s make a few things clear.</p><ul><li><p>It is obvious that the code presented above is publicly available, owing to the <strong>Kerckhoff’s Principle</strong> which states that <strong>cryptosystem should be secure even if everything about the system, except the key, is public knowledge.</strong></p></li><li><p><strong>Access to ciphertexts</strong>: We assume a chosen-plaintext or known-plaintext scenario, meaning we either know the original message or can encrypt arbitrary messages and collect ciphertexts.</p></li><li><p><strong>Implementation-level behavior</strong>: We consider not just the mathematics, but also how the algorithm is implemented — which opens up potential side channels like timing variations or key reuse.</p></li><li><p><strong>Goal of cryptanalysis</strong>: We aim to recover the secret key <code>s</code>, distinguish ciphertexts from random noise, or infer plaintexts under various attack models, using practical and scalable methods — including statistical, lattice-based, and implementation-aware attacks.</p></li><li><p><strong>Math alert</strong>: I will go heavy on math at some places, cuz it would be necessary in the parts where we exploit the parameters selection for the system.</p></li></ul><p>Now let’s start with the method which includes less abstract maths.</p><h2 id="Machine-Learning-Attack"><a href="#Machine-Learning-Attack" class="headerlink" title="Machine Learning Attack"></a>Machine Learning Attack</h2><p>Now let’s start diving easy. I don’t wanna overwhelm anyone with loads of complex info. Let’s go easy and step by step. First, ML means training, which needs data. What will be the data?</p><h3 id="Data-generation"><a href="#Data-generation" class="headerlink" title="Data generation"></a>Data generation</h3><p>We, are collecting LWE samples which are tuples of the form: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>∈</mo><msubsup><mi mathvariant="double-struck">Z</mi><mi>q</mi><mi>n</mi></msubsup><mo>×</mo><msub><mi mathvariant="double-struck">Z</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">(\mathbf{a}, b) \in \mathbb{Z}_q^n \times \mathbb{Z}_q </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathbf">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.072em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.975em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbb">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, where each sample obviously follows:</p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mo stretchy="false">⟨</mo><mi mathvariant="bold">a</mi><mo separator="true">,</mo><mi mathvariant="bold">s</mi><mo stretchy="false">⟩</mo><mo>+</mo><mi>e</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>m</mi><mspace></mspace><mspace width="0.6667em"/><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>q</mi></mrow><annotation encoding="application/x-tex">b = \langle \mathbf{a}, \mathbf{s} \rangle + e + \Delta m \mod q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord mathbf">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">s</span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal">m</span><span class="mspace allowbreak"></span><span class="mspace" style="margin-right:0.6667em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">mod</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span>.</p><p>Along with these tuples, we are also collecting the corresponding secret keys.</p><p>Now, for training purposes, we will only use the public vectors <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">a</mi></mrow><annotation encoding="application/x-tex">\mathbf{a} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf">a</span></span></span></span>s as the <strong>input vectors</strong> and the corresponding secret key bits as <strong>target-labels</strong>.</p><p>Analyzing the data we collected and the data we’ll use for the training, we can infer that we are casting this <strong>LWE secret key</strong> problem as a <strong>supervised classification task</strong>. Where, each bit position of the secret key is treated as a separate <strong>binary classification problem</strong>.</p><p>For those not so familiar with ML concepts (like me lol), <strong>supervised classification task</strong> is a task, where we have a <strong>labeled dataset</strong>: each input (feature vector) is mapped with the correct output <strong>label</strong>. And the goal is to <strong>train</strong> the model to learn mapping from input to labels, so it can classify <strong>new, unseen inputs</strong>. And <strong>binary classification</strong> means that output (lables) are from a finite set of two elements, 0 and 1 in our case.</p><p>Here, is the python code for data generation and preparation of the training data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">n = <span class="number">512</span></span><br><span class="line">q = <span class="number">12289</span></span><br><span class="line">p = <span class="number">4</span></span><br><span class="line">delta = q // p</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">lwe_data_generator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cpp_executable_path = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.cpp_code = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#include &lt;iostream&gt;</span></span><br><span class="line"><span class="string">#include &lt;vector&gt;</span></span><br><span class="line"><span class="string">#include &lt;random&gt;</span></span><br><span class="line"><span class="string">#include &lt;cmath&gt;</span></span><br><span class="line"><span class="string">#include &lt;cassert&gt;</span></span><br><span class="line"><span class="string">constexpr int n = 512;</span></span><br><span class="line"><span class="string">constexpr int q = 12289;</span></span><br><span class="line"><span class="string">constexpr int p = 4;</span></span><br><span class="line"><span class="string">constexpr int delta = q / p;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">int sample_discrete_gaussian(std::mt19937&amp; gen, double sigma = 3.2) &#123;</span></span><br><span class="line"><span class="string">    std::normal_distribution&lt;&gt; dist(0.0, sigma);</span></span><br><span class="line"><span class="string">    return static_cast&lt;int&gt;(std::round(dist(gen))) % q;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">std::vector&lt;int&gt; key_gen(std::mt19937&amp; gen) &#123;</span></span><br><span class="line"><span class="string">    std::vector&lt;int&gt; s(n);</span></span><br><span class="line"><span class="string">    std::bernoulli_distribution bern(0.5);</span></span><br><span class="line"><span class="string">    for (int&amp; si : s) si = bern(gen);</span></span><br><span class="line"><span class="string">    return s;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">int dot_mod_q(const std::vector&lt;int&gt;&amp; a, const std::vector&lt;int&gt;&amp; b) &#123;</span></span><br><span class="line"><span class="string">    assert(a.size() == b.size());</span></span><br><span class="line"><span class="string">    int64_t sum = 0;</span></span><br><span class="line"><span class="string">    for (size_t i = 0; i &lt; a.size(); ++i)</span></span><br><span class="line"><span class="string">        sum += static_cast&lt;int64_t&gt;(a[i]) * b[i];</span></span><br><span class="line"><span class="string">    return static_cast&lt;int&gt;(sum % q);</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">std::pair&lt;std::vector&lt;int&gt;, int&gt; encrypt(int m, const std::vector&lt;int&gt;&amp; s, std::mt19937&amp; gen) &#123;</span></span><br><span class="line"><span class="string">    std::uniform_int_distribution&lt;&gt; uniform_q(0, q - 1);</span></span><br><span class="line"><span class="string">    std::vector&lt;int&gt; a(n);</span></span><br><span class="line"><span class="string">    for (int&amp; ai : a) ai = uniform_q(gen);</span></span><br><span class="line"><span class="string">    int e = sample_discrete_gaussian(gen);</span></span><br><span class="line"><span class="string">    int b = (dot_mod_q(a, s) + delta * m + e) % q;</span></span><br><span class="line"><span class="string">    return &#123;a, b&#125;;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">int main() &#123;</span></span><br><span class="line"><span class="string">    std::random_device rd;</span></span><br><span class="line"><span class="string">    std::mt19937 gen(rd());</span></span><br><span class="line"><span class="string">    std::vector&lt;int&gt; s = key_gen(gen);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    // Output secret key</span></span><br><span class="line"><span class="string">    std::cout &lt;&lt; &quot;SECRET:&quot;;</span></span><br><span class="line"><span class="string">    for (int si : s) std::cout &lt;&lt; &quot; &quot; &lt;&lt; si;</span></span><br><span class="line"><span class="string">    std::cout &lt;&lt; std::endl;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    // Generate training samples</span></span><br><span class="line"><span class="string">    for (int i = 0; i &lt; 10000; ++i) &#123;</span></span><br><span class="line"><span class="string">        auto ct = encrypt(0, s, gen);  // Always encrypt 0 for key recovery</span></span><br><span class="line"><span class="string">        std::cout &lt;&lt; &quot;SAMPLE:&quot;;</span></span><br><span class="line"><span class="string">        for (int ai : ct.first) std::cout &lt;&lt; &quot; &quot; &lt;&lt; ai;</span></span><br><span class="line"><span class="string">        std::cout &lt;&lt; &quot; &quot; &lt;&lt; ct.second &lt;&lt; std::endl;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    return 0;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_data</span>(<span class="params">self, num_samples = <span class="number">10000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;generate lwe samples and secret key&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tempfile.NamedTemporaryFile(mode=<span class="string">&#x27;w&#x27;</span>, suffix=<span class="string">&#x27;.cpp&#x27;</span>, delete=<span class="literal">False</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="variable language_">self</span>.cpp_code)</span><br><span class="line">            cpp_file = f.name</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># compile</span></span><br><span class="line">            exe_file = cpp_file.replace(<span class="string">&#x27;.cpp&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            subprocess.run([<span class="string">&#x27;g++&#x27;</span>, <span class="string">&#x27;-o&#x27;</span>, exe_file, cpp_file], check=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># run and capture output</span></span><br><span class="line">            result = subprocess.run([exe_file], capture_output=<span class="literal">True</span>, text=<span class="literal">True</span>, check=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># parse output</span></span><br><span class="line">            lines = result.stdout.strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># extract secret key</span></span><br><span class="line">            secret_line = [line <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">if</span> line.startswith(<span class="string">&#x27;SECRET:&#x27;</span>)][<span class="number">0</span>]</span><br><span class="line">            secret = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, secret_line.split()[<span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># extract samples</span></span><br><span class="line">            sample_lines = [line <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">if</span> line.startswith(<span class="string">&#x27;SAMPLE:&#x27;</span>)]</span><br><span class="line">            samples = []</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> sample_lines:</span><br><span class="line">                parts = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, line.split()[<span class="number">1</span>:]))</span><br><span class="line">                a = parts[:-<span class="number">1</span>]  <span class="comment"># first n elements</span></span><br><span class="line">                b = parts[-<span class="number">1</span>]   <span class="comment"># last element</span></span><br><span class="line">                samples.append((a, b))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> np.array(secret), samples</span><br><span class="line"></span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            <span class="comment"># cleanup</span></span><br><span class="line">            <span class="keyword">for</span> file <span class="keyword">in</span> [cpp_file, exe_file]:</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(file):</span><br><span class="line">                    os.unlink(file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_training_data</span>(<span class="params">secret, samples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;convert LWE samples to ML training data&quot;&quot;&quot;</span></span><br><span class="line">    X = []  <span class="comment"># input vectors (a values)</span></span><br><span class="line">    y = []  <span class="comment"># target labels (secret bits)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> a, b <span class="keyword">in</span> samples:</span><br><span class="line">        X.append(a)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(X), np.array(secret)</span><br></pre></td></tr></table></figure><h3 id="Model-for-Single-Bit-Prediction"><a href="#Model-for-Single-Bit-Prediction" class="headerlink" title="Model for Single Bit Prediction"></a>Model for Single Bit Prediction</h3><p>Now, we construct a neural network for predicting a single bit of the secret key. First, let’s recall the fact that LWE encryption sample looks something like:</p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mo stretchy="false">⟨</mo><mi mathvariant="bold">a</mi><mo separator="true">,</mo><mi mathvariant="bold">s</mi><mo stretchy="false">⟩</mo><mo>+</mo><mi>e</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>m</mi><mspace></mspace><mspace width="0.6667em"/><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">d</mi></mrow><mtext> </mtext><mtext> </mtext><mi>q</mi></mrow><annotation encoding="application/x-tex">b = \langle \mathbf{a}, \mathbf{s} \rangle + e + \Delta m \mod q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⟨</span><span class="mord mathbf">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathbf">s</span><span class="mclose">⟩</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal">m</span><span class="mspace allowbreak"></span><span class="mspace" style="margin-right:0.6667em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">mod</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span>.</p><p>In out attack, the secret <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">s</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>s</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>s</mi><mn>511</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{s} = (s_0, s_1, \cdots, s_{511}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">511</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> is a 512-bit vector. The key idea is: <strong>can we learn the value of one secret bit <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> just from the vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">a</mi></mrow><annotation encoding="application/x-tex">\mathbf{a} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf">a</span></span></span></span></strong>. Another perspective, treat it like a classification problem-binary decision: is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn><mtext> or </mtext><mn>1</mn></mrow><annotation encoding="application/x-tex">s_i = 0 \text{ or } 1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span><span class="mord text"><span class="mord"> or </span></span><span class="mord">1</span></span></span></span>.</p><p>Now neural network (a fancy function approximator) takes in input values, and it learns to give the correct output through repeated training on examples.</p><p>In our case:</p><ul><li><strong>Input</strong>: the 512-dimension public vector from the LWE ciphertext.</li><li><strong>Output</strong>: prediction whether the secret bit is 0 or 1.</li></ul><p>To predict a single bit, we use a small feed-forward neural network. In other words, each layer passes its output to the next, in a simple pipelined manner.</p><p>Here’s how the network is structured:</p><table><thead><tr><th>Layer</th><th>Description</th></tr></thead><tbody><tr><td><strong>Input Layer</strong></td><td>Takes in the 512-length LWE vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">a</mi></mrow><annotation encoding="application/x-tex">\mathbf{a}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf">a</span></span></span></span></td></tr><tr><td><strong>Dense Layer (256 units)</strong></td><td>Fully connected to all input values, learns patterns</td></tr><tr><td><strong>Dropout (30%)</strong></td><td>Randomly disables some neurons during training to prevent overfitting</td></tr><tr><td><strong>Dense Layer (128 units)</strong></td><td>Learns more abstract features</td></tr><tr><td><strong>Dropout (30%)</strong></td><td>Again helps generalize</td></tr><tr><td><strong>Dense Layer (64 units)</strong></td><td>Further processing</td></tr><tr><td><strong>Output Layer (1 unit, sigmoid)</strong></td><td>Outputs a number between 0 and 1 representing probability that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">s_i = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></td></tr></tbody></table><p>We train this model by feeding it:</p><ul><li>Thousands of LWE samples where the message is always 0.</li><li>The actual secret bit <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> corresponding to each sample.</li></ul><p>The model learns by adjusting internal weights to reduce the prediction errors. It utilizes:</p><ul><li><strong>Binary cross-entropy</strong>: a loss function for yes&#x2F;no predictions.</li><li><strong>Adam optimizer</strong>: a well-tuned optimizer that speeds up learning.</li><li><strong>Accuracy</strong>: to measure how often it correctly guesses the bit.</li></ul><p>Now don’t panic if you don’t understand much of these terms. I am also not an ML guy, and I had to do a lot of googling for getting these things down, I encourage you doing the same or just go with the flow with high level idea I presented for these things.</p><p>Anyways, the python code looks much shorter than the explanation :p</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_bit_model</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;neural network to predict one secret bit&quot;&quot;&quot;</span></span><br><span class="line">    model = keras.Sequential([</span><br><span class="line">        keras.layers.Dense(<span class="number">256</span>, activation = <span class="string">&#x27;relu&#x27;</span>, input_shape = (<span class="variable language_">self</span>.n)),</span><br><span class="line">        keras.layers.Dropout(<span class="number">0.3</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">128</span>, activation = <span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        keras.layers.Dropout(<span class="number">0.3</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">64</span>, activation = <span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">compile</span>(</span><br><span class="line">        optimizer = <span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">        loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">        metric = [<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>The blog seems to be getting very big now. And I still have to train the model, which god knows if I’ll be able to do on my laptop or not. So let’s end this part here. See you very soon with part 2.</p><p>peace. da1729</p>]]></content>
      
      
      
        <tags>
            
            <tag> Cryptography, Cryptanalysis, Post Quantum Cryptography, Fully Homomorphic Encryption </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lottery Ticket Hypothesis Part 2</title>
      <link href="/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/"/>
      <url>/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/</url>
      
        <content type="html"><![CDATA[<h2 id="Iterative-Pruning-and-Finding-the-Winning-Ticket"><a href="#Iterative-Pruning-and-Finding-the-Winning-Ticket" class="headerlink" title="Iterative Pruning and Finding the Winning Ticket"></a>Iterative Pruning and Finding the Winning Ticket</h2><p>So far, we’ve talked about the idea that there’s a smaller subnetwork—our so-called winning ticket—hidden within a big neural network. But how do we actually find this winning ticket? That’s where <strong>iterative pruning</strong> steps in.</p><h3 id="The-Iterative-Pruning-Process"><a href="#The-Iterative-Pruning-Process" class="headerlink" title="The Iterative Pruning Process"></a>The Iterative Pruning Process</h3><span id="more"></span><p>Instead of pruning once and hoping we get lucky, iterative pruning does the following:</p><ul><li><strong>Train the full network</strong> for a fixed number of iterations.</li><li><strong>Prune a small percentage</strong> (say, 10%-20%) of the lowest magnitude weights.</li><li><strong>Reset the remaining weights back</strong> to their original initialization.</li><li><strong>Repeat steps 1–3</strong> for several rounds.</li></ul><p>This slow and steady process lets us uncover subnetworks that are small but still highly capable—our winning tickets.</p><h3 id="Why-Iterative-Pruning-Works-Better"><a href="#Why-Iterative-Pruning-Works-Better" class="headerlink" title="Why Iterative Pruning Works Better"></a>Why Iterative Pruning Works Better</h3><p>Turns out, one-shot pruning (cutting lots of weights at once) often fails to find the best subnetworks, especially when we go too small. Iterative pruning, on the other hand, carefully preserves the parts of the network that matter, leading to <strong>better performance at smaller sizes</strong>.</p><hr><p>In the experiments, they could reduce the network size by up to 90%, and the resulting subnetworks still learned faster and better than the full network!</p><hr><h2 id="Do-Winning-Tickets-Generalize-Better"><a href="#Do-Winning-Tickets-Generalize-Better" class="headerlink" title="Do Winning Tickets Generalize Better?"></a>Do Winning Tickets Generalize Better?</h2><p>Now here’s where things get spicy. When comparing test accuracies, the researchers noticed something curious:</p><ul><li>The winning tickets not only learned faster,</li><li>They often had <strong>better generalization</strong> than the original model!</li></ul><p>This means that they didn’t just memorize training data—they actually learned to perform better on unseen test data.</p><p>This idea is related to something called <strong>Occam’s Hill</strong>—too big and you overfit, too small and you underfit. Winning tickets land at a sweet spot: small enough to avoid overfitting, but just right to still learn effectively.</p><h2 id="Initialization-Matters-A-Lot"><a href="#Initialization-Matters-A-Lot" class="headerlink" title="Initialization Matters (A Lot)"></a>Initialization Matters (A Lot)</h2><p>Another key takeaway: it’s not just the structure of the subnetwork that matters. It’s also the <strong>exact initial weights</strong>.</p><p>If you take a winning ticket’s structure and randomly reinitialize it, it <strong>loses its magic</strong>—learning slows down and performance drops.</p><h2 id="Expanding-to-Convolutional-Networks"><a href="#Expanding-to-Convolutional-Networks" class="headerlink" title="Expanding to Convolutional Networks"></a>Expanding to Convolutional Networks</h2><p>The authors didn’t just test on simple fully-connected networks like LeNet on MNIST. They also ran experiments on <strong>convolutional networks</strong> like Conv-2, Conv-4, and Conv-6 on CIFAR-10.</p><p>Surprise surprise: they found <strong>winning tickets</strong> there too. In fact, the same pattern repeated:</p><ul><li>Winning tickets learn faster</li><li>They reach higher accuracy</li><li>They generalize better</li><li>Initialization still matters</li></ul><p>The success wasn’t limited to toy datasets—this was happening on moderately complex image classification tasks too.</p><h2 id="Drop-Out-Pruning"><a href="#Drop-Out-Pruning" class="headerlink" title="Drop-Out + Pruning"></a>Drop-Out + Pruning</h2><p>What happens when you combine <strong>dropout</strong> with pruning?*</p><p>Turns out, dropout helps too! Dropout already encourages the network to be robust to missing connections. So when you prune, the network is more resilient.</p><p>When they trained networks <strong>with dropout</strong> and applied iterative pruning, the test accuracy <strong>improved even further</strong>. This hints that dropout may help in preparing the network for successful pruning.</p><h2 id="The-Big-Leagues-VGG-19-and-RESNET-18"><a href="#The-Big-Leagues-VGG-19-and-RESNET-18" class="headerlink" title="The Big Leagues: VGG-19 and RESNET-18"></a>The Big Leagues: VGG-19 and RESNET-18</h2><p>Taking it up a notch, the paper also tested on deeper, real-world architectures:</p><ul><li><strong>VGG-19</strong></li><li><strong>ResNet-18</strong></li></ul><p>The pattern mostly held up—but with a twist. For these deep networks, iterative pruning only worked well if they used <strong>learning rate warm-up</strong>.</p><p>Without warm-up, pruning failed to find winning tickets. With warm-up (i.e., slowly increasing the learning rate at the beginning of training), they were back in business.</p><p>So yes—winning tickets exist even in deep networks, but only if you treat them with care.</p><h2 id="Key-Takeaways"><a href="#Key-Takeaways" class="headerlink" title="Key Takeaways"></a>Key Takeaways</h2><ul><li>Big neural networks contain hidden winning tickets—smaller subnetworks that can be trained to match or exceed full network performance.</li><li>You find them by <strong>pruning</strong> and <strong>resetting</strong> repeatedly.</li><li>These subnetworks not only match accuracy, but often learn <strong>faster</strong> and generalize <strong>better</strong>.</li><li>The <strong>initialization</strong> is crucial—you can’t just randomly reinitialize and expect the same results.</li><li>Even deeper networks like VGG and ResNet have winning tickets, but they may require careful tuning (e.g., learning rate warm-up).</li><li>Pruning isn’t just for compression—it might teach us something deep about how neural networks work.</li></ul><p>Now, what I am trying to do is replecating the results found by the authors myself. So stick around and keep a look over my <a href="https://github.com/DA1729">GitHub</a>.</p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lottery Ticket Hypothesis Part-1</title>
      <link href="/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/"/>
      <url>/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/</url>
      
        <content type="html"><![CDATA[<p>So, I am about to start on a new project realted to implementation of Neural Networks on FPGAs, you for acceleration purposes.</p><span id="more"></span><p>Also, I have a new website design which I am absolutely loving (credits to Freemind.386 and others mentioned at the bottom of the page). Yeah so I was reading about the implementation process of AI Inferences on FPGAs and the first step happened to be opimizing the model itself for hardware implementation and the Vitis AI page mentioned “Pruning” as one of the techniques, and obviously I went to the rabbit-hole (side track) and came across this very interesting hypothesis (one metioned in the title) and found the original paper referenced below. Since, I am not “the AI expert” as we have around ourselves, so understanding this paper is not the easiest work for me. I am reading it as a person who only knows the basics of AI and Neural Networks, hence the following blog is written in a very intuitive manner (at least to satisfy my intuition) and obviously if I am serious enough to write a blog and leave all the other somewhat more important stuff aside for now, I won’t half assedly read the paper and the just yap over the blog. Also if the further sentences don’t sound like me, that’s cuz I asked ChatGPT to turn my notes into a blog, so there is everything I have understood from the paper but in different wordings, so that should be fine ig…. enjoy! Also a side note, this new style just capitalize everything written in Markdown bold, so I am not screaming at you just highlighting that point.</p><h2 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h2><p>Deep neural networks, especially fully connected ones, tend to be overparameterized. While this over-parameterization can be useful for training, it also makes models heavy and resource-intensive. To tackle this, we use pruning—a process where we remove unnecessary weights from the network.</p><p>Surprisingly, pruning can reduce parameter counts by over 90% without harming the model’s accuracy. But this raises a natural question:</p><p><strong>If we can prune a trained model down to a smaller size without losing accuracy, why not just train a smaller model from the start?</strong></p><p>Turns out, that doesn’t quite work.</p><h2 id="Why-Not-Train-Small-from-the-Beginning"><a href="#Why-Not-Train-Small-from-the-Beginning" class="headerlink" title="Why Not Train Small from the Beginning"></a>Why Not Train Small from the Beginning</h2><p>Experiments show that architectures uncovered by pruning—despite being smaller and sufficient are harder to train from scratch. When randomly initialized and trained in isolation, these pruned networks often fail to reach the same level of accuracy as the full-sized original network.</p><p>However, there’s a clever workaround.</p><p>After pruning a model, we can retain the original weights (instead of random reinitialization) and then retrain the original model. This makes the learning process much faster than randomly initializing the values over some distribution. </p><h2 id="An-Experiment-in-Sparsity"><a href="#An-Experiment-in-Sparsity" class="headerlink" title="An Experiment in Sparsity"></a>An Experiment in Sparsity</h2><p>To understand this better, let’s consider a basic experiment:</p><ul><li>Start with a fully connected convolutional neural network (CNN).</li><li>Perform unstructured pruning by randomly removing connections.</li><li>Train the pruned network while tracking the iteration with the minimum validation loss.</li><li>Evaluate the final test accuracy at this “best” iteration.</li></ul><p>The observations were telling:</p><p><strong>Sparser networks learn slower and tend to be less accurate.</strong></p><p>Below are plots illustrating this trend. In the first two graphs, as the percentage of remaining weights decreases, the number of iterations to reach peak performance increases. In the last two, we see test accuracy steadily drop with increased pruning.</p><p><img src="/images/8.png" alt="Image Credits: [1]"></p><h2 id="The-Lottery-Ticket-Hypothesis"><a href="#The-Lottery-Ticket-Hypothesis" class="headerlink" title="The Lottery Ticket Hypothesis"></a>The Lottery Ticket Hypothesis</h2><p>From this behavior emerges the Lottery Ticket Hypothesis, first introduced by Jonathan Frankle and Michael Carbin in 2018. The hypothesis makes a bold but fascinating claim:</p><p><strong>A randomly-initialized, dense neural network contains a subnetwork (a “winning ticket”) that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.</strong></p><p>Let’s formalize this: </p><ul><li><p>Start with a dense feedforward neural network <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x;\theta) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>, where the initial parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\theta_0 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> are sampled randomly from a distribution <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><msub><mi>θ</mi><mn>0</mn></msub></msub></mrow><annotation encoding="application/x-tex">D_{\theta_0} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9334em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>.</p></li><li><p>Train this network using <strong>Stochastic Gradient Descent (SGD)</strong>, a technique that updates weights using small, random batches of data rather than the full dataset at each step.</p></li><li><p>Let the training reach its minimum validation loss <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span> at iteration <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> with test accuracy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span>.</p></li></ul><p>Now introduce a binary mask <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>∈</mo><msup><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow><mrow><mi mathvariant="normal">∣</mi><mi>θ</mi><mi mathvariant="normal">∣</mi></mrow></msup></mrow><annotation encoding="application/x-tex">m \in {0, 1}^{|\theta|} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1168em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord"><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9223em;"><span style="top:-3.0973em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mord mtight">∣</span></span></span></span></span></span></span></span></span></span></span></span>, which indicates which parameters are kept (1) and which are pruned (0). We now train a new network, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>m</mi><mo>⋅</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x; m \cdot \theta) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>, using the same initialization for the remaining parameters.</p><p>According to the Lottery Ticket Hypothesis, there exists such a mask <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span> for which:</p><ul><li>The pruned model reaches minimum validation loss <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>l</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">l&#x27; </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> at iteration <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">j&#x27; </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>≤</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">j&#x27; \leq j </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>,</li><li>It achieves a test accuracy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>≥</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">a&#x27; \geq a </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8879em;vertical-align:-0.136em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span>,</li><li>And it uses fewer parameters than the original model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>m</mi><msub><mi mathvariant="normal">∣</mi><mn>0</mn></msub><mo>&lt;</mo><mi mathvariant="normal">∣</mi><mi>θ</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|m|_0 &lt; |\theta| </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">m</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mord">∣</span></span></span></span>.</li></ul><p>In simpler terms: hidden within every big neural network is a smaller, efficient subnetwork—a winning ticket—that can be trained just as well if it’s initialized correctly.</p><h2 id="What-Counts-as-a-Winning-Ticket"><a href="#What-Counts-as-a-Winning-Ticket" class="headerlink" title="What Counts as a Winning Ticket?"></a>What Counts as a Winning Ticket?</h2><p>These “winning tickets” are usually uncovered through standard pruning techniques applied to fully-connected or convolutional feedforward networks.</p><p>It’s crucial to note that simply reinitializing these sub-networks randomly strips away their special status. When reinitialized, these subnetworks lose their performance edge, emphasizing the importance of the initial weight configuration.</p><p>Now, we get a feel for why we are calling them lottery tickets, because out of random initial parameters, for only a specific initialization of parameters result in the sub-network matches the performance of the original network</p><h2 id="How-to-identify-the-Winning-Tickets"><a href="#How-to-identify-the-Winning-Tickets" class="headerlink" title="How to identify the Winning Tickets"></a>How to identify the Winning Tickets</h2><ul><li><p>Randomly initialize a neural network <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x;\theta_0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> where (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>∼</mo><msub><mi>D</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\theta_0 \sim D_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>).</p></li><li><p>Train the network for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> iterations, arriving at parameters <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\theta_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>.</p></li><li><p>Prune <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">p\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9444em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord">%</span></span></span></span> of the parameters in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\theta_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>, creating a mask <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>. </p></li><li><p>Reset the remaining parameters to their values in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\theta_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, creating the winning ticket <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>m</mi><mo>⋅</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x; m\cdot \theta_0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.</p></li></ul><p>This approach of identifying the winning tickets is a one-shot approach. But, the original paper focuses more upon the iterative pruning, i.e., repeatedly train, prune and reset the network for over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> rounds; each round we prune <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mfrac><mn>1</mn><mi>n</mi></mfrac></msup><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">p^\frac{1}{n}\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1485em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954em;"><span style="top:-3.363em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span><span class="mord">%</span></span></span></span> of the weights that survived the previous rounds.</p><p>Results suggest that iterative pruning finds winning tickets that match the accuracy of the original network at smaller sizes than does one-shot pruning.</p><p>Let’s end this part right here, next part, we dive into more experimental results and of-course, iterative pruning. </p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI - Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In Memory Computation using Analog Part 2</title>
      <link href="/2025/03/18/In-Memory-Computation-using-Analog-Part-2/"/>
      <url>/2025/03/18/In-Memory-Computation-using-Analog-Part-2/</url>
      
        <content type="html"><![CDATA[<h2 id="Matrix-Multiplication-through-MAC-operations"><a href="#Matrix-Multiplication-through-MAC-operations" class="headerlink" title="Matrix Multiplication through MAC operations"></a>Matrix Multiplication through MAC operations</h2><p>Below, I have presented a python code, illustrating matrix multiplication using MAC operation. But, why matrix multiplication only? Because everything is a fking MATRIX!!! (that’s why the film is called Matrix). Physicists, electrical engineers, computer scientists&#x2F;engineers just love representing everything in matrix, and why not, they make everything more streamlined and easy to represent. Since, we are representing everything in matrices, especially in machine learning and AI, like we have the weights matrices, input vectors, output vectors, etc., we have to do a lot of matrix multiplication and in hardware, using MAC operators, we can easily perform it. Now, carefully look and understand the python code below:</p><span id="more"></span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matrix_multiply_mac</span>(<span class="params">A, B</span>):</span><br><span class="line"></span><br><span class="line">    A = np.array(A)</span><br><span class="line">    B = np.array(B)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> A.shape[<span class="number">1</span>] != B.shape[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Matrix dimensions do not match for multiplication.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    C = np.zeros((A.shape[<span class="number">0</span>], B.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Can you explicitly see me using the MAC operation here? what is the accumulator?</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(B.shape[<span class="number">1</span>]):</span><br><span class="line">            mac = <span class="number">0</span>  </span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">1</span>]):</span><br><span class="line">                mac += A[i][k] * B[k][j]  </span><br><span class="line">            C[i][j] = mac</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> C</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">B = [[<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>]]</span><br><span class="line">result = matrix_multiply_mac(A, B)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Resultant Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>Now, that (I hope) you have read and understood the code above, one can realize that we can use the circuit we designed in the previous part for the same operation. Hence, we can do matrix multiplication through analog computing now, how cool!</p><p>But why should we go for analog rather than digital? In digital, the energy complexity grows a lot faster as the number of bits are increased, speaking with numbers, an 8-bit MAC energy can be 100 times the energy for 1 bit. </p><p>Let’s end this part here for now, as I wrote this very impulsively out a sudden motivation (and too keep the momentum going) and did not plan it too much before writing LMAO.</p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] J. -s. Seo et al., “Digital Versus Analog Artificial Intelligence Accelerators: Advances, trends, and emerging designs,” in IEEE Solid-State Circuits Magazine, vol. 14, no. 3, pp. 65-79, Summer 2022, doi: 10.1109&#x2F;MSSC.2022.3182935.<br>keywords: {AI accelerators;Market research;In-memory computing;Hardware;System analysis and design;Switching circuits},</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In-Memory Computation using Analog Part-1</title>
      <link href="/2025/03/15/In-Memory-Computation-using-Analog-Part-1/"/>
      <url>/2025/03/15/In-Memory-Computation-using-Analog-Part-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Von-Neumann-Bottleneck"><a href="#Von-Neumann-Bottleneck" class="headerlink" title="Von Neumann Bottleneck"></a>Von Neumann Bottleneck</h2><p>There has been an improvement in the number of transistors on a chip. More transistors mean that we have increased our ability to store more memory in less physical space. Memory storage is more efficient than ever.</p><p>Today, AI and machine learning are being studied. This requires us to store and process a large density of data, which is possible given the environment: processors and storage solutions. Also, Von Neumann Architecture requires us to store data in a separate block, and the processor needs an individual block. These different blocks are connected by buses. Given this architecture, to process these large-density data, the transfer rates must also be at par with the processing speed, maybe even faster. However, over the years, the increase in transfer speedhas only made a few gains.</p><span id="more"></span><p>When the processor has to stay idle to fetch the data from the memory block, this condition is called the <strong>Von-Neumann Bottleneck</strong>.</p><p>Some attempts to surpass this limitation have been made like: </p><ul><li><p><strong>Caching</strong>: Chaches are temporary storage units between the main memory block and the processor. It can store a subset of data so that future requests for that data can be served faster. For example, they store results of earlier computations or a copy of data stored elsewhere.</p></li><li><p><strong>Hardware Acceleration</strong>: Hardware like GPUs, FPGAs, and ASICs are brought into the picture for faster response from the hardware side.</p></li></ul><p>But these come with some limitations: </p><ul><li><p><strong>Limitations of Caching</strong>:</p><ul><li><p><strong>Size</strong>: Larger caches increase hit rates but consume more silicon area and power. </p></li><li><p>In multicore systems, maintaining consistency across caches is difficult.</p></li><li><p><strong>Memory Latency and Bandwidth Issues</strong>: If the working set exceeds capacity, frequent primary memory access still causes stalls.</p></li></ul></li><li><p><strong>Hardware Accelerators’ Limitations</strong>:</p><ul><li><p><strong>Domain-Specificity</strong>: FPGAs, TPUs, and GPUs lack generality. They are often made for specific tasks, which, economically speaking, makes them challenging to produce. </p></li><li><p>At the end of the day, communications are still being made over buses, so the transfer limitation persists. </p></li><li><p><strong>Software and Compatibility Issues</strong>: These devices run on specific firmware and can cause compatibility issues. </p></li><li><p><strong>Power and Heat Management</strong>: These hardware accelerators generate much heat and consume much power, which obviously isn’t preferable.</p></li></ul></li></ul><p>Now, we dive into analog methods of overcoming this phenomenon. Of course, some digital methods have been proposed but let’s stick to the title of the blog for now and maybe (definitely) I’ll discuss digital methods in a future blog.</p><h2 id="Analog-Implementation-of-MACS"><a href="#Analog-Implementation-of-MACS" class="headerlink" title="Analog Implementation of MACS"></a>Analog Implementation of MACS</h2><p>MAC, or Multiply-Accumulate Operation, is a common step which computes the product of two numbers and adds that product to an accumulator. MAC operations account for over 90% of Neural Network and AI computations. Yeah, so they are “kind of” important.</p><p>In the following circuit, we have 10 MOSFETs in total (5 PMOS, 5 CMOS), let us label them: <strong>PM<sub>1</sub></strong>, <strong>PM<sub>2</sub></strong>, <strong>PM<sub>3</sub></strong>, <strong>PM<sub>4</sub></strong>, <strong>PM<sub>5</sub></strong>, <strong>NM<sub>1</sub></strong>, <strong>NM<sub>2</sub></strong>, <strong>NM<sub>3</sub></strong>, <strong>NM<sub>4</sub></strong>, <strong>NM<sub>5</sub></strong>.</p><p>These MOSFETs are linearly biased (if you somewhat unfamiliar with working of MOSFET, go watch Engineering Mindset’s video on MOSFET on YouTube, I found it very good for a quick get around). We are applying differential inputs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo separator="true">,</mo><mo>−</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo separator="true">,</mo><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>w</mi><mo separator="true">,</mo><mo>−</mo><mi mathvariant="normal">Δ</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">+\Delta x, -\Delta x, +\Delta w , -\Delta w </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">+</span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">+</span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>.</p><p>The given transistors are now arranged in the following circuit (Image Courtesy: Reference [1]):</p><p><img src="/images/1.png" alt="MAC Operator"></p><p>Now, let’s get into some transistor math. </p><p>Since, all the transistors are operating in linear region, drain current <strong>I<sub>d2</sub></strong> is given by: </p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>d</mi><mn>2</mn></mrow></msub><mo>=</mo><msub><mi>K</mi><mi>n</mi></msub><mo>∗</mo><mo stretchy="false">[</mo><msub><mi>V</mi><mi>b</mi></msub><mo>−</mo><mi mathvariant="normal">Δ</mi><mi>w</mi><mo>−</mo><msub><mi>V</mi><mrow><mi>t</mi><mi>h</mi><mi>n</mi></mrow></msub><mo>−</mo><mfrac><mrow><msub><mi>V</mi><mi>b</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi></mrow><mn>2</mn></mfrac><mo stretchy="false">]</mo><mo>∗</mo><mo stretchy="false">(</mo><msub><mi>V</mi><mi>b</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I_{d2} =K_{n}*[V_{b}-\Delta w - V_{thn} - \frac{V_{b} + \Delta x}{2}]*(V_{b}+\Delta x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">hn</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2392em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8942em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4159em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.2222em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">Δ</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> </p><p>For knowing what each term means, refer to [1]. </p><p>Now, we are taking the transconductance factors and threshold voltages of the N and P MOSFETS to be equal, we get the following expression for the output current: </p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mn>4</mn><mo>∗</mo><mi>K</mi><mo>∗</mo><mi mathvariant="normal">Δ</mi><mi>w</mi><mo>∗</mo><mi mathvariant="normal">Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">I_{out} = 4*K*\Delta w * \Delta x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal">x</span></span></span></span></p><p>If you observer the above expression, we have multiplied two numbers! Now, all we have left to do is accumulate.</p><p>The load MOSFETS: <strong>PM<sub>5</sub></strong> and <strong>NM<sub>5</sub></strong> can seen as an equivalent load resistor, which will convert the output current to an output voltage:</p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>y</mi><mo>=</mo><msub><mi>V</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>−</mo><msub><mi>V</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Delta y = V_{out}-V_{outbias} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">bia</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><p>This can be easily visualized in the figure below: </p><p><img src="/images/2.png" alt="Image Courtesy : [1]"></p><p>Now, closely look at the (b) part of the above image, what we are doing is we are adding output currents of multiple sources (or I should say <strong>multipliers</strong>), such that the output voltage can be given by: </p><p style="text-align:center;"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mo>∗</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msub><mi>I</mi><mi>i</mi></msub><mo>∗</mo><msub><mi>R</mi><mrow><mi>l</mi><mi>o</mi><mi>a</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">V = \frac{1}{N}*\sum_{i=1}^{N} I_{i}*R_{load}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2809em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><p>With this, we have successfully created our analog MAC unit. Let us end this part-1 here. Next part, we will delve into experimental results, architecture, and maybe hybrid models proposed. </p><p>peace. da1729</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] J. Zhu, B. Chen, Z. Yang, L. Meng and T. T. Ye, “Analog Circuit Implementation of Neural Networks for In-Sensor Computing,” 2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), Tampa, FL, USA, 2021, pp. 150-156, doi: 10.1109&#x2F;ISVLSI51109.2021.00037. keywords: {Convolution;Neural networks;Linearity;Analog circuits;Very large scale integration;CMOS process;Silicon;Analog Computing;In-Sensor Computing;Edge Computing},</p><p>[2] Robert Sheldon, “von Neumann bottleneck”, TechTarget, <a href="https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running">https://www.techtarget.com/whatis/definition/von-Neumann-bottleneck#:~:text=The%20von%20Neumann%20bottleneck%20is,processing%20while%20they%20were%20running</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Analog </tag>
            
            <tag> VLSI </tag>
            
            <tag> Hardware Acceleration </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
