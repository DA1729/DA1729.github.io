<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lottery Ticket Hypothesis Part-1 | da1729&#39;s Blog</title>
  
  <!-- SEO Meta Tags -->
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="author" content="Daksh Pandey">
  
  <!-- Open Graph -->
  <meta property="og:title" content="Lottery Ticket Hypothesis Part-1">
  <meta property="og:description" content="">
  <meta property="og:type" content="post">
  <meta property="og:url" content="/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/">
  <meta property="og:site_name" content="da1729&#39;s Blog">
  
  <!-- CSS -->
  
<link rel="stylesheet" href="/css/style.css">

  
  <!-- RSS Feed -->
  
  
  <!-- Favicon -->
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div class="page-wrapper">
    <!-- Header -->
    <header class="header">
      <h1>
        <a href="/" class="site-title">da1729&#39;s Blog</a>
      </h1>
      
        <p class="site-subtitle">cryptography, digital design, embedded, rf, ...</p>
      
      
      <!-- Navigation -->
      <nav class="nav">
        
          <a href="/" class="nav-link">Home</a>
        
          <a href="/quick" class="nav-link">Quick Posts</a>
        
          <a href="/archives" class="nav-link">Archives</a>
        
          <a href="/about" class="nav-link">About</a>
        
        
        <!-- Social Links -->
        
          
            <a href="https://github.com/DA1729" class="nav-link" target="_blank" rel="noopener">github</a>
          
            <a href="https://x.com/sp0oky_daksh" class="nav-link" target="_blank" rel="noopener">twitter</a>
          
            <a href="mailto:dakshpandey177@gmail.com" class="nav-link" target="_blank" rel="noopener">email</a>
          
            <a href="https://sp0oky-portfolio.vercel.app/" class="nav-link" target="_blank" rel="noopener">portfolio</a>
          
        
      </nav>
    </header>

    <!-- Main Content -->
    <div class="container">
      <div class="content">
        <main class="main-content">
          <!-- Individual Post Page -->
<article class="article-card">
  <header class="article-header">
    <div class="article-meta">
      <time datetime="2025-04-13T14:36:02.000Z">
        April 13, 2025
      </time>
      
      
        <span> • Updated: September 27, 2025</span>
      
    </div>
    
    <h1 class="article-title">Lottery Ticket Hypothesis Part-1</h1>
    
    
      <div class="article-tags">
        
          <a href="/tags/AI-Acceleration/" class="tag">#AI - Acceleration</a>
        
      </div>
    
  </header>
  
  <div class="article-content">
    <div class="post-content">
      <p>So, I am about to start on a new project realted to implementation of Neural Networks on FPGAs, you for acceleration purposes.</p>
<span id="more"></span>

<p>Also, I have a new website design which I am absolutely loving (credits to Freemind.386 and others mentioned at the bottom of the page). Yeah so I was reading about the implementation process of AI Inferences on FPGAs and the first step happened to be opimizing the model itself for hardware implementation and the Vitis AI page mentioned “Pruning” as one of the techniques, and obviously I went to the rabbit-hole (side track) and came across this very interesting hypothesis (one metioned in the title) and found the original paper referenced below. Since, I am not “the AI expert” as we have around ourselves, so understanding this paper is not the easiest work for me. I am reading it as a person who only knows the basics of AI and Neural Networks, hence the following blog is written in a very intuitive manner (at least to satisfy my intuition) and obviously if I am serious enough to write a blog and leave all the other somewhat more important stuff aside for now, I won’t half assedly read the paper and the just yap over the blog. Also if the further sentences don’t sound like me, that’s cuz I asked ChatGPT to turn my notes into a blog, so there is everything I have understood from the paper but in different wordings, so that should be fine ig…. enjoy! Also a side note, this new style just capitalize everything written in Markdown bold, so I am not screaming at you just highlighting that point.</p>
<h2><span id="pruning">Pruning</span></h2><p>Deep neural networks, especially fully connected ones, tend to be overparameterized. While this over-parameterization can be useful for training, it also makes models heavy and resource-intensive. To tackle this, we use pruning—a process where we remove unnecessary weights from the network.</p>
<p>Surprisingly, pruning can reduce parameter counts by over 90% without harming the model’s accuracy. But this raises a natural question:</p>
<p><strong>If we can prune a trained model down to a smaller size without losing accuracy, why not just train a smaller model from the start?</strong></p>
<p>Turns out, that doesn’t quite work.</p>
<h2><span id="why-not-train-small-from-the-beginning">Why Not Train Small from the Beginning</span></h2><p>Experiments show that architectures uncovered by pruning—despite being smaller and sufficient are harder to train from scratch. When randomly initialized and trained in isolation, these pruned networks often fail to reach the same level of accuracy as the full-sized original network.</p>
<p>However, there’s a clever workaround.</p>
<p>After pruning a model, we can retain the original weights (instead of random reinitialization) and then retrain the original model. This makes the learning process much faster than randomly initializing the values over some distribution. </p>
<h2><span id="an-experiment-in-sparsity">An Experiment in Sparsity</span></h2><p>To understand this better, let’s consider a basic experiment:</p>
<ul>
<li>Start with a fully connected convolutional neural network (CNN).</li>
<li>Perform unstructured pruning by randomly removing connections.</li>
<li>Train the pruned network while tracking the iteration with the minimum validation loss.</li>
<li>Evaluate the final test accuracy at this “best” iteration.</li>
</ul>
<p>The observations were telling:</p>
<p><strong>Sparser networks learn slower and tend to be less accurate.</strong></p>
<p>Below are plots illustrating this trend. In the first two graphs, as the percentage of remaining weights decreases, the number of iterations to reach peak performance increases. In the last two, we see test accuracy steadily drop with increased pruning.</p>
<p><img src="/images/8.png" alt="Image Credits: [1]"></p>
<h2><span id="the-lottery-ticket-hypothesis">The Lottery Ticket Hypothesis</span></h2><p>From this behavior emerges the Lottery Ticket Hypothesis, first introduced by Jonathan Frankle and Michael Carbin in 2018. The hypothesis makes a bold but fascinating claim:</p>
<p><strong>A randomly-initialized, dense neural network contains a subnetwork (a “winning ticket”) that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.</strong></p>
<p>Let’s formalize this:</p>
<ul>
<li><p>Start with a dense feedforward neural network $f(x;\theta)$, where the initial parameters $\theta_0$ are sampled randomly from a distribution $D_{\theta_0}$.</p>
</li>
<li><p>Train this network using <strong>Stochastic Gradient Descent (SGD)</strong>, a technique that updates weights using small, random batches of data rather than the full dataset at each step.</p>
</li>
<li><p>Let the training reach its minimum validation loss $l$ at iteration $j$ with test accuracy $a$.</p>
</li>
</ul>
<p>Now introduce a binary mask $m \in {0, 1}^{|\theta|}$, which indicates which parameters are kept (1) and which are pruned (0). We now train a new network, $f(x; m \cdot \theta)$, using the same initialization for the remaining parameters.</p>
<p>According to the Lottery Ticket Hypothesis, there exists such a mask $m$ for which:</p>
<ul>
<li>The pruned model reaches minimum validation loss $l’$ at iteration $j’$, where $j’ \leq j$,</li>
<li>It achieves a test accuracy $a’ \geq a$,</li>
<li>And it uses fewer parameters than the original model $|m|_0 &lt; |\theta|$.</li>
</ul>
<p>In simpler terms: hidden within every big neural network is a smaller, efficient subnetwork—a winning ticket—that can be trained just as well if it’s initialized correctly.</p>
<h2><span id="what-counts-as-a-winning-ticket">What Counts as a Winning Ticket?</span></h2><p>These “winning tickets” are usually uncovered through standard pruning techniques applied to fully-connected or convolutional feedforward networks.</p>
<p>It’s crucial to note that simply reinitializing these sub-networks randomly strips away their special status. When reinitialized, these subnetworks lose their performance edge, emphasizing the importance of the initial weight configuration.</p>
<p>Now, we get a feel for why we are calling them lottery tickets, because out of random initial parameters, for only a specific initialization of parameters result in the sub-network matches the performance of the original network</p>
<h2><span id="how-to-identify-the-winning-tickets">How to identify the Winning Tickets</span></h2><ul>
<li><p>Randomly initialize a neural network $f(x;\theta_0)$ where ($\theta_0 \sim D_\theta$).</p>
</li>
<li><p>Train the network for $j$ iterations, arriving at parameters $\theta_j$.</p>
</li>
<li><p>Prune $p%$ of the parameters in $\theta_j$, creating a mask $m$.</p>
</li>
<li><p>Reset the remaining parameters to their values in $\theta_0$, creating the winning ticket $f(x; m\cdot \theta_0)$.</p>
</li>
</ul>
<p>This approach of identifying the winning tickets is a one-shot approach. But, the original paper focuses more upon the iterative pruning, i.e., repeatedly train, prune and reset the network for over $n$ rounds; each round we prune $p^\frac{1}{n}%$ of the weights that survived the previous rounds.</p>
<p>Results suggest that iterative pruning finds winning tickets that match the accuracy of the original network at smaller sizes than does one-shot pruning.</p>
<p>Let’s end this part right here, next part, we dive into more experimental results and of-course, iterative pruning. </p>
<p>peace. da1729</p>
<h2><span id="references">References</span></h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>

    </div>
  </div>
</article>

<!-- Post Navigation -->

  <nav class="post-nav mt-2">
    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
      
        <div class="widget">
          <div class="widget-title">Previous Post</div>
          <a href="/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/">Lottery Ticket Hypothesis Part 2</a>
        </div>
      
      
      
        <div class="widget">
          <div class="widget-title">Next Post</div>
          <a href="/2025/03/18/In-Memory-Computation-using-Analog-Part-2/">In Memory Computation using Analog Part 2</a>
        </div>
      
    </div>
  </nav>


<!-- Related Posts -->

        </main>
        
        <!-- Sidebar -->
        <aside class="sidebar">
          <!-- Sidebar Widgets -->

<!-- Recent Posts Widget -->

  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <ul>
      
        <li>
          <a href="/2025/12/06/MPC-in-the-Head-MPCitH/">MPC in the Head (MPCitH)</a>
          <div style="font-size: 0.8em; color: var(--text-secondary); margin-top: 4px;">
            Dec 6, 2025
          </div>
        </li>
      
        <li>
          <a href="/2025/12/04/Gentry-Lee-Encoding-for-Efficient-Matrix-FHE/">Gentry-Lee Encoding for Efficient Matrix FHE</a>
          <div style="font-size: 0.8em; color: var(--text-secondary); margin-top: 4px;">
            Dec 4, 2025
          </div>
        </li>
      
        <li>
          <a href="/2025/12/04/Multi-Party-Computation-part-2/">Yao&#39;s Garbled Circuits</a>
          <div style="font-size: 0.8em; color: var(--text-secondary); margin-top: 4px;">
            Dec 4, 2025
          </div>
        </li>
      
        <li>
          <a href="/2025/12/01/Multi-Party-Computation/">Multi-Party Computation part 1</a>
          <div style="font-size: 0.8em; color: var(--text-secondary); margin-top: 4px;">
            Dec 1, 2025
          </div>
        </li>
      
        <li>
          <a href="/2025/09/27/Need-for-Gadget-Decomposition-in-LWE-Based-Cryptosystems/">Need for Gadget Decomposition in LWE Based Cryptosystems</a>
          <div style="font-size: 0.8em; color: var(--text-secondary); margin-top: 4px;">
            Sep 27, 2025
          </div>
        </li>
      
    </ul>
  </div>


<!-- Categories Widget -->


<!-- Tags Widget -->

  <div class="widget">
    <h3 class="widget-title">Tags</h3>
    <div style="display: flex; flex-wrap: wrap; gap: 8px;">
      
        <a href="/tags/AI-Acceleration/" class="tag">
          #AI - Acceleration
        </a>
      
        <a href="/tags/AI-Acceleration/" class="tag">
          #AI Acceleration
        </a>
      
        <a href="/tags/Abstract-Algebra/" class="tag">
          #Abstract Algebra
        </a>
      
        <a href="/tags/Analog/" class="tag">
          #Analog
        </a>
      
        <a href="/tags/Cryptanalysis/" class="tag">
          #Cryptanalysis
        </a>
      
        <a href="/tags/Cryptography/" class="tag">
          #Cryptography
        </a>
      
        <a href="/tags/Fully-Homomorphic-Encryption/" class="tag">
          #Fully Homomorphic Encryption
        </a>
      
        <a href="/tags/Hardware-Acceleration/" class="tag">
          #Hardware Acceleration
        </a>
      
        <a href="/tags/Philosphy/" class="tag">
          #Philosphy
        </a>
      
        <a href="/tags/Post-Quantum-Cryptography/" class="tag">
          #Post Quantum Cryptography
        </a>
      
        <a href="/tags/Post-Quantum-Cryptography/" class="tag">
          #Post-Quantum Cryptography
        </a>
      
        <a href="/tags/Ring-Theory/" class="tag">
          #Ring Theory
        </a>
      
        <a href="/tags/Secure-Computing/" class="tag">
          #Secure Computing
        </a>
      
        <a href="/tags/VLSI/" class="tag">
          #VLSI
        </a>
      
        <a href="/tags/Zero-Knowledge/" class="tag">
          #Zero-Knowledge
        </a>
      
    </div>
  </div>


<!-- Archive Widget -->

  <div class="widget">
    <h3 class="widget-title">Archive</h3>
    <ul>
      
      
        
          <li>
            <a href="/archives/2025/">
              December 2025 (4)
            </a>
          </li>
        
          <li>
            <a href="/archives/2025/">
              September 2025 (2)
            </a>
          </li>
        
          <li>
            <a href="/archives/2025/">
              August 2025 (1)
            </a>
          </li>
        
          <li>
            <a href="/archives/2025/">
              July 2025 (1)
            </a>
          </li>
        
          <li>
            <a href="/archives/2025/">
              April 2025 (2)
            </a>
          </li>
        
          <li>
            <a href="/archives/2025/">
              March 2025 (2)
            </a>
          </li>
        
      
    </ul>
  </div>


<!-- About Widget -->
<div class="widget">
  <h3 class="widget-title">About</h3>
  <p style="font-size: 0.9em; line-height: 1.6;">
    Welcome to my blog! Here I write about various topics including technology, programming, and more.
  </p>
  
  
    <div style="margin-top: 15px;">
      <strong style="font-size: 0.9em;">Find me on:</strong>
      <div style="margin-top: 8px; display: flex; flex-wrap: wrap; gap: 8px;">
        
          <a href="https://github.com/DA1729" class="tag" target="_blank" rel="noopener">
            github
          </a>
        
          <a href="https://x.com/sp0oky_daksh" class="tag" target="_blank" rel="noopener">
            twitter
          </a>
        
          <a href="mailto:dakshpandey177@gmail.com" class="tag" target="_blank" rel="noopener">
            email
          </a>
        
          <a href="https://sp0oky-portfolio.vercel.app/" class="tag" target="_blank" rel="noopener">
            portfolio
          </a>
        
      </div>
    </div>
  
  
  <div style="margin-top: 15px; padding-top: 15px; border-top: 1px solid var(--border-color);">
    <p style="font-size: 0.8em; color: var(--text-secondary); opacity: 0.8;">
      Theme designed with 
      <a href="https://claude.ai/code" target="_blank" rel="noopener" style="color: var(--link-color); text-decoration: none;">Claude Code</a>
    </p>
  </div>
</div>
        </aside>
      </div>
    </div>

    <!-- Footer -->
    <footer class="footer">
      <p>© 2025 Daksh Pandey. Portfolio-inspired theme crafted with Claude Code.</p>
    </footer>
  </div>

  <!-- JavaScript -->
  
<script src="/js/main.js"></script>

  
  <!-- Math Support -->
  
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          ignoreHtmlClass: 'tex2jax_ignore',
          processHtmlClass: 'tex2jax_process'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
</body>
</html>