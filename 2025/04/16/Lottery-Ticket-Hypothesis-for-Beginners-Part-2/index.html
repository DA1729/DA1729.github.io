<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Lottery Ticket Hypothesis Part 2 | da1729&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Iterative Pruning and Finding the Winning TicketSo far, we’ve talked about the idea that there’s a smaller subnetwork—our so-called winning ticket—hidden within a big neural network. But how do we act">
<meta property="og:type" content="article">
<meta property="og:title" content="Lottery Ticket Hypothesis Part 2">
<meta property="og:url" content="https://da1729.github.io/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/index.html">
<meta property="og:site_name" content="da1729&#39;s Blog">
<meta property="og:description" content="Iterative Pruning and Finding the Winning TicketSo far, we’ve talked about the idea that there’s a smaller subnetwork—our so-called winning ticket—hidden within a big neural network. But how do we act">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-04-16T12:19:20.000Z">
<meta property="article:modified_time" content="2025-05-29T19:24:34.152Z">
<meta property="article:author" content="Daksh Pandey">
<meta property="article:tag" content="AI Acceleration">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="da1729's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">da1729&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">embedded, rf, vlsi, comp eng, ...</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://DA1729.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Lottery-Ticket-Hypothesis-for-Beginners-Part-2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/" class="article-date">
  <time class="dt-published" datetime="2025-04-16T12:19:20.000Z" itemprop="datePublished">2025-04-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Lottery Ticket Hypothesis Part 2
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Iterative-Pruning-and-Finding-the-Winning-Ticket"><a href="#Iterative-Pruning-and-Finding-the-Winning-Ticket" class="headerlink" title="Iterative Pruning and Finding the Winning Ticket"></a>Iterative Pruning and Finding the Winning Ticket</h2><p>So far, we’ve talked about the idea that there’s a smaller subnetwork—our so-called winning ticket—hidden within a big neural network. But how do we actually find this winning ticket? That’s where <strong>iterative pruning</strong> steps in.</p>
<h3 id="The-Iterative-Pruning-Process"><a href="#The-Iterative-Pruning-Process" class="headerlink" title="The Iterative Pruning Process"></a>The Iterative Pruning Process</h3><span id="more"></span>

<p>Instead of pruning once and hoping we get lucky, iterative pruning does the following:</p>
<ul>
<li><strong>Train the full network</strong> for a fixed number of iterations.</li>
<li><strong>Prune a small percentage</strong> (say, 10%-20%) of the lowest magnitude weights.</li>
<li><strong>Reset the remaining weights back</strong> to their original initialization.</li>
<li><strong>Repeat steps 1–3</strong> for several rounds.</li>
</ul>
<p>This slow and steady process lets us uncover subnetworks that are small but still highly capable—our winning tickets.</p>
<h3 id="Why-Iterative-Pruning-Works-Better"><a href="#Why-Iterative-Pruning-Works-Better" class="headerlink" title="Why Iterative Pruning Works Better"></a>Why Iterative Pruning Works Better</h3><p>Turns out, one-shot pruning (cutting lots of weights at once) often fails to find the best subnetworks, especially when we go too small. Iterative pruning, on the other hand, carefully preserves the parts of the network that matter, leading to <strong>better performance at smaller sizes</strong>.</p>
<hr>
<p>In the experiments, they could reduce the network size by up to 90%, and the resulting subnetworks still learned faster and better than the full network!</p>
<hr>
<h2 id="Do-Winning-Tickets-Generalize-Better"><a href="#Do-Winning-Tickets-Generalize-Better" class="headerlink" title="Do Winning Tickets Generalize Better?"></a>Do Winning Tickets Generalize Better?</h2><p>Now here’s where things get spicy. When comparing test accuracies, the researchers noticed something curious:</p>
<ul>
<li>The winning tickets not only learned faster,</li>
<li>They often had <strong>better generalization</strong> than the original model!</li>
</ul>
<p>This means that they didn’t just memorize training data—they actually learned to perform better on unseen test data.</p>
<p>This idea is related to something called <strong>Occam’s Hill</strong>—too big and you overfit, too small and you underfit. Winning tickets land at a sweet spot: small enough to avoid overfitting, but just right to still learn effectively.</p>
<h2 id="Initialization-Matters-A-Lot"><a href="#Initialization-Matters-A-Lot" class="headerlink" title="Initialization Matters (A Lot)"></a>Initialization Matters (A Lot)</h2><p>Another key takeaway: it’s not just the structure of the subnetwork that matters. It’s also the <strong>exact initial weights</strong>.</p>
<p>If you take a winning ticket’s structure and randomly reinitialize it, it <strong>loses its magic</strong>—learning slows down and performance drops.</p>
<h2 id="Expanding-to-Convolutional-Networks"><a href="#Expanding-to-Convolutional-Networks" class="headerlink" title="Expanding to Convolutional Networks"></a>Expanding to Convolutional Networks</h2><p>The authors didn’t just test on simple fully-connected networks like LeNet on MNIST. They also ran experiments on <strong>convolutional networks</strong> like Conv-2, Conv-4, and Conv-6 on CIFAR-10.</p>
<p>Surprise surprise: they found <strong>winning tickets</strong> there too. In fact, the same pattern repeated:</p>
<ul>
<li>Winning tickets learn faster</li>
<li>They reach higher accuracy</li>
<li>They generalize better</li>
<li>Initialization still matters</li>
</ul>
<p>The success wasn’t limited to toy datasets—this was happening on moderately complex image classification tasks too.</p>
<h2 id="Drop-Out-Pruning"><a href="#Drop-Out-Pruning" class="headerlink" title="Drop-Out + Pruning"></a>Drop-Out + Pruning</h2><p>What happens when you combine <strong>dropout</strong> with pruning?*</p>
<p>Turns out, dropout helps too! Dropout already encourages the network to be robust to missing connections. So when you prune, the network is more resilient.</p>
<p>When they trained networks <strong>with dropout</strong> and applied iterative pruning, the test accuracy <strong>improved even further</strong>. This hints that dropout may help in preparing the network for successful pruning.</p>
<h2 id="The-Big-Leagues-VGG-19-and-RESNET-18"><a href="#The-Big-Leagues-VGG-19-and-RESNET-18" class="headerlink" title="The Big Leagues: VGG-19 and RESNET-18"></a>The Big Leagues: VGG-19 and RESNET-18</h2><p>Taking it up a notch, the paper also tested on deeper, real-world architectures:</p>
<ul>
<li><strong>VGG-19</strong></li>
<li><strong>ResNet-18</strong></li>
</ul>
<p>The pattern mostly held up—but with a twist. For these deep networks, iterative pruning only worked well if they used <strong>learning rate warm-up</strong>.</p>
<p>Without warm-up, pruning failed to find winning tickets. With warm-up (i.e., slowly increasing the learning rate at the beginning of training), they were back in business.</p>
<p>So yes—winning tickets exist even in deep networks, but only if you treat them with care.</p>
<h2 id="Key-Takeaways"><a href="#Key-Takeaways" class="headerlink" title="Key Takeaways"></a>Key Takeaways</h2><ul>
<li>Big neural networks contain hidden winning tickets—smaller subnetworks that can be trained to match or exceed full network performance.</li>
<li>You find them by <strong>pruning</strong> and <strong>resetting</strong> repeatedly.</li>
<li>These subnetworks not only match accuracy, but often learn <strong>faster</strong> and generalize <strong>better</strong>.</li>
<li>The <strong>initialization</strong> is crucial—you can’t just randomly reinitialize and expect the same results.</li>
<li>Even deeper networks like VGG and ResNet have winning tickets, but they may require careful tuning (e.g., learning rate warm-up).</li>
<li>Pruning isn’t just for compression—it might teach us something deep about how neural networks work.</li>
</ul>
<p>Now, what I am trying to do is replecating the results found by the authors myself. So stick around and keep a look over my <a target="_blank" rel="noopener" href="https://github.com/DA1729">GitHub</a>.</p>
<p>peace. da1729</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] @misc{frankle2019lotterytickethypothesisfinding,<br>      title&#x3D;{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},<br>      author&#x3D;{Jonathan Frankle and Michael Carbin},<br>      year&#x3D;{2019},<br>      eprint&#x3D;{1803.03635},<br>      archivePrefix&#x3D;{arXiv},<br>      primaryClass&#x3D;{cs.LG},<br>      url&#x3D;{<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>},<br>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://da1729.github.io/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/" data-id="cmcmv8tss0005lwbnhuv2egzg" data-title="Lottery Ticket Hypothesis Part 2" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI-Acceleration/" rel="tag">AI Acceleration</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Lottery Ticket Hypothesis Part-1</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-Acceleration/" rel="tag">AI - Acceleration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-Acceleration/" rel="tag">AI Acceleration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Analog/" rel="tag">Analog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hardware-Acceleration/" rel="tag">Hardware Acceleration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VLSI/" rel="tag">VLSI</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI-Acceleration/" style="font-size: 10px;">AI - Acceleration</a> <a href="/tags/AI-Acceleration/" style="font-size: 10px;">AI Acceleration</a> <a href="/tags/Analog/" style="font-size: 20px;">Analog</a> <a href="/tags/Hardware-Acceleration/" style="font-size: 20px;">Hardware Acceleration</a> <a href="/tags/VLSI/" style="font-size: 20px;">VLSI</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/04/16/Lottery-Ticket-Hypothesis-for-Beginners-Part-2/">Lottery Ticket Hypothesis Part 2</a>
          </li>
        
          <li>
            <a href="/2025/04/13/Lottery-Ticket-Hypothesis-for-Beginners/">Lottery Ticket Hypothesis Part-1</a>
          </li>
        
          <li>
            <a href="/2025/03/18/In-Memory-Computation-using-Analog-Part-2/">In Memory Computation using Analog Part 2</a>
          </li>
        
          <li>
            <a href="/2025/03/15/In-Memory-Computation-using-Analog-Part-1/">In-Memory Computation using Analog Part-1</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 Daksh Pandey<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>